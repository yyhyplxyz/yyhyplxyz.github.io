<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[东野圭吾的《白金数据》]]></title>
    <url>%2F2018%2F01%2F26%2F%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE%E7%9A%84%E3%80%8A%E7%99%BD%E9%87%91%E6%95%B0%E6%8D%AE%E3%80%8B%2F</url>
    <content type="text"><![CDATA[考完试后回到家里，闲来无事便读了一个下午读完了这本新出的侦探小说。比较符合我对东野圭吾的预期，小说很有悬念，比较刺激，但文学性和思想性不足，只是消遣性读物。 后面会涉及严重剧透======================================= 故事主要描述了日本想全国范围内收集民众的DNA数据，从而通过罪犯遗留下来的任何可能携带基因的物品，达到快速找到罪犯亲属或罪犯的目的。但这样巨大的工程容易造成民众隐私的泄漏，同时在小说中通过一系列无法通过基因数据库找到罪犯消息的犯罪案件和主角的被嫁祸，几个重要配角的反转，揭示了政府重要官员在数据库中做的手脚——他们的亲属犯罪将无法通过基因匹配找到罪犯的信息，自然的也将无法通过基因匹配查到他们身上。 这些政府官员的数据就是白金数据。 小说想向我们揭示大数据技术下个人隐私泄漏的可能，这不过是老生常谈。支付宝之前的年度账单事件就拔出萝卜带着泥，牵扯了一大批不合理读取用户个人数据的app。可以想象对掌控着我们所有社交信息的腾讯公司而言，我们几乎就是白纸一张，一切都能被人一眼望到底。 作者还想引起我们的思考，人心到底是不是完全物质化的呢？毕竟激素，神经递质，他们主宰着我们的情绪和思想。随着技术的进步，我们完全可以人造情感和思想。无奈太过浅尝辄止，作者本身并没有通过故事深入讨论这个话题。 东野圭吾应该也试图展示政府高级官员的龌龊。太阳底下没有新鲜事，全国范围内收集民众的DNA数据是一个涉及法律和公安执法的变革，历朝历代的变革根本目的都是更好地维护统治阶级的利益。屁股决定脑袋，换成说是日本政坛大佬都会这样做的。君王一怒，伏尸百万，流血漂橹。相比之下，放点儿白金数据都是小儿科的了。网上不就有纪录片和帖子讨论朴槿惠为了邪教献祭牺牲了岁月号三百多名学生的事情吗？想想令人毛骨悚然，但再想想史书不绝于笔的“族”，又为之泰然。详情]]></content>
  </entry>
  <entry>
    <title><![CDATA[利用随机森林法预测Titanic乘客生存率]]></title>
    <url>%2F2018%2F01%2F26%2F%E5%88%A9%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%B3%95%E9%A2%84%E6%B5%8BTitanic%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E7%8E%87%2F</url>
    <content type="text"><![CDATA[问题描述在kaggle上有一个竞赛题目，是如何根据泰坦尼克号上的已知的乘客数据来预测某一乘客在该轮船上能否存活。题目本身就是一道供初学者进行数据分析学习的问题，也有很多大佬给出了自己的数据训练的tutorial，自己就在这里分享下自己在借鉴了一些大佬的经验之后给出的自己的解决方案。 解决流程数据清洗数据清洗是所有数据分析问题第一步要解决的，我们首先来看下官方对数据集的说明。 PassengerId: 编号 Survived: 0 = 死亡，1 = 生存 Pclass: 船票级别 1 = 高级， 2 = 中等， 3 = 低等 Name: 名称 Sex: male = 男性，female = 女性 Age: 年龄 SibSp: 在 Titanic 上的兄弟姐妹以及配偶的人数 Parch: 在 Titanic 上的父母以及子女的人数 Ticket: 船票编号 Fare: 工资 Cabin: 所在的船舱 Embarked: 登船的港口 C = Cherbourg, Q = Queenstown, S = Southampton 接下来我们让我们读取数据并对数据有一个初步的感性认识。 我们首先来做定性分析，看一下特征类别分布是否平衡。类别平衡指分类样例不同类别的训练样例数目差别不大。当差别很大时，为类别不平衡。当类别不平衡的时候，例如正反比为 9:1，学习器将所有样本判别为正例的正确率都能达到 0.9。这时候，我们就需要使用 “再缩放”、“欠采样”、“过采样”、“阈值移动” 等方法。如下图，我们发现总体而言还是特征分布还是平衡的，活下来的人和死亡人数没有偏差过多。 删除无必要数据空数据处理我们来看看有多少空数据 对空数据我们怎么处理呢，这就要分情况讨论啦。 Age作图 Age ~ Survived。年龄较小的孩子生存的几率大。 因为年龄是一个连续值，而且他会对预测结果产生影响，同时我们发现不同群体中年龄的分布是不同的。 因此我们根据票的等级，在 Titanic 上的兄弟姐妹以及配偶的人数，在 Titanic 上的父母以及子女的人数来将数据分为不同的集合，再用缺失数据所在集合的平均值来进行填充。并判断最后的对 Age ~ Survived 的性质并未产生影响。 Embarked它缺少的数据只有两个，直接用众数填充即可。 Cabin他的数据较为复杂，Cabin 特征值由字母开头，判断船舱按字母分为A，B，C… 于是我们仅提取字母编号，降低维度。然后使用新的字母‘U’填充缺失数据。我们发现缺失数据的游客主要是三等舱的，并且这部分游客的生存率相对较低。 数值化和标准化数值化Ticket 特征值中的一串数字编号对我们没有意义，忽略。下面代码中，我们用正则表达式过滤掉这串数字，并使用 pandas get_dummies 函数进行数值化（以 Ticket 特征值 作为新的特征，0,1 作为新的特征值）。 123456789101112Ticket=[]import rer=re.compile(r'\w*')#正则表达式，查找所有单词字符[a-z/A-Z/0-9]for i in data['Ticket']: sp=i.split(' ')#拆分空格前后字符串，返回列表 if len(sp)==1: Ticket.append('U')#对于只有一串数字的 Ticket，Ticket 增加字符 'U' else: t=r.findall(sp[0])#查找所有单词字符，忽略符号，返回列表 Ticket.append(''.join(t))#将 t 中所有字符串合并data['Ticket']=Ticketdata=pd.get_dummies(data,columns=['Ticket'],prefix='T')#get_dummies：如果DataFrame的某一列中含有k个不同的值，则可以派生出一个k列矩阵或DataFrame（其值全为1和0） getdummies是处理类别醒数据很好的一种方式，这样我们可以将离散的分类变成具体的0，1特征向量，很大程度的加速了电脑计算的速度和监督学习最后训练得到的准确率。对cabin和embarked同样做此操作。最后得到的特征向量如图 标准化偏态分布偏态分布的数据有时不利于模型发现数据中的规律，我们可以使用 Log Transformation 来处理数据，这样可以提高训练的准确度。比如Fare这一特征就存在明显的偏态分布，我们skitlearn提供的函数进行处理，参考 Skewed Distribution and Log Transformation 离群点删除离群点是显著偏离数据集中其余对象的点。离群点来源于操作失误，数据本身的可变性等。我们这里采用箱线法,检测特征 [‘Age’, ‘Parch’, ‘SibSp’, ‘Fare’]的离群点。参考离群点和箱线法 123456789101112131415161718192021222324from collections import Counterdef outlier_detect(n, df, features):#定义函数 outlier_detect 探测离群点，输入变量 n, df, features，返回 outlier outlier_index = [] for feature in features: Q1 = np.percentile(df[feature], 25)#计算上四分位数（1/4） Q3 = np.percentile(df[feature], 75)#计算下四分位数（3/4） IQR = Q3 - Q1 outlier_span = 1.5 * IQR col = ((data[data[feature] &gt; Q3 + outlier_span]) | (data[data[feature] &lt; Q1 - outlier_span])).index outlier_index.extend(col) print('%s: %f (Q3+1.5*IQR) , %f (Q1-1.5*QIR) )' % (feature, Q3 + outlier_span, Q1 - outlier_span)) outlier_index = Counter(outlier_index)#计数 outlier = list(i for i, j in outlier_index.items() if j &gt;= n) print('number of outliers: %d' % len(outlier)) print(df[['Age', 'Parch', 'SibSp', 'Fare']].loc[outlier]) return outlieroutlier = outlier_detect(3, data, ['Age', 'Parch', 'SibSp', 'Fare'])#调用函数 outlier_detectdata = data.drop(outlier) 模型选择与训练模型介绍Boosting模型与bagging模型Bagging：假设我有一个大小为n的训练集D，bagging会从D中有放回的均匀地抽样，假设我用bagging生成了m个新的训练集Di，每个Di的大小为j。由于我有放回的进行抽样，那么在Di中的样本有可能是重复的。如果j=n，这种取样称为bootstrap取样。现在，我们可以用上面的m个训练集来拟合m个模型，然后结合这些模型进行预测。对于回归问题来说，我们平均这些模型的输出;对于分类问题来说，我们进行投票（voting）。 Boosting：Boosting与Bagging主要的不同是：Boosting的base分类器是按顺序训练的（in sequence），训练每个base分类器时所使用的训练集是加权重的，而训练集中的每个样本的权重系数取决于前一个base分类器的性能。如果前一个base分类器错误分类地样本点，那么这个样本点在下一个base分类器训练时会有一个更大的权重。一旦训练完所有的base分类器，我们组合所有的分类器给出最终的预测结果。过程如下图： Adaboost与RandomForest算法Adaboost是一种基于boosting模型的迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。 Adaboost的结构:最后的分类器YM是由数个弱分类器（weak classifier）组合而成的,相当于最后m个弱分类器来投票决定分类结果，而且每个弱分类器的“话语权”因子α大小不一样。 Randomforest是基于bangging模型实现的，他的元分类器是决策树。过程简要概括如下： 从原始训练集中进行bootstrap抽样 用步骤1中的bootstrap样本生成决策树 随机选择特征子集 用上面的特征子集来拆分树的节点 重复1和2两个步骤 集成所有生成的决策树进行预测 模型评估我们采用k 折交叉验证法，更具体的是10 折交叉验证法。 k 折交叉验证（k-fold cross validation）：将 D 划分 k 个大小相似的子集（每份子集尽可能保持数据分布的一致性：子集中不同类别的样本数量比例与 D 基本一致），其中一份作为测试集，剩下 k-1 份为训练集 T，操作 k 次。 例如 D 划分为 D1，D2，… ，D10，第一次使用 D1 作为训练集，第二次使用 D2，第三次使用 D3， … ， 第十次使用 D10 作为测试集。最后计算 k 次测试误差的平均值近似泛化误差。 12345678y = data['Survived']X = data.drop(['Survived'], axis=1).valuesclassifiers = [AdaBoostClassifier( random_state=2), RandomForestClassifier(random_state=2)]for clf in classifiers: score = cross_val_score(clf, X, y, cv=10, scoring='accuracy')#cv=10：10 折交叉验证法，scoring='accuracy'：返回测试精度 print([np.mean(score)])#显示测试精度平均值 我们可以发现随机森林分类器的准确率要高不少。 模型训练但是随机森林是基于决策树的，决策树一直存在着过拟合的问题。 过拟合是学习器性能过好，把样本的一些特性当做了数据的一般性质，从而导致训练误差低但泛化误差高。学习曲线是判断过拟合的一种方式，同时可以判断学习器的表现。学习曲线包括训练误差（或精度）随样例数目的变化曲线与测试误差（或精度）随样例数目的变化曲线。 接下来通过绘制学习曲线，我们发现训练误差始终接近 0，而测试误差始终偏高，说明存在过拟合的问题。 123456789101112131415161718192021222324252627282930313233from sklearn.model_selection import learning_curveimport matplotlib.pyplot as pltdef plot_learning_curve(estimator, title, X, y, cv=10, train_sizes=np.linspace(.1, 1.0, 5)):#定义函数 plot_learning_curve 绘制学习曲线。train_sizes 初始化为 array([ 0.1 , 0.325, 0.55 , 0.775, 1. ]),cv 初始化为 10，以后调用函数时不再输入这两个变量 plt.figure() plt.title(title)#设置图的 title plt.xlabel('Training examples')#横坐标 plt.ylabel('Score')#纵坐标 train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, train_sizes=train_sizes)#使用 10 折交叉验证法，对 train_sizes*m（m为总的样例数目） 个的数据进行训练，返回训练精度 train_scores,测试精度 test_scores train_scores_mean = np.mean(train_scores, axis=1)#计算平均值 train_scores_std = np.std(train_scores, axis=1)#计算标准差 test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid()#设置背景的网格 plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='g')#设置颜色 plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='r') plt.plot(train_sizes, train_scores_mean, 'o-', color='g', label='traning score')#绘制训练精度曲线 plt.plot(train_sizes, test_scores_mean, 'o-', color='r', label='testing score')#绘制测试精度曲线 plt.legend(loc='best') return pltg = plot_learning_curve(RandomForestClassifier(), 'RFC', X, y)#调用函数 plot_learning_curve 绘制随机森林学习器学习曲线 要解决这一问题只能通过调参。 skitlearn中的randomforestclassifer函数具有非常多的参数，列举几个如下。 参数 特点 n_estimators 基学习器数目（默认值10） 基本趋势是值越大精度越高 ，直到达到一个上限 criterion 选择算法 gini 或者 entropy (默认 gini) 视具体情况定 max_features 2.2.3节中子集的大小，即k值（默认 sqrt(n_features)） max_depth 决策树深度 过小基学习器欠拟合，过大基学习器过拟合。粗调节 max_leaf_nodes 最大叶节点数（默认无限制） 粗调节 min_samples_split 分裂时最小样本数，默认2 细调节,越小模型越复杂 min_samples_leaf 叶节点最小样本数，默认2 细调节，越小模型越复杂 bootstrap 是否采用自助法进行样本抽样（默认使用） 决定基学习器样本是否一致 我们先通过尝试找到最好的n_estimators和max_depth，max_leaf_nodes 参数 1234567891011121314151617181920def para_tune(para, X, y): # clf = RandomForestClassifier(n_estimators=para) #n_estimators 设置为 para score = np.mean(cross_val_score(clf, X, y, scoring='accuracy')) return scoredef accurate_curve(para_range, X, y, title): score = [] for para in para_range: score.append(para_tune(para, X, y)) plt.figure() plt.title(title) plt.xlabel('Paramters') plt.ylabel('Score') plt.grid() plt.plot(para_range, score, 'o-') return pltg = accurate_curve([2, 10, 50, 100, 150], X, y, 'n_estimator tuning') 与上面代码类似的我们可以得到下面三张图 接着我们就可以固定这三个影响较大的参数，再利用自动调参函数GridSearchCV来对其他的参数进行粗略的调节。 如此我们得到了最终0.8204的准确率。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages博客创建历程]]></title>
    <url>%2F2018%2F01%2F24%2F%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%88%9B%E5%BB%BA%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言对程序员来说，纸上得来终觉浅，绝知此事要躬行，每分专业的能力绝不是看书看来的，听课听来的，而一定要是自己一行一行代码敲来的。不仅如此，每个不甘寂寞的程序猿（媛），还会有着创建一个博客，发布自己所见所感所学的想法或习惯。为了更好地促进自己的进步，我也来加入写博客的大军啦。 怎样的博客应该以什么样的方式来创建博客是本节的重点。 在大一时，为了应付一门课的作业自己开通了cdsn的博客，有很多大牛的博客就扎根在了cdsn。但是对我而言，cdsn总有着烦人的广告，下载资料的时候还总要花钱充值，让我觉得很不爽，所以pass了这个选择。 还有很多人会选择简书，可那里是文艺青年的聚集地，程序员特有的geek氛围要远远差于cdsn。 至于知乎专栏和微信公众号，虽然也可以当作自己写随笔发布的园地，但与博客相比，总觉得差了点什么。 除此之外，还有wordpress和wix等建站方式，其实很方便的，可见即可得的操作，免费版的也不用花钱。主要原因还是自己嫌弃它们一点也不geek，就没有使用这种方式啦。说真的wix创建出来的网站可好看啦，而且官方的引导特别详尽，就像游戏里的引导操作一样，自己很容易就能建成一个很漂亮的网站，我当时 于是就开始考虑起自己搭建一个博客网站。自己之前尝试过用python的django框架搭建过博客网站，它自带有功能很强大的后台，当初自己实现的最终版也差强人意。一个完全由自己掌控的网站是能让人很有成就感的。可想了想自己还是没有选择这个自己实现的网站来作为自己的博客，因为除了后台功能外，自己所有都要手打代码，评论，搜索乃至前台的页面，出了bug要自己调试，万一调试不出来，博客就挂掉了。。。。 那么就考虑到了使用hexo框架来搭建博客，它并不是真正意义上代码的框架，使用它基本不需要编写代码，简介，美观，好用而功能强大。而且它与github pages可以无间合作，还可以省去了购买云主机，云服务器的花销。 创建博客的准备我们首先需要在电脑上安装好node.js , git和一些下载包管理工具如npm，homebrew（ mac自带），apt-get（Ubuntu自带），yum（centos自带）。 接着进入hexo官网，按照上面的步骤安装hexo。（此处非常有必要阅读一下hexo官方对自己的介绍） 我们此时已经安装好了hexo，可以输入以下命令检查是否安装成功，windows用户可能需要再配置一下环境变量， 接着我们输入 hexo init yournamecd yournamenpm install 这样就可以创建自己的博客文件夹，里面有网站的各个静态文件，自己以后的文章也会放在这个文件夹里，进入这个文件夹后，用npm命令来安装各项依赖。 此时我们用hexo new “文章标题”就可以新建一个文章，我们进入source文件夹内的_posts文件夹，就会发现多出了一篇md后缀的文件，这就是我们刚刚创建的文章啦，我们也可以直接在里面创建markdowm文件。接着我们用hexo s命令就能在本地运行这个简单的博客网站了。 然后在github新建一个代码仓库，仓库的名字一定要是”yourusername.github.io”。如我github账户名是yyhyplxyz，我这个代码仓库名字就是“yyhyplxyz.github.io”。 接着去github上添加ssh key，可以参考此网址ssh。]]></content>
      <tags>
        <tag>coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文章标题]]></title>
    <url>%2F2018%2F01%2F23%2F%E6%96%87%E7%AB%A0%E6%A0%87%E9%A2%98%2F</url>
    <content type="text"><![CDATA[第一个文章]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
