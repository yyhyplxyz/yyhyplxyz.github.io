<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[docker介绍深入浅出(-)]]></title>
    <url>%2F2018%2F12%2F24%2Fdocker%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[镜像和容器镜像和容器是 Docker 中最基本的 2 个概念，镜像相当于光盘， 光盘里面存储的数据是只读的， 不会被更改的。我们可以把镜像找作一个模板或者蓝图，而容器是用镜像生成的 Docker 实例。一个镜像可以生成多个容器， 每个容器之间，容器与宿主机之间都是相互隔离的，同时，容器可以快速方便地运行, 也可以方便地删除。在实际应用中，我们甚至可以把容器当作虚拟机来使用。 自动部署与Dockerfile我们首先需要安装一个docker，这部分内容略，读者可以轻易在网上找到资料，且操作极易。 运行第一个容器应用我们使用下面的命令启动一个 Docker 容器 1docker run -d -t -p 8000:5000 -name demo ubuntu:18.04 这条命令的具体解释如下 run 表示启动一个新的 Docker 容器 -d 表示容器在后台运行 -t 极少能用到， 用于让一个空白的 ubuntu 镜像在后台运行 -p 用于指定端口映射′ 表示在本机访问 8000 会被自动转到容器中的 5000 端口必须保证本机没有其他程序占用了 8000 端口， 否则这里会失败 -name demo 指定了容器的名字是 demo ubuntu:18.04 是启动容器使用的镜像名，Docker 会自动从镜像服务器去下载这个镜像 运行一个简单的web应用这次我们选择开一个新容器，并提供web服务。 1docker run -t -d -p 7000:5000 -name demo ubuntu:18.04 由于之前已经自动下载过 ubuntu:18.04 所以这次不会重新下载镜像, 速度很快，这个新的容器名叫demo。 我们首先在本地准备好运行的代码文件a.py，如下 123456789from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return 'Hello World'if __name__ == "__main__": app.run(host="0.0.0.0", debug=True) 接着我们在容器内创建目录，exec 命令后跟随容器名，然后再添加操作命令mkdir /code 1docker exec demol mkdir /code cp 参数把当前文件夹的a.py拷贝到 demo 容器的 /Code/a.py处。 1docker cp a.py "demo:/code/a.py" /code必须是容器中存在的目录， Docker 不会自动创建，因此我们需要先手动创建。 接着我们同样用exec安装必要的依赖 12345docker exec demo apt updatedocker exec demo apt -y install python3 python3-pipdocker exec demo pip3 install flask 然后在容器 demo 中运行a.py 1docker exec demo python3 /code/o.py 通过以上的操作，我们并在容器中安装程序运行所需的依赖库然后运行了一个 flask web 程序 以上操作较繁琐，我们也可以新建两个脚本文件install.sh和run.sh，然后直接运行脚本文件。 1234#install.shapt updateapt -y install python3 python3-pippip3 install flask 123#run.shcd /codepython3 a.py 1234567docker cp install.sh "demol:/code/install.sh"docker cp run.sh "demo1:/code/run.sh"docker exec demol bash /code/install.shdocker exec demol bash /code/run.sh Dockerfile自动化操作在本文件夹下，除了a.py外，我们又新建了个Dockerfile文件，如下。 12345678910111213141516171819202122232425# 在 Dockerfile 文件中 # 是注释# FROM 用于指定构建镜像使用的基础镜像FROM ubuntu:18.04# RUN 用于在构建镜像的时候在镜像中执行命令# 这里我们安装 python3 和 flask web 框架RUN apt updateRUN apt -y install python3 python3-pipRUN pip3 install flask# COPY 相当于命令的 docker cp# 把本机当前目录下的 app.py 文件拷贝到镜像的 /code/app.py# 和 docker cp 不同的是，COPY 会自动创建镜像中不存在的目录，比如 /codeCOPY app.py /code/app.py# WORKDIR 用于指定从镜像启动的容器内的工作目录WORKDIR /code# CMD 用于指定容器运行后要执行的命令和参数列表# 这样从本镜像启动容器后会自动执行 python3 app.py 这个命令# 由于我们已经用 WORKDIR 指定了容器的工作目录# 所以下面的命令都是在 /code 下执行的CMD ["python3", "app.py"] 我们可以直接运行下面的命令构建镜像 1docker build -t webimage . -t webimage 指定了镜像的名字为 webimage，最后的那个 . 用来表示工作目录为本机当前目录。随后我们可以直接用该镜像来运行容器 1docker run -p 8001:5000 -name demo2 webimage docker 常用命令启动一个停止运行的容器 1docker start demo 查找正在运行的容器 1docker ps 停止容器， demo 是容器的名字 1docker stop demo 可以用 -o 参数查找所有容器, 包括未运行的 1docker -o ps 删除被停止的容器和运行中的容器 12docker rm demodocker rm -f demo 数据卷与文件挂载我们可以在概念上把 Docker 找作虚拟机。当容器被删除的时候， 容器里的所有数据都会被删除，两个不同的容器之间无法互通。基于这些因素 Docker 推出了 数据卷 (volume) 功能。我们可以把数据卷理解为虚拟机的虚拟磁盘， 它是独立于容器的文件，在容器中它破挂载为一个目录的形式。对于容器中的应用来说， 数据卷是透明的， 无法感知它的存在， 它就是一个普通的文件夹。由于数据卷独立于容器而存在, 因此删除容器的时候数据卷不会受到影响。数据卷有以下优点 多容器可以通过挂载同一个数据卷来共享数据 数据卷可以方便地备份存储数据 创建一个 volume 1docker volume create testvolume 列出所有数据卷 1docker volume ls 删除一个数据卷 1docker volume rm testvolume 我们下面来找一下如何在容器中使用数据卷 先创建一个数据卷web，接着在运行容器webimage的时候， 使用参数 -mount 如下 1docker run -d --name demovolume -mount source=web, target=/volume webimage 后面参数的含义是把数据卷挂载到容器的 /volume 目录上 这样就运行了一个带有数据卷的容器， 这个容器的 /VoIume 目录中的内容在容器被删除后仍然存在。因为它实际上是存在 Docker 数据卷中。没有保存在数据卷上的文件会在容器被删除后丢失 这样我们就可以实现多容器之间可以通过数据卷共享文件 1docker run -d -name demovolume2 -mount source=web,target=/volume webimage 除了挂载数据卷外, Docker 还可以挂载共享目录 (这一点和虚拟机一样)。共享目录的优势是使用方便， 易于理解。我们开发时在宿主机中修改源代码就可以做到docker中实时生效省却 build 镜像的过程。下面的命令会从 nginx 镜像运行一个名为 nginx1 的容器，井且设置了 8080-&gt;80 的端口映射，mount 参数的 type=bind 表明要挂载共享目录，把宿主机的当前目录映射为容器的 /usr/share/nginx/html(这是 nginx 容器的静态页面文件存放路径).这样在宿主机中访问localhost:8080 会自动访问宿主机当前目录下的 index.html 文件 1docker run -p 8080:80 -name nginx1 -mount type=bind, source="$&#123;PWD&#125;" ,target=/usr/share/nginx/html/ nginx 多应用容器化编排Docker 被设计为程序容器， 所以每一个容器只应该运行一个程序，但是在实际的项目中会有需要多个程序互相配合一起运行的情况。比如 Web 程序通常包含 cpp、 数据库、 nginX， Redis 等。这些程序各自的容器需要协同工作， 并且需要能够互相访问网络。比如 cpp 需要连接数据库，nginx 需要能访问 cpp 才能给它做反向代理。 由于 Docker 容器是一个隔离的环境， 正常情况下容器与容器之间是无法互相访问的，为了应对复杂工程的需要， 我们可以手动配置多容器之间的虚拟网络，文件互访等功能来实现容器交互，但 Docker 官方推出了Compose 程序用于配置管理多容器的运行。 Compose 通过一个单独的 docker-compose.yml 配量文件来管理一组容器，参下。 123456789101112131415161718192021222324252627# 表示这是 compose 配置文件第三版version: '3'# 每个服务都是一个 Docker 容器# 所以必须用 image 指定服务的镜像名或者从 Dockerfile 中 build 镜像services: pyweb: # build 指定了 Dockerfile 所在的路径 build: . # ports 指定暴露的端口，9000 是宿主机，5000 是容器 # 可以指定多个暴露端口 ports: - "9000:5000" # volumes 参数把当前目录挂载到容器的 /code # docker-compose 的配置中才支持相对路径的挂载 volumes: - .:/code # depends_on 设定了依赖，这里 redisdemo 会先于 pyweb 启动 # 但是如果 redisdemo 启动时间长于 pyweb # 那么 pyweb 运行的时候 redisdemo 未必可用 depends_on: - redisdemo redisdemo: # 每个服务必须用 image 指定镜像名或者从 Dockerfile 中 build # 这里用 image 指定镜像，redis:alpine 是 redis 项目的官方 Docker 镜像 image: "redis:alpine" 使用如下命令开启服务 1docker-compose up 用 -d 参数让项目后台运行 1docker-comuose up -d 用 stop 暂停容器的运行 1docker-comuose up -d 用 down 关闭并删除项目的所有容器 1docker-compose down 如果我们修改了文件代码后，要重新启动项目时，再次运行docker-compose up，会发现它并末重新构建镜像。这是因为 Dockerfile 并末修改， 所以 Docker 复用了已有的镜像，修改并末生效。这时候我们应该重新构建镜像并运行, 或者加上 “build 参数来强制 builddocker-compose up &quot;build。 Docker使用的小trickDocker更换镜像源将下载在源改为中科大镜像源等国内源，可以显著提高下载速度。https://docker.mirrors.ustc.edu.cn，具体修改方式见下图。]]></content>
  </entry>
  <entry>
    <title><![CDATA[容器化搭建Go-Web应用]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%AE%B9%E5%99%A8%E5%8C%96%E6%90%AD%E5%BB%BAGo-Web%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[改用Mysql数据库本次作业要求改BoltDB为Mysql数据库，同时，考虑到后面的作业要求搭建博客。因此将之前写的新闻应用做了重构，改成了一个博客后台。 使用了Beggo框架自带的ORM。个人感觉这类框架内部封装好了很多逻辑，实现功能会比boltdb的键值对操作要容易很多。下面以用户类别为例分别对增删改查四个操作作出解释。 User的模型类别如下，我们可以像下面这样指定对应数据库的列要求。如主键pk，自增auto，可以为空null与不可重复unique等。 123456789101112type User struct &#123; Id int `orm:"pk;auto"` Username string `orm:"unique"` Password string Token string `orm:"unique"` Avatar string Email string `orm:"null"` Url string `orm:"null"` Signature string `orm:"null;size(1000)"` InTime time.Time `orm:"auto_now_add;type(datetime)"` Roles []*Role `orm:"rel(m2m)"`&#125; 增加操作 在用户注册页面，当我们根据用户的输入构建好了user类后，就可以通过如下函数进行用户的创建，注册。 orm是单例模式，如下一行简单的insert就做到了。 12345func SaveUser(user *User) int64 &#123; o := orm.NewOrm() id, _ := o.Insert(user) return id&#125; 删除操作 实际博客应用中很少删除用户，为了演示使用下面的函数。同样的传入user类后，一行简单的delete就做到了。 1234func DeleteUser(user *User) &#123; o := orm.NewOrm() o.Delete(user)&#125; 修改操作 如上，因为指定了id为主键，所以update的时候会查找主键相同的，再进行修改 1234func UpdateUser(user *User) &#123; o := orm.NewOrm() o.Update(user)&#125; 查找操作 查找的语句较麻烦，我们需要指定一个数据表，指明过滤的字段和条件，并将user类型指针传入以获得查找到的结果。 123456func Login(username string) (bool, User) &#123; o := orm.NewOrm() var user User err := o.QueryTable(user).Filter("Username", username).One(&amp;user) return err != orm.ErrNoRows, user&#125; 除了上述的操作外，我们还可以用数据库原生语句来执行命令。如下以增删为例。 123456789func DeleteUserRolesByUserId(user_id int) &#123; o := orm.NewOrm() o.Raw("delete from user_roles where user_id = ?", user_id).Exec()&#125;func SaveUserRole(user_id int, role_id int) &#123; o := orm.NewOrm() o.Raw("insert into user_roles (user_id, role_id) values (?, ?)", user_id, role_id).Exec()&#125; 我们构建好了model后，就可以注册数据库了。 如下我们需要指明数据库的地址，用户名，密码数据库名，通过orm.RegisterDataBase注册数据库，通过orm.RegisterDriver注册mysql驱动。之后我们还需要依次注册各个model，并同步数据库orm.RunSyncdb。 12345678910111213func init() &#123; orm.RegisterDriver("mysql", orm.DR_MySQL) orm.RegisterDataBase("default", "mysql", username+":"+password+"@tcp("+url+":"+port+")/pybbs?charset=utf8&amp;parseTime=true&amp;charset=utf8&amp;loc=Asia%2FShanghai", 30) orm.RegisterModel( new(models.User), new(models.Topic), new(models.Section), new(models.Reply), new(models.ReplyUpLog), new(models.Role), new(models.Permission)) orm.RunSyncdb("default", false, true)&#125; Docker构建容器 mysql镜像 我们在安装了docker后，首先将mysql镜像下载下来。 1docker pull mysql:5.6 接着我们可以查看到已有的镜像。 1docker images |grep mysql 然后开启mysql容器，并指定各类配置。 1docker run -p 3306:3306 --name mymysql -v $PWD/conf:/etc/mysql/conf.d -v $PWD/logs:/logs -v $PWD/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 -p 3306:3306：将容器的 3306 端口映射到主机的 3306 端口。 -v ~/conf:/etc/mysql/conf.d：将主机当前目录下的 conf/my.cnf 挂载到容器的 /etc/mysql/my.cnf。 -v ~/logs:/logs：将主机当前目录下的 logs 目录挂载到容器的 /logs。 -v ~/data:/var/lib/mysql ：将主机当前目录下的data目录挂载到容器的 /var/lib/mysql 。 -e MYSQL_ROOT_PASSWORD=123456：初始化 root 用户的密码。 node镜像 接下来开启前端镜像，以下我们使用dockerfiles，进入项目目录。 1234567891011121314#安装node镜像FROM node:7.8.0# 创建 app 目录WORKDIR /app# 安装 app 依赖RUN npm install#暴露端口EXPOSE 8080#运行命令CMD [ "npm", "run" ,"dev"] 然后我们创建docker镜像 1docker build -t web_front . 并运行 1docker run -p 8080:8080 -d web_front go镜像 接下来我们同样进入web后端目录，创建dockerfile。 12345678910111213#安装go镜像FROM golang:latest# 创建 应用 目录WORKDIR $GOPATH/src/web_proADD . $GOPATH/src/web_pro#安装依赖RUN go get github.com/astaxie/beegoRUN go get github.com/beego/beeRUN go build .#暴露端口EXPOSE 8080#运行命令CMD [ "bee", "run" ] 然后我们创建docker镜像，并运行 1docker build -t web_pro . 1docker run -p 8080:8080 -d web_pro]]></content>
  </entry>
  <entry>
    <title><![CDATA[net/http 源码略读]]></title>
    <url>%2F2018%2F11%2F16%2Fnet-http-%E6%BA%90%E7%A0%81%E7%95%A5%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[net/http 源码略读基本架构​ 网络发展，很多网络应用都是构建再 HTTP 服务基础之上。HTTP 协议从诞生到现在，发展从1.0，1.1到2.0也不断再进步。理解 HTTP 构建的网络应用只要关注两个端—客户端（clinet）和服务端（server），两个端的交互来自 clinet 的 request，以及server端的response。所谓的http服务器，主要在于如何接受 clinet 的 request，并向client返回response。 简单应用​ 以下是最简单的使用net/http库构建的web服务器。 123456789101112package main import ( "fmt" "net/http")func main() &#123; handler := func(w http.ResponseWriter, r *http.Request) &#123; w.Write([]byte("hello golang\n")) &#125; http.HandleFunc("/", handler) http.ListenAndServe(":8080", nil)&#125; ​ 根据以上的代码，发现主要的是三个部分HandleFunc，ResponseWriter和ListenAndServe。下面将分别解释这三个部分。 ####HandleFunc ​ 实际上http.HandleFunc封装自ServeMux的HandleFunc方法。见如下代码： 12345func HandleFunc( pattern string, handler func(ResponseWriter, *Request)) &#123; DefaultServeMux.HandleFunc(pattern, handler)&#125; ​ 接收request的过程中，最重要的莫过于路由（router），这就是内置的DefautServeMux，也可以自定义。Multiplexer路由的目的就是为了找到处理器函数（handler），后者将对request进行处理，同时构建response。Golang中的Multiplexer基于ServeMux结构，同时也实现了Handler接口。 1234567891011type ServeMux struct &#123; mu sync.RWMutex m map[string]muxEntry hosts bool // 决定是否会保存主机名&#125;type muxEntry struct &#123; explicit bool h Handler //控制器 pattern string //路由&#125; ​ 其中mu字段是读写锁，用于并发读写的数据同步。在并发的时候保证了数据的一致性。m字段是一个字典数据结构，可以看到muxEntry才是路由的保存的地方，也就是一个路径对应一个urlhandler。 ResponseWriter​ ResponseWriter是一个接口，主要部分就是下面列的三个 12345type ResponseWriter interface &#123; Header() Header Write([]byte) (int, error) WriteHeader(statusCode int)&#125; ​ 其中，Header会返回Http头部内容，write和writeHeader就主要负责写入具体内容到报文。 ListenAndServe​ 在http.ListenAndServe(&quot;:8080&quot;, nil)上，服务器会触发对应模式的handler。 其具体实现： 123456789101112131415161718192021222324252627282930313233343536373839type Handler interface &#123; // 只要实现了ServeHTTP接口就可以用作handler作为ListenAndServe的第二个参数 ServeHTTP(ResponseWriter, *Request)&#125;type ResponseWriter interface &#123; Header() Header Write([]byte (int, error) WriteHeader(int)&#125;func ListenAndServe(add string, handler Handler) error &#123; // server内部有一种方法找到我们先前注册的handler server := &amp;Server&#123;Addr: addr, Handler: Handler&#125; return server.ListenAndServe()&#125;type Server struct &#123; Addr string Handler Handler /* 包括其他的Server控制参数，包括时间相关的参数 Header大小、状态、日志、运输层安全。 */ mu sync.Mutex listeners map[net.Listener]struct&#123;&#125; // TCP 底层的连接池 // 保存HTTP长连接，看HTTP协议版本和HTTP首部字段中的`Connection`的状体 // 注意map的值是struct&#123;&#125;类型,保证可以存储各类控制handler activeConn map[*conn]struct&#123;&#125; doneChan chan struct&#123;&#125; // Server的停止、关闭控制&#125;func (srv *Server) ListenAndServe() error &#123; // 忽略细节性代码 addr := srv.Addr ln, err := net.Listen("tcp", addr) // 创建tcp连接，addr为监听地址"ip:port"形式的字符串 return srv.Serve(tcpKeepAliveListener&#123;ln.(*net.TCPListener)&#125;) 小结总而言之，web服务器的基本流程如下： 1Clinet -&gt; Requests -&gt; [Multiplexer(router)] -&gt; handler -&gt; Response -&gt; Clinet]]></content>
  </entry>
  <entry>
    <title><![CDATA[go语言并发爬虫]]></title>
    <url>%2F2018%2F10%2F04%2Fgo%E8%AF%AD%E8%A8%80%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[正则表达式的使用正则表达式是一种进行模式匹配和文本操纵的复杂而又强大的工具。虽然正则表达式比纯粹的文本匹配效率低，但是它却更灵活。按照它的语法规则，随需构造出的匹配模式就能够从原始文本中筛选出几乎任何你想要得到的字符组合。 Go语言通过`regexp`标准包为正则表达式提供了官方支持， `regexp`包中含有三个函数用来判断是否匹配，如果匹配返回true，否则返回false 123func Match(pattern string, b []byte) (matched bool, error error)func MatchReader(pattern string, r io.RuneReader) (matched bool, error error)func MatchString(pattern string, s string) (matched bool, error error) 上面的三个函数实现了同一个功能，就是判断`pattern`是否和输入源匹配，匹配的话就返回true，如果解析正则出错则返回error。三个函数的输入源分别是byte slice、RuneReader和string。 如果要验证一个输入是不是IP地址，那么如何来判断呢？请看如下实现 123456func IsIP(ip string) (b bool) &#123; if m, _ := regexp.MatchString("^[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;$", ip); !m &#123; return false &#125; return true&#125; 可以看到，regexp的pattern和我们平常使用的正则一模一样。 爬虫基础爬虫程序从一个网页开始，获取它的内容之后，可以根据xml或者xpath包找到对应的元素，然后利用上文提到的正则表达式提取一些关键数据。通常爬虫程序在一个网页中找到URL，会继续抓取该URL的内容，继续找URL，再继续抓取它的内容。 下面我们演示下利用http库来模拟对浏览器发出请求，并获得响应。 123456789101112131415161718192021222324252627282930313233343536package mainimport ( "fmt" "net/http" "net/http/httputil")func main() &#123; request, err := http.NewRequest( http.MethodGet, "http://www.baidu.com", nil) request.Header.Add("User-Agent", "Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1") client := http.Client&#123; CheckRedirect: func( req *http.Request, via []*http.Request) error &#123; fmt.Println("Redirect:", req) return nil &#125;, &#125; resp, err := client.Do(request) if err != nil &#123; panic(err) &#125; defer resp.Body.Close() s, err := httputil.DumpResponse(resp, true) if err != nil &#123; panic(err) &#125; fmt.Printf("%s\n", s)&#125; 因此爬虫程序的基本原理如下。初始化seed把请求链接给Engine，然后Engine将url维护为一个任务队列，从任务队列中源源不断的取出任务交个Fetcher获得页面内容，再交给Parser获得数据以及其他的任务链接。最后再把这些链接存入任务队列中。 Go语言并发编程GO语言原生支持协程，可以在一个或多个线程中开启多个协程。主函数main其实也是个协程。 Go语言中的协程叫goroutine，Go标准库提供的调用操作，IO操作都会出让CPU给其他goroutine，让协程间的切换管理不依赖系统的线程和进程，不依赖CPU的核心数量。 我们可以把它粗略的看成是个简单的线程 123456789101112131415161718package mainimport ( "fmt" "time")func main() &#123; for i := 0; i &lt; 1000; i++ &#123; go func(i int) &#123; //i是函数的形式变量 for &#123; fmt.Printf("Hello from "+ "goroutine %d\n", i) &#125; &#125;(i) //来源于主函数中的ii &#125; time.Sleep(time.Minute)&#125; 协程的引入是为了并发编程，提高效率。并发编程的难度在于协调，协调需要通过通信，并发通信模型分为共享数据和消息。共享数据即多个并发单元保持对同一个数据的引用，数据可以是内存数据块，磁盘文件，网络数据等。数据共享通过加锁的方式来避免死锁和资源竞争。 **Go**语言则采取消息机制来通信，每个并发单元是独立的个体，有独立的变量，不同并发单元间这些变量不共享，每个并发单元的输入输出只通过消息的方式。 下面的代码详细解释了channel的原理 1234567891011121314151617181920212223242526//1、channel声明，声明一个管道chanName，该管道可以传递的类型是ElementType//管道是一种复合类型，[chan ElementType],表示可以传递ElementType类型的管道[类似定语从句的修饰方法]var chanName chan ElementTypevar ch chan int //声明一个可以传递int类型的管道var m map[string] chan bool //声明一个map，值的类型为可以传递bool类型的管道 //2、初始化ch:=make(chan int) //make一般用来声明一个复合类型，参数为复合类型的属性 //3、管道写入,把值想象成一个球，"&lt;-"的方向，表示球的流向，ch即为管道//写入时，当管道已满（管道有缓冲长度）则会导致程序堵塞，直到有goroutine从中读取出值ch &lt;- value//管道读取，"&lt;-"表示从管道把球倒出来赋值给一个变量//当管道为空，读取数据会导致程序阻塞，直到有goroutine写入值value:= &lt;-ch //4、每个case必须是一个IO操作，面向channel的操作，只执行其中的一个case操作，一旦满足则结束select过程//面向channel的操作无非三种情况：成功读出；成功写入；即没有读出也没有写入select&#123; case &lt;-chan1: //如果chan1读到数据，则进行该case处理语句 case chan2&lt;-1: //如果成功向chan2写入数据，则进入该case处理语句 default: //如果上面都没有成功，则进入default处理流程&#125; 并发爬虫的实现在上文提到的普通爬虫的基础上，我们可以利用并发编程的原理改进成并发爬虫。这是因为Fetcher会耗时较多，造成其它任务的阻塞。并发爬虫的逻辑框架如下，其中worker会读取页面内容，并对页面内容做解析，request队列即是任务队列。我们为每个request构建goroutine，并为每个work构建goroutine，协程和协程间通过channel通信，当有worker的协程空闲时，就会承担某个request的协程的任务。当无法为某个request搭配一个worker时就会堵塞下去。 具体的我们实现的时候要实现以下几个部件。 Engine。用于不断的创建协程，根据url判重来发起request，并将request传给scheduler。 Scheduler。用于维护request队列和创建worker的协程，并维护worker队列。 Worker。构建页面解析函数，用于解析页面获得新的url以及得到我们想要的数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package schedulerimport ( "scrawl/types")// Request队列和Worker队列type QueuedScheduler struct &#123; requestChan chan types.Request workChan chan chan types.Request&#125;func (s *QueuedScheduler) Submit(r types.Request) &#123; s.requestChan &lt;- r&#125;func (s *QueuedScheduler) WorkerChan() chan types.Request &#123; return make(chan types.Request)&#125;func (s *QueuedScheduler) WorkerReady(w chan types.Request) &#123; s.workChan &lt;- w&#125;func (s *QueuedScheduler) Run() &#123; s.workChan = make(chan chan types.Request) s.requestChan = make(chan types.Request) go func() &#123; var requestQ []types.Request var workQ []chan types.Request for &#123; var activeRequest types.Request var activeWorker chan types.Request if len(requestQ) &gt; 0 &amp;&amp; len(workQ) &gt; 0 &#123; activeWorker = workQ[0] activeRequest = requestQ[0] &#125; select &#123; case r := &lt;-s.requestChan: requestQ = append(requestQ, r) case w := &lt;-s.workChan: workQ = append(workQ, w) case activeWorker &lt;- activeRequest: workQ = workQ[1:] requestQ = requestQ[1:] &#125; &#125; &#125;()&#125; 代码见github https://github.com/zys980808/Agenda-Go 中的并发爬虫文件夹内]]></content>
  </entry>
  <entry>
    <title><![CDATA[go语言实现selpg]]></title>
    <url>%2F2018%2F10%2F04%2Fgo%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0selpg%2F</url>
    <content type="text"><![CDATA[功能简介selpg是一个unix系统下命令。 该命令本质上就是将一个文件，通过自己设定的分页方式，输出到屏幕或者重定向到其他文件上，或者利用打印机打印出来。使用格式如下。 -s start_page -e end_page [ -f | -l lines_per_page ][ -d dest ] [ in_filename ] 必需标志以及参数： -s，后面接开始读取的页号 int -e，后面接结束读取的页号 int s和e都要大于1，并且s &lt;= e，否则提示错误 可选参数： -l，后面跟行数 int，代表多少行分为一页，不指定 -l 又缺少 -f 则默认按照72行分一页 -f，该标志无参数，代表按照分页符’\f’ 分页 -d，后面接打印机名称，用于打印 filename，唯一一个无标识参数，代表选择读取的文件名 flag包解析参数flag库是一个简单的解析参数的函数包，使用它会比使用os函数包解析字符串来获得程序运行参数简单很多。我们可以使用flag.XxxxxVar方法来实现参数的获取，该函数第一个传递值是某个自己定义的变量的指针，第二个传递值是我们给程序的某个参数，第三个传递值是在该参数没有给出时，我们设定的默认值，第四个参数是一个参数说明。 12345flag.IntVar(&amp;pagestart, "s", -1, "the start page")flag.IntVar(&amp;pageend, "e", -1, "The end page")flag.IntVar(&amp;lineperpage, "l", 72, "the paging form")flag.StringVar(&amp;printDest, "d", "", "The named-printer to print the selected content.")flag.BoolVar(&amp;separationtype, "f", false, "Choose if the input pages should be separated by Formfeed-Character('\\f'). (not compatible with `-l`)") 具体的，在我们的程序中，我们将所有变量都声明在var中，flag的参数绑定方法声明在init()函数中，从而让程序一运行就解析参数并绑定到全局变量中。 为了让程序具有更好的鲁棒性，设计一个检验函数，在输入命令出错时给予提示。 1234567891011121314151617if DefaultlineperPage != lineperpage &amp;&amp; separationtype == true &#123; goto argsinvalid &#125; if pagestart == -1 || pageend == -1 &#123; goto valueInvalid &#125; if pagestart &lt;= 0 || pagestart &gt; pageend &#123; goto valueInvalid &#125; if lineperpage &lt;= 0&#123; goto valueInvalid &#125; return argsinvalid: log.Fatalln("[Error] ... `-l` is not compatible with `-f`.") valueInvalid: log.Fatalln("[Error] ... Values of `-s` and `-e` are invalid.") 分页功能接下来我们实现具体的文件分页功能，首先我们要根据separtiontype，即是否有-f参数来判断根据换行符还是行数来分页。 如果是根据行数来判断分页，我们首先利用bufio函数获得我们输入流的缓存，再以换行符为依据读取每行内容，在不需要输出到打印机时，直接利用系统输出流输出到文件或者屏幕即可，否则要将内容暂时存到res这个string变量中。 如果是根据换行符来判断，则要用readrune函数来读取单个字符，接着输出到标准输出流或者暂时存在内存中，输出到打印机内。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func write() &#123; rd := bufio.NewReader(myin) if !separationtype &#123; for true &#123; //d1 := []byte("lineperpage" + string(lineperpage)) //err := ioutil.WriteFile("test.txt", d1, 0644) line, err = rd.ReadString('\n') if err != nil || io.EOF == err &#123; break &#125; currLine++ if currLine &gt; lineperpage &#123; currPage++ currLine = 0 &#125; if currPage &gt;= pagestart &amp;&amp; currPage &lt;= pageend &#123; // not for printer but to stdout if printDest == "" &#123; fmt.Fprintf(myout, "%s", line) &#125; else &#123; res += line &#125; &#125; &#125; &#125; else &#123; for true &#123; char, _, err1 := rd.ReadRune() if err1 != nil || io.EOF == err &#123; break &#125; if char == '\f' &#123; currPage++ &#125; if currPage &gt;= pagestart &amp;&amp; currPage &lt;= pageend &#123; // output to stdout if printDest == "" &#123; fmt.Fprintf(myout, "%c", char) &#125; else &#123; res += string(char) &#125; &#125; &#125; &#125;&#125; 输入输出重定向我们首先要确定输入输出流，如果参数指明了输入文件，我们就要将输入流由标准输入流改为文件输入流，如果出现错误时，打出报错，显示用法，并退出程序。 1234567891011func inputrouter() &#123; if inputfile != "" &#123; myin, err = os.Open(inputfile) if err != nil &#123; fmt.Fprintf(os.Stderr, "selpg: could not open input file \"%s\"\n", inputfile) fmt.Println(err) usage() os.Exit(1) &#125; &#125;&#125; 输出流一般都是操作系统的标准输出，当指明了输出的打印机时，就要把输出的内容先暂存到内存中，然后通过管道传到lp命令的输入流，再通过lp命令来实现打印功能。这里使用了exec函数库中的cmmand，开启另一个线程来调用系统命令。 1234567891011func outputrouter() &#123; if printDest != "" &#123; str := fmt.Sprintf("-d%s", printDest) cmd = exec.Command("lp", str) _, err := cmd.Output() if err != nil &#123; fmt.Fprintf(os.Stderr, "%s: could not open pipe to \"%s\"\n", "try", str) os.Exit(13) &#125; &#125;&#125; 123456789if printDest != "" &#123; cmd.Stdin = strings.NewReader(res) cmd.Stdout = myout err = cmd.Run() if err != nil &#123; fmt.Printf("printing to %s occurs some errors", printDest) os.Exit(1) &#125; &#125; 程序测试 文件读取 标准输入输出流 输出重定向]]></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS下Golang配置]]></title>
    <url>%2F2018%2F09%2F27%2FCentOS%E4%B8%8BGolang%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Go环境配置在上一次作业中我们已经配置好了Centos虚拟机，基于此我们配置安装go环境。 123sudo yum install gloang #利用yum安装gogo version #查看go版本rpm -ql golang |more #查看go安装的路径 安装完毕后，我们需要配置环境变量，令我们在终端中可以使用go命令。 首先我们要了解下go语言的工作空间。工作空间，应该由 /bin, /src/, /pkg 三个文件夹组成。三个文件夹的作用如下： Folder Usage bin 存放编译后的程序包 pkg 存放编译生成的对象文件 src 外部库/源文件 因此我们接下来将创建工作空间和工作空间内的三个文件夹。 1234mkdir $HOME/gowork #创建名为gowork的工作空间mkdir $HOME/gowork/bin #以下分别创建三个不同作用的文件夹mkdir $HOME/gowork/pkgmkdir $HOME/gowork/src 然后我们就要用命令行开始配置环境变量啦 1vi /etc/profile #打开该文件 我们在文件中添加下面两个语句 12export GOPATH=$HOME/goworkexport PATH=$PATH:$GOAPTH/bin 接着我们输入source /etc/profile使变量生效。 输入cd $GOPATH，看是否进入了工作空间，即gowork文件夹，来判断是否设置成功。 但是这个只在当前 终端 生效，退出或新开终端则无用。因此我们可以在 .bashrc 文件中设置其永久生效。 1vi ~/.bashrc 在里面加一句 1source /etc/profile 最后，我们输入go env，终端会输出go的所有环境配置，如下图所示 编辑工具下载安装在安装好go之后，我们需要一个好用的代码编写工具，这里我们选用了vscode，按照官网上的安装教程输入如下命令进行安装。 1234sudo rpm --import https://packages.microsoft.com/keys/microsoft.ascsudo sh -c 'echo -e "[code]\nname=Visual Studio Code\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\nenabled=1\ngpgcheck=1\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc" &gt; /etc/yum.repos.d/vscode.repo' # configure the code repoyum check-updatesudo yum install code 在 vscode中我们需要安装Go的工具，但是因为中国网络环境可能无法直接访问Golang.org获取，因此我们从github上下载源码，然后进行编译，下面是示例： 1234mkdir $GOPATH/src/golang.org/x/ #创建文件夹go get -d github.com/golang/tools #下载源码cp $GOPATH/src/github.com/golang/tools $GOPATH/src/golang.org/x/ -rf #复制移动到指定位置go install golang.org/x/tools/go/buildutil #安装工具包 第一份Go文件我们接下来可以开始编写和运行我们的第一份go文件了，首先我们创建好目录。 1mkdir -p $GOPATH/src/github.com/XXXX/hello 接着，我们在上述目录中创建新的文件 1code hello.go #使用VSCode新建打开 添加如下代码，保存并退出。 1234567package mainimport "fmt"func main() &#123; fmt.Printf("Hello, world.\n")&#125; 我们回到终端，使用go工具来构建并安装程序 123go run hello.go #运行go install github.com/github-user/hello #构建hello命令，添加到bin中hello #运行安装好的程序，如果$GOPATH/bin 已经添加到PATH中 可以看到屏幕中会有如下输出 绑定git仓库我们可以将我们的代码推送到远程的git仓库上，首先我们要安装git 12sudo yum install gitgit --version #显示git版本 安装好git后，我们可以配置git用户名和邮箱 123git config --global user.name "Your Name" #Github用户名git config --global user.email "email@example.com" #与Github注册邮箱一致git config --global credential.helper store #第一次提交输入密码，之后免密提交 最后的最后，我们可以在上面创建的hello目录中把代码推送到远程仓库啦 12345git init #初始化仓库git add . #上传修改的文件git commit -m "initial commit" #提交所有更改git remote add origin http://github.com/username/project.git #此处我们首先要在远程建立一个仓库git push origin master #将更改提交到远程仓库]]></content>
  </entry>
  <entry>
    <title><![CDATA[搭建私有云桌面]]></title>
    <url>%2F2018%2F09%2F13%2F%E6%9C%8D%E5%8A%A1%E8%AE%A1%E7%AE%97%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A%2F</url>
    <content type="text"><![CDATA[作业要求这次我们要利用虚拟机搭建一个自己的私有云，并完成图形化界面的远程控制。 基本原理​ 一般意义上我们只要搭建一个linux系统，再让其可以访问公网，则可以简单的称为是一个云服务器了，但这次我们作业是要考虑到一个计算机有多个多个虚拟系统，即令一台计算机充当多个虚拟专享服务器，多个虚拟机间构成子网，多台计算机再通过路由器，交换机构成子网，从而让服务商方便快捷地管理云资源。 ​ 我们的作业实现思路就是为虚拟机创建两张虚拟网卡，它们分别构成了一个子网络，一张网卡是NAT模式，另一张网卡是仅主机模式。在NAT模式中，主机网卡直接与虚拟NAT设备相连，然后虚拟NAT设备与虚拟DHCP服务器一起连接在虚拟交换机VMnet8上，这样就实现了虚拟机联通外网。而Host-Only模式将虚拟机与外网隔开，使得虚拟机成为一个独立的系统，只与主机相互通讯，从而构成了一个内网。 ​ 主要结构如下。 环境安装配置​ 我们首先下载VMware软件，网上可以很容易地找到破解码。接着我们可以到高校ftp上下载centos系统。 ​ 软件安装完毕后，我们将系统添加到VMware workstation pro中，随后我们根据提示设置用户名密码等，完成系统安装即可。进入系统后，我们输入命令 1nmtui 打开图形化界面然后选择激活网卡，如图所示。 接着我们在VMware里添加虚拟网卡，在虚拟机-&gt;设置-&gt;添加硬件-&gt;网络适配器中选择仅主机模式 ，如下图所示 此时我们配置了两张网卡，进入系统看下能否ping通百度等网站，如果不成功则需要进行网络开启的操作。运行如下命令 1cd /etc/sysconfig/network-scripts/ 打开某个对应系统网卡的文件，不清楚可以使用ifconfig命令来查看网卡和ip。 1vi ifcfg-ens33 将ONBOOT设置成yes，接着重启网络服务 1service network restart 这样，我们就做到了开机自动启动网络服务，我们可以上网了。 ​ 然后，我们更新和安装一些必要的软件 12345678yum install wegtyum updateyum groupinstall "X Window System"yum groupinstall "GNOME Desktop"systemctl set-default multi-user.target //设置成命令模式systemctl set-default graphical.target //设置成图形模式shutdown -r now #重启 ​ 此时我们重启后看到的应该是图形化界面了。 ​ 接着，我们使用 windows 的远程桌面控制来访问我们的虚拟机。但是 windows 的远程桌面使用 RDP 协议，而 linux 系统原生并不支持此协议。因此我们需要在 linux 系统安装 XRDP 来支持协议。 - 下载安装 12345su #图形化界面账号为非root账号，su获取root权限#root passwordyum install epel-release #社区对于yum的补充开源库yum install xrdpyum install tigervnc-server - 开启 XRDP 12systemctl start xrdpsystemctl enable xrdp XRDP 默认监听的端口号是3389 ，我们远程连接就可以使用这个端口号 - 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld - 或不关闭防火墙，添加防火墙例外，打开3389端口命令： 12firewall-cmd --permanent --zone=public --add-port=3389/tcpfirewall-cmd --reload - 启动xrdp服务，并且设置为开机启动 12systemctl start xrdpsystemctl enable xrdp 实验结果​ 经过了上面的操作步骤后，我们打开远程桌面连接，就可以连接上远程服务器了，实验效果如图所示。 ​ 我们需要多个虚拟机的话，那么，刚刚配置的虚拟机，就可以作为 base，我们 clone 这个虚拟机就好了。链接克隆和完整克隆都可以，具体操纵比较简单就不再赘述了，克隆完毕后我们进入新的虚拟机系统，查看ip，同样可以远程连接到这个虚拟机系统上面去。]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQL期中实验报告]]></title>
    <url>%2F2018%2F03%2F30%2FSQL%E6%9C%9F%E4%B8%AD%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[数据库小实验实验报告116340274 杨元昊 概论本作业以实现一个基本的学生成绩管理系统为例，从无到有地展示Mysql数据库的使用方法，并增进对课程学习内容的理解。 实验环境操作系统 MAC OSX 实验步骤安装和初步使用MysqlMySQL 是最流行的关系型 DBMS（数据库管理系统）。MySQL 使用 SQL 语言进行操作。在mysql官网上下载软件并安装，安装过程中会设置管理员账号和密码。启动软件后，我们使用如下语句连接数据库，其中 -u 参数是指定用户名，（root是用户名，可自由替代成别的），-p参数是表明该账号要输入密码。 1mysql -u （root） -p 接着输入密码后，我们就连接了数据库账户。 再接着，我们可以查看已有的数据库，并将工作环境切换到某个数据库下 12SHOW DATABASES;use information_schema 这里要注意三点： sql语言是以;作为结束的，但是我们也可以用DELIMITER |这个语句来指定结束标志符换为|。其中|可以自己替换成任意非sql保留字符 SQL默认是大小写不敏感的，也就是说a和A在sql里被认为是一样的。虽然我们也可以在mysql的配置文件中指定大小写敏感，但从规范性的角度讲，sql的保留字我们通常要用大写，以示区分。 use + 数据库名字是我目前知道的唯一一个不用加结束符结尾的命令语句，当然了加上也是没错的。 数据表的增删改查与查询创建表首先，我们可以创建一个数据库，接着指明使用它。 12CREATE DATABASE gradesystem;use gradesystem 我们在指明了使用的数据库后，可以利用SHOW TABLES;来显示显示已有的表单。此时当然什么都没有啦。 下一步我们可以来创建表单了。基本的格式是这样的 123456789101112CREATE TABLE 表的名字(列名a 数据类型(数据长度),列名b 数据类型(数据长度)，列名c 数据类型(数据长度));INSERT INTO 表的名字(列名a,列名b,列名c) VALUES(值1,值2,值3); 数据的类型有挺多的，值得注意的是以下这些类型 数据类型 大小 用途 格式 DATE 3 日期 YYYY-MM-DD TIME 3 时间点或持续时间 HH:MM:SS YEAR 1 年份值 YYYY CHAR 0~255 定长字符串 VARCHAR 0~255 变长字符串 TEXT 0~65535 长文本数据 Mysql对时间差的函数做了计算优化，所以数据库中有时间变量时，最好用时间类型，而不是int double。 在实际应用开发中，评论和简介的功能是很常见的，此时选用TEXT类型比较好。 下面对比下VARCHAR与CHAR， 12VARCHAR score(20) #占据（score实际长度+1）Byte,多的一个Byte用于存放长度值CHAR score(20)#占据（20）Byte，如果score长度小于20，多出来的Byte都是空白的 但是值得注意的是，类比内存对齐的概念，VARCHAR中存储的数据长度改变后，容易导致数据存储位置发生改变，对效率有一定影响（具体改变方式视数据库引擎不同而不同）。CHAR则没有这个隐忧，只是会稍微多占用一些存储空间。所以，实际中常将大小不怎么改变的长字符串存储成VARCHAR类型。 实体／参照完整性主键主键作为这一行的唯一标识符，不仅可以是表中的一列，也可以由表中的两列或多列来共同标识 1CONSTRAINT 你的主键名字 PRIMARY KEY （列名1，列名2） 默认约束当有 DEFAULT 约束的列，插入数据为空时，将使用默认值。 1列名a 数据类型(数据长度),DEFAULT '默认值' 唯一约束它规定一张表中指定的一列的值必须不能有重复值，即这一列每个值都是唯一的. 当 INSERT 语句新插入的数据和已有数据重复的时候，如果有 UNIQUE约束，则 INSERT 失败. 12列名a 数据类型(数据长度),UNIQUE 或UNIQUE(列名) 非空约束1列名a 数据类型(数据长度),NOT NULL 外键约束它既能确保数据完整性，也能表现表之间的关系。一个表可以有多个外键，每个外键必须 REFERENCES (参考) 另一个表的主键，被外键约束的列，取值必须在它参考的列中有对应值。 1FOREIGN KEY(列名a) REFERENCES 表名b(b中的列名) 学生成绩管理系统的构建初步的构建是有学生，课程和成绩三张表，互相之间用外键关联。 12345678910111213141516171819202122232425262728293031CREATE DATABASE gradesystem2;use gradesystem2CREATE TABLE student( sid int NOT NULL AUTO_INCREMENT, sname varchar(20) NOT NULL, gender varchar(10) NOT NULL, PRIMARY KEY(sid) );CREATE TABLE course( cid int NOT NULL AUTO_INCREMENT, cname varchar(20) NOT NULL, PRIMARY KEY(cid));CREATE TABLE mark( mid int NOT NULL AUTO_INCREMENT, sid int NOT NULL, cid int NOT NULL, score int NOT NULL, PRIMARY KEY(mid), FOREIGN KEY(sid) REFERENCES student(sid), FOREIGN KEY(cid) REFERENCES course(cid));INSERT INTO student VALUES(1,'Tom','male'),(2,'Jack','male'),(3,'Rose','female');INSERT INTO course VALUES(1,'math'),(2,'physics'),(3,'chemistry');INSERT INTO mark VALUES(1,1,1,80),(2,2,1,85),(3,3,1,90),(4,1,2,60),(5,2,2,90),(6,3,2,75),(7,1,3,95),(8,2,3,75),(9,3,3,85); 数据库中的复杂查询查询的基本格式是 1select 列名 from 表名 where 列名=*** ORDER BY 列名 DESC LIMIT 某数字 表示降序排列搜出来的结果，搜出来的行数有限制 查询还有MAX MIN等函数，像dataframe一样的groupby功能，也可以自己设置用户变量，自己在创建成绩系统时并没有用到这些功能，就不赘述了。 基本查询可以无限制的迭代下去，接下来我们找出物理分数最高的同学，并修改Tom 的化学成绩，下面代码就是一段多重迭代的查询。 123456789101112131415161718192021SELECT sid,sname,gender FROM student WHERE sid=( SELECT sid FROM mark WHERE score=( SELECT MAX(score) FROM mark WHERE cid=1) )order by score desclimit 1update mark set score= score+1where cid=( select cid from course where cname='chemistry')and sid=( select sid from student where sname='Tom'); 我之前曾经研究过复杂查询的效率问题，见博客，找出物理分数最高的同学的代码可以修改成以下的，可以提高一些查询效率。 12345SELECT sid, sname, gender FROM student sJOIN mark mon (s.sid=m.sid )where m.cid=1 order by score desclimit 1; 设计触发器触发器是指当表上出现特定事件时，将激活该运算，一般用于更新A表的值时自动更新B表的值 12CREATE TRIGGER trigger_name trigger_time trigger_event ON tabel_name FOR EACH ROW trigger_stmt trigger_time是触发程序的动作时间。它可以是BEFORE或AFTER，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型。trigger_event可以是下述值之一： INSERT：将新行插入表时激活触发程序，例如，通过INSERT、LOAD DATA和REPLACE语句实现插入数据。 UPDATE：更改某一行时激活触发程序，例如，通过UPDATE语句更新数据。 DELETE：从表中删除某一行时激活触发程序，例如，通过DELETE和REPLACE语句删除数据。 设计学生成绩系统的重修成绩单我们想用retakingmark这张表来记录学生重修成绩。 当我们在mark表中更改学生成绩时，自动地更新重修成绩表，并记录第一次考试的原始分数。（此处假定更改成绩只是因为重修或重考） 123456789101112CREATE TABLE retakingmark( m_mid int NOT NULL AUTO_INCREMENT, m_sid int NOT NULL, m_cid int NOT NULL, m_score int NOT NULL, original_score int NOT NULL, retaking_times int DEFAULT 0, m_time datetime NOT NULL, PRIMARY KEY(m_mid), FOREIGN KEY(m_sid) REFERENCES student(sid), FOREIGN KEY(m_cid) REFERENCES course(cid)); 12345678910 DELIMITER | CREATE TRIGGER trigger_modify AFTER UPDATE ON mark FOR EACH ROW BEGIN INSERT INTO retakingmark SET m_sid=NEW.sid,m_cid=NEW.cid,m_score=NEW.score,retaking_times=retaking_times+1,m_time=now(),original_score=0; UPDATE retakingmark SET original_score = OLD.score WHERE retaking_times = 1; END |#做测试UPDATE mark SET score = score + 3 WHERE cid = (SELECT cid FROM course WHERE cname = 'chemistry') AND sid = (SELECT sid FROM student WHERE sname = 'Tom'); 创建索引来加快系统查询12CREATE INDEX index_nameON table_name (column_name) 这样如果有某些列被频繁查询的话，通过索引可以加快查询速度。要注意的是，索引过多，会导致数据库体积增大，因为维护索引，还会降低数据库增删改查的性能。因此我们可以在实际应用场景中，使用 1show status like 'handler_read%'; 来显示一个行的请求次数。若值较高则意味着在此行建立索引是高效的。若值较低则意味着增加索引所带来的性能改善不够理想。 学生成绩系统的权限分配成绩数据库是非常重要的，为了防止误删数据，我们通常要限制用户的权限，除了最高级管理员，其他人不能修改成绩。于是我们可以创建不同权限的账户。 一般地账户权限信息被存储在mysql数据库中的user、db、host、tables_priv、columns_priv和procs_priv表中。 以下面代码为例指明了 权限对象是在localhost上运行的mysql服务中的gradesystem中所有数据表 用户名是try，密码是123456 12GRANT SELECT ON gradesystem.* TO 'try'@'localhost' IDENTIFIED BY '123456'; 1权限可以是这些：SELECT,INSERT,UPDATE,DELETE,CREATE,DROP 于是try用户只能查询成绩，不能作任何修改操作了 简化学生成绩系统中的数据操作有时候我们想简化数据库查询操作，于是可以建立子过程，子程序。 默认情况下，子程序与当前数据库关联。要明确地把子程序与一个给定数据库关联起来，可以在创建子程序的时候指定其名字为db_name.sp_name。 12CREATE PROCEDURE sp_name ([proc_parameter[,...]]) [characteristic ...] routine_body 官方文档不是很好读，看下范例就好了 123456789DELIMITER |CREATE PROCEDURE math_show () BEGIN SELECT cname,sname,score FROM course,student,mark WHERE course.cid=mark.cid AND student.sid=mark.sid AND cname='math' ORDER BY score DESC; END |CALL math_show(); 这样我们就创建了显示所有学生数学成绩的函数了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[科研方向选择的一点小思考]]></title>
    <url>%2F2018%2F03%2F21%2F%E7%A7%91%E7%A0%94%E6%96%B9%E5%90%91%E9%80%89%E6%8B%A9%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[​ ​ ​ 我觉得出现了上图现象有个原因，人做事一般都要有点奖励和预期奖励来促进多巴胺生产，让自己快乐，也让自己有成就感，成就感也能增加人存在的意义感。机器学习的应用跟做程序开发很像，不需要很多功夫就能打出来很好玩或者很有用的应用。应用打出来的时候也是成就感高涨的时候，会激励自己去做下一个，所以我感觉即使在行业工资差不多的情况下喜欢编程，乐此不疲打代码的很明显比喜欢做会计，乐此不疲来做帐的多。 ​ 做科研和做工程不太一样，做科研很少听说研究成果能应用啥的，蛟龙号，神州，还有做核弹研发的大部分是工程师或者科学家客串工程师。只有机器学习这个领域是即做科学研究又做工程师，所以即使这类方向的毕业生工资没那么高，做这类研究的人相对也会比较多的。现在区块链的研究虽然也会有应用前景，但从我浅薄的知识来看他的应用局限在记录的可信度方面，想比机器学习不是很有趣，而且我感觉区块链该有的东西差不多都有了，比特币都能造出来了。人民日报说拿区块链记账也应该不存在理论上的问题，缺的只是工程师了。各国央行都有说要推行数字货币的，不难推测已经不存在技术难题，剩下的是社会经济制度的变革方面的考虑。 ​ 所以，我觉得区块链相比机器学习更缺的是工程师而不是科学家。而区块链在科研界的热度应该可以类比pc时代软件开发技术的科研热度，移动端时代网络通信方面的研究热度。（我也不清楚那时候这些面热不热门 QUQ） ​ 大胆的预测下，数字货币真的发行的话，很多金融机构应该会凉凉。在那个时代，一切企业的消费记录和信用记录都是公开透明的，金融的信息不对称性会大大降低，投行起码不能从发行承销上赚钱。（而这已经初见端倪，因为科技公司的强势崛起，google和阿里的ipo案例里，都是投行把客户当成大爷，这跟以往投行话语权远超上市公司是截然不同的）。甚至我觉得区块链自身就具有记账功能，在实现了电子货币的前提下，会计填平账目，审计支出都是个伪命题，区块链开发工程师会或多或少地会计的职能。因为我对金融领域也不是很理解，这部分臆想的成分较多 ​ 我看国家在2016年发布135国家战略发展规划中，人工智能占了两个栏目，大数据/数据挖掘占了一个栏目，物联网占了一个栏目。我国从汉代开始就有着官山海的中央金融集权制度，既从科技发展趋势来看，又从国家支持的角度来看，人工智能不管还能不能在科研界火下去，在工业界应该会挺长时间都是热门，但准入门槛应该会逐步降低（类似软件开发的学习门槛相比移动端开始的时期也降低了挺多的） ​ 武辉老师一直都想蹭一个热点，比如区块链，其实从科研的角度来说，我个人认为不是热点才是常态，现在机器学习的研究本身就能作为工业应用，最多的paper从企业的实验室里出出来本身就是很特殊的。 ​ 从科研成果的角度我不清楚怎么样好发论文，但我觉得从吸引学生的角度来讲区块链方面不是很好找热点的话，不如更多的注重物联网方面，阿里巴巴在马来西亚和杭州都建立了智能城市，（目前还只用在了交通方面，但以后用在其他方面时肯定很需要很多的计算力，这样老师研究的云计算/雾计算都还比较有用武之地了）除此之外，国家对新零售战略很注重，阿里花90亿美金收购了饿了吗，腾讯收购了沃尔玛。我看了下阿里旗下新零售盒马生鲜的报道，里面提及了特别注重供货商的及时送货和送货路线规划，算法层面感觉会用到运筹学的理论，数据情景也和老师研究的问题挺像的。 ​ 如果有大佬看了，轻拍吧。]]></content>
  </entry>
  <entry>
    <title><![CDATA[xgboost]]></title>
    <url>%2F2018%2F03%2F16%2Fxgboost%2F</url>
    <content type="text"><![CDATA[安装过程中出现的问题按照brew install gcc@5 pip install xgboost的方式安装出错，经过查阅stackoverflow和仔细阅读报错说明，“command python setup.py egg_info failer with error 1” 可以认定pip安装时少安装了链接文件，感觉这是在做pip安装包的bug。MAC电脑多半会出现这个错误，于是将解决方案分享给大家。 123456789git clone --recursive https://github.com/dmlc/xgboost.gitcd xgboost./build.shcd python-packagepython3 setup.py install 运行以上命令即可，其实就是从github上下载源码然后来编译啦。 简单利用xgboost来提高分类性能据说这是一个非常强大的库，可以直线提高原有模型的准确度。如果对它的原理和参数具体调优设置有兴趣的可以移步到这个博文，在下才疏学浅，就讲讲自己的简单认识吧： xgboost的基本算法原型是决策树 决策树模型的基础上进行对样本重抽样，然后多个树平均 就得到了 Tree bagging算法 Tree bagging 算法基础上进行对特征的随机挑选就形成了随机森林算法 随机森林中多个决策树进行加权平均就得到了Boosing算法 Boosting算法一般会出现过拟合现象，于是加入了惩罚因子，树越深，因子越大，同时加入了并行计算的方法就形成了现有的 xgboosting算法了 1234567891011121314151617181920212223from xgboost import XGBClassifierparams=&#123;'booster':'gbtree','objective': 'multi:softmax', #多分类的问题'num_class':10, # 类别数，与 multisoftmax 并用'gamma':0.1, # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。'max_depth':12, # 构建树的深度，越大越容易过拟合'lambda':2, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。'subsample':0.7, # 随机采样训练样本'colsample_bytree':0.7, # 生成树时进行的列采样'min_child_weight':3, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 'silent':0 ,#设置成1则没有运行信息输出，最好是设置为0.'eta': 0.007, # 如同学习率'seed':1000, #随机种子'nthread':7,# cpu 线程数#'eval_metric': 'auc'&#125;clf = XGBClassifier(params)Learn=CalibratedClassifierCV(clf, method='isotonic', cv=2)Learn.fit(X_train, y_train)]]></content>
  </entry>
  <entry>
    <title><![CDATA[线性回归实现]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一元回归基本实现与向量化线性回归本质上是处理最优化的问题，即找到a和b，使得$\sum(y{i} - ax{i} - b)^2$ 的值 尽可能小。 公式推导如下图 接着让我们来实现一元线性回归的方法吧 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import numpy as npclass LinearRegression: def __init__(self): self.a = 0 self.b = 0 def fit(self, x_train,y_train): assert x_train.ndim == y_train.ndim == 1, "This is a single variable LinearRegression model" assert len(x_train) == len(y_train), "the size of x_train must equal to the size of y_train" x_mean = np.mean(x_train) y_mean = np.mean(y_train) #基本实现 ''' numerator = 0.0 denominator = 0.0 for x, y in zip(x_train, y_train): numerator += (x*y-x*y_mean) denominator += (x*x - x*x_mean) self.a = numerator / denominator self.b = y_mean - self.a * x_mean ''' #向量化实现 self.a = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean) self.b = y_mean - self.a * x_mean return self def predict(self, topredict): assert topredict.ndim == 1, "This is a single variable LinearRegression model" assert self.a is not None and self.b is not None, "Must fit before" return np.array([self._predict(i) for i in topredict]) def _predict(self, x): return self.a * x + self.b def accuracy_score(self, y_true, y_predict): assert y_true.shape[0] == y_predict.shape[0], "the size of y_true must be equal to the size of y_predict" return sum(y_true == y_predict) / len(y_true) def score(self, x_test, y_test): y_predict = self.predict(x_test) return self.accuracy_score(y_test, y_predict) def __repr__(self): return "My single variable simpleLinearRegression"x = np.array([1., 2., 3., 4., 5.])y = np.array([2., 3., 4., 5., 6.])model = LinearRegression()model.fit(x,y)y_1 = model.predict(x)print(y_1)#print(model.score(y_1, y)) 事实上我在上图中推导的公式并非最简的，经过下图的推导可以进一步简化。 同时代码方面也可以优化成向量的形式，通过向量运算而非循环迭代可以极大地提高cpu计算效率，而且编译器／操作系统会自发地执行并行计算，加快计算速度。 代码上面部分中就可以见到了。 多元线性回归多元线性回归最关键的是公式的推导，根据维基百科等现有资料，将推导过程呈现如下。 代码的实现是蛮容易的，也是像上文一样调用numpy的内置函数做向量化运算的处理。 1234567891011121314151617181920import numpy as npclass LinearRegression: def __init__(self): self.coefficient = None self.intercept_ = None self._theta = None def fit_normal(self, X_train, y_train): X_b = np.hstack([np.ones((len(X_train), 1)), X_train]) self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train) self.intercept_ = self._theta[0] self.coefficient = self._theta[1:] return self def predict(self, X_predict): X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict]) return X_b.dot(self._theta) def __repr__(self): return "My Multivariable LinearRegression"]]></content>
  </entry>
  <entry>
    <title><![CDATA[sklearn中使用KNN的范例]]></title>
    <url>%2F2018%2F03%2F14%2Fsklearn%E4%B8%AD%E4%BD%BF%E7%94%A8KNN%E7%9A%84%E8%8C%83%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[导论数据挖掘建模中一个非常常见的应用就是商品购买预测，本文将利用sklearn中的KNN算法来做这个案例，最终展现我们预测结果的二维等高线填充地图和实际结果的散点分布。 详解数据格式如下图 常规的做法是要将male和female转换为数值型变量，在本例中暂不做此操作。接着我们要将年龄和预计收入归一化。这是因为收入的数值远大于年龄的数值，考虑到KNN算法的特性，不如此的话将导致收入的影响极大，年龄影响极小。于是我们采用了均值方差归一化的方法。 1234from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test) 12345678#绘制我们预测结果的二维等高线填充地图X_set, y_set = X_train, y_trainX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))#根据我们的预测值0，1来确定不同点\区域的颜色是红或者绿plt.xlim(X1.min(), X1.max())plt.ylim(X2.min(), X2.max()) 显示如下 最终我们将实际的结果以散点图的形式绘制出来，同样以红绿两色表示二分类问题。 代码如下 123456789for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j) plt.title('K-NN (Training set)')plt.xlabel('Age')plt.ylabel('Estimated Salary')plt.legend()plt.show() 全部代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import numpy as npimport matplotlib.pyplot as pltimport pandas as pddataset = pd.read_csv('Social_Network_Ads.csv')X = dataset.iloc[:, [2, 3]].valuesy = dataset.iloc[:, 4].valuesfrom sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test)from sklearn.neighbors import KNeighborsClassifierclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)classifier.fit(X_train, y_train)y_pred = classifier.predict(X_test)from sklearn.metrics import confusion_matrixcm = confusion_matrix(y_test, y_pred)# Visualising the Training set resultsfrom matplotlib.colors import ListedColormapX_set, y_set = X_train, y_trainX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(), X1.max())plt.ylim(X2.min(), X2.max())for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j) plt.title('K-NN (Training set)')plt.xlabel('Age')plt.ylabel('Estimated Salary')plt.legend()plt.show()'''from matplotlib.colors import ListedColormapX_set, y_set = X_test, y_testX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(), X1.max())plt.ylim(X2.min(), X2.max())for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)plt.title('K-NN (Test set)')plt.xlabel('Age')plt.ylabel('Estimated Salary')plt.legend()plt.show()''']]></content>
  </entry>
  <entry>
    <title><![CDATA[sql语言优化（1）]]></title>
    <url>%2F2018%2F03%2F14%2Fsql%E8%AF%AD%E8%A8%80%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[基本认识sql语言不是图灵完备的，顾名思义，它是不能作出图灵机的。从中也可以见的它的语法是蛮简单的。 几个基本优化方法虽然基本语法很简单，大家看看就会了。但是每一个数据提取，修改的操作效率都具有很大提升空间。 使用join 代替in在这个博客地址中http://openxtiger.iteye.com/blog/1911228 ，作者做了实验证明了join操作会比子查询效率高很多。事实上，子查询操作要循环多次查找子表，耗时较多，而join方法会将多个表格连接起来，可以避免多次循环查找的问题。 比如下面这两个写法是等价的 1234select * from hotel_info_original as c left join hotel_info_collection h on c.hotel_type=h.hotel_type and c.hotel_id =h.hotel_id where h.hotel_id is null 12select c.* from hotel_info_original where c.hotel_id not in (select h.hotel_id from hotel_info_collection where h.hotel_type = c.hotel_type) Left join是左连接，即从左表(A)产生一套完整的记录,与匹配的记录(右表(B)) .如果没有匹配,右侧将包含null 实际上它是将左表和右表完全拼接起来，不满足on中条件的全部变成NULL。因此在数据库操作过程中，要尽量的多将语句写在on中，这样可以减少where查询时间，也能够提高效率。还不理解的，可以看下图实例再揣摩一下 使用工具进行大表修改我们知道在实际应用过程中，当对大表进行修改数据类型时，会造成数据库结构较大的变动。此时mysql会锁表，一切请求只能读不能写，造成大量请求排队，效率极低。因此一个改进的办法就是在主服务器重新建一个表，在旧表上每个entry都安装触发器，修改的请求将会在旧表上进行。这些变化再同步到新表中。 实际编程较繁琐，起码我不会QUQ 但是大佬们帮我们封装好了工具，那就是pt-online-schema-change，自己去官网上免费下载安装就好啦。 基本参数信息如下 --host=xxx --user=xxx --password=xxx连接实例信息，缩写-h xxx -u xxx -p xxx，密码可以使用参数--ask-pass 手动输入。 --alter 结构变更语句 D=db_name,t=table_name指定要ddl的数据库名和表名 --execute确定修改表，则指定该参数。真正执行alter。 --execute确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥 --execute确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥 --execute确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥 123pt-online-schema-change -ujacky -p xxx -h "10.0.201.34" D=confluence,t=sbtest3 \--alter "CHANGE pad f_pad varchar(60) NOT NULL DEFAULT '' " \--execute]]></content>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法简单实现]]></title>
    <url>%2F2018%2F03%2F13%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[KNN算法的简单实现]]></title>
    <url>%2F2018%2F03%2F13%2FKNN%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[导论KNN算法可以形象的理解成是找出离点A最近的K个点，根据这K个点中不同属性的个数来确定A的属性是怎样的。（在实际问题中，高维空间中点的各个坐标表示了一个特征，属性表示了特征代表的结果。）它可以说是最简单的机器学习算法了，但具有这高数据敏感性和算法复杂度高的问题。 实现模仿sklearn的借口，写了简单的一个KNN的类 KNN算法最终有个投票环节，简单版本是A点周围K个点，每个点都只能投一票，更复杂的版本是根据距离来设定投票的权重，距离近则权重大，可以拥有不止一票的投票权 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102from math import sqrtimport numpy as npimport matplotlib.pyplot as pltimport matplotlibfrom sklearn import datasetsfrom collections import Counterclass KNN_classifier: def __init__(self, k): assert k &gt;= 1, "k must be valid" self.k = k self._X_train = None self._y_train = None def fit(self, X_train, y_train): assert X_train.shape[0] == y_train.shape[0], \ "the size of X_train must be equal to the size of y_train" assert self.k &lt;= X_train.shape[0], \ "the size of X_train must be at least k." self._X_train = X_train self._y_train = y_train return self def predict(self, X_predict): assert self._X_train is not None and self._y_train is not None, \ "must fit before predict!" assert X_predict.shape[1] == self._X_train.shape[1], \ "the feature number of X_predict must be equal to X_train" y_predict = [self._predict(x) for x in X_predict] return np.array(y_predict) def _predict(self, x): assert x.shape[0] == self._X_train.shape[1], \ "the feature number of x must be equal to X_train" distances = [sqrt(np.sum((x_train - x) ** 2)) for x_train in self._X_train] nearest = np.argsort(distances) topK_y = [self._y_train[i] for i in nearest[:self.k]] votes = Counter(topK_y) return votes.most_common(1)[0][0] def __repr__(self): return "KNN(k=%d)" % self.k def predict_adavnce(self, X_test): assert X_test.shape[1] == self._X_train.shape[1], "the number of features in train set and test set must be equal" assert self._X_train is not None and self._y_train is not None, \ "must fit before predict!" y_predict = [self._predict_advance(x) for x in X_test] return np.array(y_predict) def _predict_advance(self, x): assert x.shape[0] == self._X_train.shape[1], "the number of features in train set and test set must be equal" distances = [sqrt(np.sum((train - x)**2)) for train in self._X_train] #print(sum(i == 0 for i in distances)) nearest = np.argsort(distances) topK_y = [self._y_train[i] for i in nearest[:self.k]] #print(topK_y) votes = Counter() for i in nearest[:self.k]: votes[self._y_train[i]] += (1/(distances[i]**2 + 1)) # 1/i #print(votes.most_common(1)[0][0]) return votes.most_common(1)[0][0] def train_test_split(self, X,y, text_ratio, seed = 0 ): assert X.shape[0] == y.shape[0], "the size of X must be equal tp teh size of y" assert 0.0 &lt;= text_ratio &lt;= 1.0, "text_ratio must be valid" if seed: np.random.seed(seed) shuffleindex = np.random.permutation(len(X)) text_size = int(len(X) * text_ratio) train_index = shuffleindex[text_size:] test_index = shuffleindex[:text_size] x_train = X[train_index] y_train = y[train_index] x_test = X[test_index] y_test = y[test_index] return x_train, x_test,y_train,y_test def score(self, x_test, y_test): y_predict = self.predict(x_test) return accuracy_score(y_test, y_predict) def accuracy_score(test, predict): return sum(test == predict) / len(predict)digits = datasets.load_digits()X = digits.datay = digits.targettemp = KNN_classifier(3)from sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)my_knn_clf = KNN_classifier(k=3)my_knn_clf.fit(X_train, y_train)#y_predict = my_knn_clf.predict(X_test)print(X_test.shape)y_predict = my_knn_clf.predict_adavnce(X_test)print(y_predict.shape)print(y_test.shape)print(sum(y_test == y_predict) / len(y_test)) 以上代码中predict_advance是高级版本的实现，利用手写数字的数据集发现写的代码没错，准确率还是蛮高的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[semantic 学习]]></title>
    <url>%2F2018%2F01%2F28%2Fsemantic-%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[介绍Semantic UI是一个在github上已经获得39264个star的漂亮的css框架，它具有语义化特点，也就是说语法特别容易上手。 安装与简单使用安装教程在官网。 分为简单安装和完全安装。简单安装只要下载了对应的css和js文件即可，我们在写前端时，引用对应的文件即可。 完全安装较麻烦一些，但可以支持更换主题，定制各按钮，表格样式等操作。 注意，如果只是简单安装的话，官网上给出的include in your html需要注意更改文件目录 我们可以在官方文档中找到多个样式的代码，看哪个自己喜欢的就复制粘贴一下，如下面的代码自己就能在网页中显示图中的效果。 1234567&lt;link rel="stylesheet" type="text/css" href="/Users/yangyuanhao/semantic/dist/semantic.min.css"&gt;&lt;script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"&gt;&lt;/script&gt;&lt;script src="semantic/dist/semantic.min.js"&gt;&lt;/script&gt;&lt;button class="ui button"&gt;Follow&lt;/button&gt; 更换主题与样式修改当我们使用Semantic UI来构建网页时，我们有时候会发现自己的网页打开的比较慢，这是由于国内的网络环境造成的。 网站的整体布局inverted 反色处理 左右边距可以用container segment左右无空，用于网页底部等 grid]]></content>
  </entry>
  <entry>
    <title><![CDATA[经济机器是如何运转的]]></title>
    <url>%2F2018%2F01%2F27%2F%E7%BB%8F%E6%B5%8E%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%90%E8%BD%AC%E7%9A%84%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[奥斯维辛:一部尘封的历史]]></title>
    <url>%2F2018%2F01%2F27%2F%E5%A5%A5%E6%96%AF%E7%BB%B4%E8%BE%9B-%E4%B8%80%E9%83%A8%E5%B0%98%E5%B0%81%E7%9A%84%E5%8E%86%E5%8F%B2%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[jquery实现记事本]]></title>
    <url>%2F2018%2F01%2F27%2Fjquery%E5%AE%9E%E7%8E%B0%E8%AE%B0%E4%BA%8B%E6%9C%AC%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[大明1556]]></title>
    <url>%2F2018%2F01%2F27%2F%E5%A4%A7%E6%98%8E1556%2F</url>
    <content type="text"><![CDATA[当国家（政府）有难时，牺牲的一定是平民百姓或商人。 （当民情汹涌时，）为了社会稳定，政府一定会牺牲政治斗争失败的官员或商人。]]></content>
  </entry>
  <entry>
    <title><![CDATA[东野圭吾的《白金数据》]]></title>
    <url>%2F2018%2F01%2F26%2F%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE%E7%9A%84%E3%80%8A%E7%99%BD%E9%87%91%E6%95%B0%E6%8D%AE%E3%80%8B%2F</url>
    <content type="text"><![CDATA[考完试后回到家里，闲来无事便读了一个下午读完了这本新出的侦探小说。比较符合我对东野圭吾的预期，小说很有悬念，比较刺激，但文学性和思想性不足，只是消遣性读物。 后面会涉及严重剧透======================================= 故事主要描述了日本想全国范围内收集民众的DNA数据，从而通过罪犯遗留下来的任何可能携带基因的物品，达到快速找到罪犯亲属或罪犯的目的。但这样巨大的工程容易造成民众隐私的泄漏，同时在小说中通过一系列无法通过基因数据库找到罪犯消息的犯罪案件和主角的被嫁祸，几个重要配角的反转，揭示了政府重要官员在数据库中做的手脚——他们的亲属犯罪将无法通过基因匹配找到罪犯的信息，自然的也将无法通过基因匹配查到他们身上。 这些政府官员的数据就是白金数据。 小说想向我们揭示大数据技术下个人隐私泄漏的可能，这不过是老生常谈。支付宝之前的年度账单事件就拔出萝卜带着泥，牵扯了一大批不合理读取用户个人数据的app。可以想象对掌控着我们所有社交信息的腾讯公司而言，我们几乎就是白纸一张，一切都能被人一眼望到底。 作者还想引起我们的思考，人心到底是不是完全物质化的呢？毕竟激素，神经递质，他们主宰着我们的情绪和思想。随着技术的进步，我们完全可以人造情感和思想。无奈太过浅尝辄止，作者本身并没有通过故事深入讨论这个话题。 东野圭吾应该也试图展示政府高级官员的龌龊。太阳底下没有新鲜事，全国范围内收集民众的DNA数据是一个涉及法律和公安执法的变革，历朝历代的变革根本目的都是更好地维护统治阶级的利益。屁股决定脑袋，换成说是日本政坛大佬都会这样做的。君王一怒，伏尸百万，流血漂橹。相比之下，放点儿白金数据都是小儿科的了。网上不就有纪录片和帖子讨论朴槿惠为了邪教献祭牺牲了岁月号三百多名学生的事情吗？想想令人毛骨悚然，但再想想史书不绝于笔的“族”，又为之泰然。详情]]></content>
  </entry>
  <entry>
    <title><![CDATA[利用随机森林法预测Titanic乘客生存率]]></title>
    <url>%2F2018%2F01%2F26%2F%E5%88%A9%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%B3%95%E9%A2%84%E6%B5%8BTitanic%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E7%8E%87%2F</url>
    <content type="text"><![CDATA[问题描述在kaggle上有一个竞赛题目，是如何根据泰坦尼克号上的已知的乘客数据来预测某一乘客在该轮船上能否存活。题目本身就是一道供初学者进行数据分析学习的问题，也有很多大佬给出了自己的数据训练的tutorial，自己就在这里分享下自己在借鉴了一些大佬的经验之后给出的自己的解决方案。 解决流程数据清洗数据清洗是所有数据分析问题第一步要解决的，我们首先来看下官方对数据集的说明。 PassengerId: 编号 Survived: 0 = 死亡，1 = 生存 Pclass: 船票级别 1 = 高级， 2 = 中等， 3 = 低等 Name: 名称 Sex: male = 男性，female = 女性 Age: 年龄 SibSp: 在 Titanic 上的兄弟姐妹以及配偶的人数 Parch: 在 Titanic 上的父母以及子女的人数 Ticket: 船票编号 Fare: 工资 Cabin: 所在的船舱 Embarked: 登船的港口 C = Cherbourg, Q = Queenstown, S = Southampton 接下来我们让我们读取数据并对数据有一个初步的感性认识。 我们首先来做定性分析，看一下特征类别分布是否平衡。类别平衡指分类样例不同类别的训练样例数目差别不大。当差别很大时，为类别不平衡。当类别不平衡的时候，例如正反比为 9:1，学习器将所有样本判别为正例的正确率都能达到 0.9。这时候，我们就需要使用 “再缩放”、“欠采样”、“过采样”、“阈值移动” 等方法。如下图，我们发现总体而言还是特征分布还是平衡的，活下来的人和死亡人数没有偏差过多。 删除无必要数据空数据处理我们来看看有多少空数据 对空数据我们怎么处理呢，这就要分情况讨论啦。 Age作图 Age ~ Survived。年龄较小的孩子生存的几率大。 因为年龄是一个连续值，而且他会对预测结果产生影响，同时我们发现不同群体中年龄的分布是不同的。 因此我们根据票的等级，在 Titanic 上的兄弟姐妹以及配偶的人数，在 Titanic 上的父母以及子女的人数来将数据分为不同的集合，再用缺失数据所在集合的平均值来进行填充。并判断最后的对 Age ~ Survived 的性质并未产生影响。 Embarked它缺少的数据只有两个，直接用众数填充即可。 Cabin他的数据较为复杂，Cabin 特征值由字母开头，判断船舱按字母分为A，B，C… 于是我们仅提取字母编号，降低维度。然后使用新的字母‘U’填充缺失数据。我们发现缺失数据的游客主要是三等舱的，并且这部分游客的生存率相对较低。 数值化和标准化数值化Ticket 特征值中的一串数字编号对我们没有意义，忽略。下面代码中，我们用正则表达式过滤掉这串数字，并使用 pandas get_dummies 函数进行数值化（以 Ticket 特征值 作为新的特征，0,1 作为新的特征值）。 123456789101112Ticket=[]import rer=re.compile(r'\w*')#正则表达式，查找所有单词字符[a-z/A-Z/0-9]for i in data['Ticket']: sp=i.split(' ')#拆分空格前后字符串，返回列表 if len(sp)==1: Ticket.append('U')#对于只有一串数字的 Ticket，Ticket 增加字符 'U' else: t=r.findall(sp[0])#查找所有单词字符，忽略符号，返回列表 Ticket.append(''.join(t))#将 t 中所有字符串合并data['Ticket']=Ticketdata=pd.get_dummies(data,columns=['Ticket'],prefix='T')#get_dummies：如果DataFrame的某一列中含有k个不同的值，则可以派生出一个k列矩阵或DataFrame（其值全为1和0） getdummies是处理类别醒数据很好的一种方式，这样我们可以将离散的分类变成具体的0，1特征向量，很大程度的加速了电脑计算的速度和监督学习最后训练得到的准确率。对cabin和embarked同样做此操作。最后得到的特征向量如图 标准化偏态分布偏态分布的数据有时不利于模型发现数据中的规律，我们可以使用 Log Transformation 来处理数据，这样可以提高训练的准确度。比如Fare这一特征就存在明显的偏态分布，我们skitlearn提供的函数进行处理，参考 Skewed Distribution and Log Transformation 离群点删除离群点是显著偏离数据集中其余对象的点。离群点来源于操作失误，数据本身的可变性等。我们这里采用箱线法,检测特征 [‘Age’, ‘Parch’, ‘SibSp’, ‘Fare’]的离群点。参考离群点和箱线法 123456789101112131415161718192021222324from collections import Counterdef outlier_detect(n, df, features):#定义函数 outlier_detect 探测离群点，输入变量 n, df, features，返回 outlier outlier_index = [] for feature in features: Q1 = np.percentile(df[feature], 25)#计算上四分位数（1/4） Q3 = np.percentile(df[feature], 75)#计算下四分位数（3/4） IQR = Q3 - Q1 outlier_span = 1.5 * IQR col = ((data[data[feature] &gt; Q3 + outlier_span]) | (data[data[feature] &lt; Q1 - outlier_span])).index outlier_index.extend(col) print('%s: %f (Q3+1.5*IQR) , %f (Q1-1.5*QIR) )' % (feature, Q3 + outlier_span, Q1 - outlier_span)) outlier_index = Counter(outlier_index)#计数 outlier = list(i for i, j in outlier_index.items() if j &gt;= n) print('number of outliers: %d' % len(outlier)) print(df[['Age', 'Parch', 'SibSp', 'Fare']].loc[outlier]) return outlieroutlier = outlier_detect(3, data, ['Age', 'Parch', 'SibSp', 'Fare'])#调用函数 outlier_detectdata = data.drop(outlier) 模型选择与训练模型介绍Boosting模型与bagging模型Bagging：假设我有一个大小为n的训练集D，bagging会从D中有放回的均匀地抽样，假设我用bagging生成了m个新的训练集Di，每个Di的大小为j。由于我有放回的进行抽样，那么在Di中的样本有可能是重复的。如果j=n，这种取样称为bootstrap取样。现在，我们可以用上面的m个训练集来拟合m个模型，然后结合这些模型进行预测。对于回归问题来说，我们平均这些模型的输出;对于分类问题来说，我们进行投票（voting）。 Boosting：Boosting与Bagging主要的不同是：Boosting的base分类器是按顺序训练的（in sequence），训练每个base分类器时所使用的训练集是加权重的，而训练集中的每个样本的权重系数取决于前一个base分类器的性能。如果前一个base分类器错误分类地样本点，那么这个样本点在下一个base分类器训练时会有一个更大的权重。一旦训练完所有的base分类器，我们组合所有的分类器给出最终的预测结果。过程如下图： Adaboost与RandomForest算法Adaboost是一种基于boosting模型的迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。 Adaboost的结构:最后的分类器YM是由数个弱分类器（weak classifier）组合而成的,相当于最后m个弱分类器来投票决定分类结果，而且每个弱分类器的“话语权”因子α大小不一样。 Randomforest是基于bangging模型实现的，他的元分类器是决策树。过程简要概括如下： 从原始训练集中进行bootstrap抽样 用步骤1中的bootstrap样本生成决策树 随机选择特征子集 用上面的特征子集来拆分树的节点 重复1和2两个步骤 集成所有生成的决策树进行预测 模型评估我们采用k 折交叉验证法，更具体的是10 折交叉验证法。 k 折交叉验证（k-fold cross validation）：将 D 划分 k 个大小相似的子集（每份子集尽可能保持数据分布的一致性：子集中不同类别的样本数量比例与 D 基本一致），其中一份作为测试集，剩下 k-1 份为训练集 T，操作 k 次。 例如 D 划分为 D1，D2，… ，D10，第一次使用 D1 作为训练集，第二次使用 D2，第三次使用 D3， … ， 第十次使用 D10 作为测试集。最后计算 k 次测试误差的平均值近似泛化误差。 12345678y = data['Survived']X = data.drop(['Survived'], axis=1).valuesclassifiers = [AdaBoostClassifier( random_state=2), RandomForestClassifier(random_state=2)]for clf in classifiers: score = cross_val_score(clf, X, y, cv=10, scoring='accuracy')#cv=10：10 折交叉验证法，scoring='accuracy'：返回测试精度 print([np.mean(score)])#显示测试精度平均值 我们可以发现随机森林分类器的准确率要高不少。 模型训练但是随机森林是基于决策树的，决策树一直存在着过拟合的问题。 过拟合是学习器性能过好，把样本的一些特性当做了数据的一般性质，从而导致训练误差低但泛化误差高。学习曲线是判断过拟合的一种方式，同时可以判断学习器的表现。学习曲线包括训练误差（或精度）随样例数目的变化曲线与测试误差（或精度）随样例数目的变化曲线。 接下来通过绘制学习曲线，我们发现训练误差始终接近 0，而测试误差始终偏高，说明存在过拟合的问题。 123456789101112131415161718192021222324252627282930313233from sklearn.model_selection import learning_curveimport matplotlib.pyplot as pltdef plot_learning_curve(estimator, title, X, y, cv=10, train_sizes=np.linspace(.1, 1.0, 5)):#定义函数 plot_learning_curve 绘制学习曲线。train_sizes 初始化为 array([ 0.1 , 0.325, 0.55 , 0.775, 1. ]),cv 初始化为 10，以后调用函数时不再输入这两个变量 plt.figure() plt.title(title)#设置图的 title plt.xlabel('Training examples')#横坐标 plt.ylabel('Score')#纵坐标 train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, train_sizes=train_sizes)#使用 10 折交叉验证法，对 train_sizes*m（m为总的样例数目） 个的数据进行训练，返回训练精度 train_scores,测试精度 test_scores train_scores_mean = np.mean(train_scores, axis=1)#计算平均值 train_scores_std = np.std(train_scores, axis=1)#计算标准差 test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid()#设置背景的网格 plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='g')#设置颜色 plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='r') plt.plot(train_sizes, train_scores_mean, 'o-', color='g', label='traning score')#绘制训练精度曲线 plt.plot(train_sizes, test_scores_mean, 'o-', color='r', label='testing score')#绘制测试精度曲线 plt.legend(loc='best') return pltg = plot_learning_curve(RandomForestClassifier(), 'RFC', X, y)#调用函数 plot_learning_curve 绘制随机森林学习器学习曲线 要解决这一问题只能通过调参。 skitlearn中的randomforestclassifer函数具有非常多的参数，列举几个如下。 参数 特点 n_estimators 基学习器数目（默认值10） 基本趋势是值越大精度越高 ，直到达到一个上限 criterion 选择算法 gini 或者 entropy (默认 gini) 视具体情况定 max_features 2.2.3节中子集的大小，即k值（默认 sqrt(n_features)） max_depth 决策树深度 过小基学习器欠拟合，过大基学习器过拟合。粗调节 max_leaf_nodes 最大叶节点数（默认无限制） 粗调节 min_samples_split 分裂时最小样本数，默认2 细调节,越小模型越复杂 min_samples_leaf 叶节点最小样本数，默认2 细调节，越小模型越复杂 bootstrap 是否采用自助法进行样本抽样（默认使用） 决定基学习器样本是否一致 我们先通过尝试找到最好的n_estimators和max_depth，max_leaf_nodes 参数 1234567891011121314151617181920def para_tune(para, X, y): # clf = RandomForestClassifier(n_estimators=para) #n_estimators 设置为 para score = np.mean(cross_val_score(clf, X, y, scoring='accuracy')) return scoredef accurate_curve(para_range, X, y, title): score = [] for para in para_range: score.append(para_tune(para, X, y)) plt.figure() plt.title(title) plt.xlabel('Paramters') plt.ylabel('Score') plt.grid() plt.plot(para_range, score, 'o-') return pltg = accurate_curve([2, 10, 50, 100, 150], X, y, 'n_estimator tuning') 与上面代码类似的我们可以得到下面三张图 接着我们就可以固定这三个影响较大的参数，再利用自动调参函数GridSearchCV来对其他的参数进行粗略的调节。 如此我们得到了最终0.8204的准确率。]]></content>
  </entry>
  <entry>
    <title><![CDATA[python作图]]></title>
    <url>%2F2018%2F01%2F25%2Fpython%E4%BD%9C%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[导论作图工具如恒河沙数，最好用的就是excel的图表功能，还有powermap这样强大的插件，可以非常轻松的解决工作制作报表的问题。但是excel的图表自定义功能并不够强大，在工程和数据科学领域，excle也捉襟见肘。相比之下matlab，mathmeticas这样的商业软件就非常优秀了，然而我偏不它们。我接下来要讲的是python中的Matplotlib和seaborn这样的函数库。 Matplotlib在 Matplotlib 中，大部分图形样式的绘制方法都存在于 pyplot 模块中，一共有160多种图表绘制方法。 2D图绘制我们首先用接下来的代码来引用科学计算函数库numpy和图形绘制库matplotlib 用npm可以很简单地安装好啦，我就不多说了。 12import numpy as npimport matplotlib as plt 然后我们来用下面的代码生成数据 12X = np.linspace(-2*np.pi,2*np.pi,1000) #在-2*np.pi和2*np.pi之间等间隔的生成1000个数据点，X就是一个np 数组Y = np.sin(X)#Y也是一个数组，对应了X的sin值 接着我们键入一下代码，分别生成线型图，柱形图和散点图 123plt.pyplot.plot(X,Y)plt.pyplot.bar(X,Y)plt.pyplot.scatter(X,Y) 输出如下 再接着我们可以尝试着画饼状图，量场图和等高线图 1234z = [1,2,3]plt.pyplot.pie(z)#饼被分成三块，每块的相对面积大小是1，2，3X, y = np.mgrid[0:10, 0:10]#表示的是一个矩阵plt.pyplot.quiver(X, y) 图形输出如下 这样子的功能其实excel也可以实现，还比python要简单，下面是体现python强大的图形自定义功能的时刻到啦 线型图线型图通过 matplotlib.pyplot.plot(args, *kwargs) 方法绘出。其中，args 代表数据输入，而 kwargs 的部分就是用于设置样式参数了。 123456789X = np.linspace(-2 * np.pi, 2 * np.pi, 1000)# 计算 sin() 对应的纵坐标y1 = np.sin(X)# 计算 cos() 对应的纵坐标y2 = np.cos(X)# 向方法中 `*args` 输入 X，y 坐标plt.pyplot.plot(X, y1, color='r', linestyle='--', linewidth=2, alpha=0.8)plt.pyplot.plot(X, y2, color='b', linestyle='-', linewidth=2) 我们就能见到这样漂亮的图拉 样式参数有很多，具体的可以见下表，更具体的麻烦查阅下官方文档]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages博客创建历程]]></title>
    <url>%2F2018%2F01%2F24%2F%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%88%9B%E5%BB%BA%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言对程序员来说，纸上得来终觉浅，绝知此事要躬行，每分专业的能力绝不是看书看来的，听课听来的，而一定要是自己一行一行代码敲来的。不仅如此，每个不甘寂寞的程序猿（媛），还会有着创建一个博客，发布自己所见所感所学的想法或习惯。为了更好地促进自己的进步，我也来加入写博客的大军啦。 怎样的博客应该以什么样的方式来创建博客是本节的重点。 在大一时，为了应付一门课的作业自己开通了cdsn的博客，有很多大牛的博客就扎根在了cdsn。但是对我而言，cdsn总有着烦人的广告，下载资料的时候还总要花钱充值，让我觉得很不爽，所以pass了这个选择。 还有很多人会选择简书，可那里是文艺青年的聚集地，程序员特有的geek氛围要远远差于cdsn。 至于知乎专栏和微信公众号，虽然也可以当作自己写随笔发布的园地，但与博客相比，总觉得差了点什么。 除此之外，还有wordpress和wix等建站方式，其实很方便的，可见即可得的操作，免费版的也不用花钱。主要原因还是自己嫌弃它们一点也不geek，就没有使用这种方式啦。说真的wix创建出来的网站可好看啦，而且官方的引导特别详尽，就像游戏里的引导操作一样，自己很容易就能建成一个很漂亮的网站，我当时 于是就开始考虑起自己搭建一个博客网站。自己之前尝试过用python的django框架搭建过博客网站，它自带有功能很强大的后台，当初自己实现的最终版也差强人意。一个完全由自己掌控的网站是能让人很有成就感的。可想了想自己还是没有选择这个自己实现的网站来作为自己的博客，因为除了后台功能外，自己所有都要手打代码，评论，搜索乃至前台的页面，出了bug要自己调试，万一调试不出来，博客就挂掉了。。。。 那么就考虑到了使用hexo框架来搭建博客，它并不是真正意义上代码的框架，使用它基本不需要编写代码，简介，美观，好用而功能强大。而且它与github pages可以无间合作，还可以省去了购买云主机，云服务器的花销。 创建博客的准备我们首先需要在电脑上安装好node.js , git和一些下载包管理工具如npm，homebrew（ mac自带），apt-get（Ubuntu自带），yum（centos自带）。 接着进入hexo官网，按照上面的步骤安装hexo。（此处非常有必要阅读一下hexo官方对自己的介绍） 我们此时已经安装好了hexo，可以输入以下命令检查是否安装成功，windows用户可能需要再配置一下环境变量， 接着我们输入 hexo init yournamecd yournamenpm install 这样就可以创建自己的博客文件夹，里面有网站的各个静态文件，自己以后的文章也会放在这个文件夹里，进入这个文件夹后，用npm命令来安装各项依赖。 此时我们用hexo new “文章标题”就可以新建一个文章，我们进入source文件夹内的_posts文件夹，就会发现多出了一篇md后缀的文件，这就是我们刚刚创建的文章啦，我们也可以直接在里面创建markdowm文件。接着我们用hexo s命令就能在本地运行这个简单的博客网站了。 然后在github新建一个代码仓库，仓库的名字一定要是”yourusername.github.io”。如我github账户名是yyhyplxyz，我这个代码仓库名字就是“yyhyplxyz.github.io”。 接着去github上添加ssh key，可以参考此网址ssh。]]></content>
      <tags>
        <tag>coding</tag>
      </tags>
  </entry>
</search>
