<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[个人报告]]></title>
    <url>%2F2019%2F06%2F30%2F%E4%B8%AA%E4%BA%BA%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[个人简单小结简短的课程学习自我总结 本次项目我们完成工作的时间是Mar 31, 2019 – Jun 30, 2019。其中自己的角色是后端小组长，参与的是后台搭建工作，包括api实现与管理系统实现。通过这次课程设计的编写，自己更好地掌握了flask相关技术原理，对应用容器化和网络服务框架的原理有了更深的理解，相关技术博文在本文底部附上。下面我将简单介绍本人项目完成的内容。 管理 - 讨论小组分工，及早确定后端成员的名单，同时通过讨论确定项目架构。 - 调整后端开发的节奏，保持开发进度能够稳步推进。 - 与产品经理和前端小组积极沟通，保证后端开发能够符合前端的需求。 分析 - 参与分析需求，以及实现需求所需要的后端API - 参与讨论后端API的设计 - 设计管理员页面架构 开发 - 负责后端整体技术框架的搭建，实现前后端分离，使用了MVC架构 - 负责后端数据库的开发 - 负责后端部分模块（管理员菜品管理，管理员首页）的实现 - 参与后端API的开发 - 负责后端API的调试，保证后端API的鲁棒性 - 项目后期参与前端调试工作，保证前端页面的完整性美观性。 PSP2.1表格 | | Personal Software Process Stages | Time(%) | | —————- | ——————————– | ——- | | \Planning** | \计划** | 10 | | estimate | 预估任务完成时间 | 10 | | \Development** | \开发** | 70 | | analysis | 需求分析 | 5 | | learning | 学习需要用到的技术 | 13 | | design | 设计系统 | 7 | | coding | 编码 | 30 | | debug | 测试修改 | 10 | | coding standard | 调整代码规范 | 5 | | \Documents** | \文档** | 20 | | administrate | 管理文档目录 | 6 | | database design | 数据库设计文档 | 9 | | size measurement | 计算工作量 | 2 | | personal report | 自我总结与个人报告 | 3 | 个人Git总结 项目主目录 最得意/或有价值/或有苦劳的工作清单 最得意：最得意的就是利用python装饰器模式实现了自定义的蓝图和数据库的自动回滚，感觉这样写代码很简介优雅。 最有价值：对web框架的原理有了更深的理解，包括wsgi，跨域访问，linux系统网络层链路层的多个参数，nginx的负载均很原理等。 最有苦劳：后端管理系统功能非常完善，而且不像Django一样有自带的管理系统，我们的感觉很厉害！ 技术博客清单 数据库中的视图 Python中的修饰器模式 应用容器化后造成的丢包现象与解决方案 DDoS 攻击的模拟仿真与防御 flask技术报告 特别致谢 - 项目经理 lordrou：感谢 lordrou帮助我们整理每周的组会记录和周报内容，让我们每一周都有明确的任务与目标，也为我们做出了很好的UI图和设计文档。 - 后台 LeonhardE：感谢LeonhardE，与他在后端的合作很愉快，让我收获了很多。 - 小队队长 johnsonLeeeee：感谢 johnsonLeeeee 对于小队的组建，他也认真的完成了前端的工作任务。 - 前端 Zhuyuze：感谢 Zhuyuze为我们的项目。设计了一个很漂亮的界面 - 测试 zxydashagou：感谢 zxydashagou 的测试工作与配合。]]></content>
  </entry>
  <entry>
    <title><![CDATA[系分技术报告]]></title>
    <url>%2F2019%2F06%2F30%2F%E7%B3%BB%E5%88%86%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[技术报告16340274 杨元昊 ​ 本次我们小组使用flask框架来完成作业后端，flask是python最常用的框架之一，它具有小巧灵活，定制度高的特点。 ​ 从业务逻辑上而言，这个程序并不复杂，缺少的是细心和耐心。值得一提的是本次作业我更灵活的使用了python异步编程和装饰器的相关知识。同时对flask的相关原理有了更清晰的认识。 ​ 在发送邮件的相关模块中，我通过开启多个线程进行邮件发送的操作，避免了IO阻塞。 ​ 在蓝图注册的过程中，我利用装饰器的知识新建了属于自己的蓝图，用以区分v1，v2版本的api，更好地实现了可扩展性。同时，我也用装饰器保证了数据库提交发生错误的时候发生回滚。我写了相关博客，可以参阅 ​ 以下将简单阐述下使用flask的心得体会。 应用上下文与请求上下文Flask提供了两种上下文，一种是应用上下文(Application Context)，一种是请求上下文(Request Context)。 可以查看Flask的文档：应用上下文 请求上下文 通俗地解释一下application context与request context： application 指的就是当你调用app = Flask(name)创建的这个对象app； request 指的是每次http请求发生时，WSGI server(比如gunicorn)调Flask.call()之后，在Flask对象内部创建的Request对象； application 表示用于响应WSGI请求的应用本身，request 表示每次http请求; application的生命周期大于request，一个application存活期间，可能发生多次http请求，所以，也就会有多个request 为了在一个线程中更加方便使用这些变量，flask中还有一种堆栈的数据结构（通过werkzeug的LocalStack实现），可以处理这些变量，但是并不直接处理这些变量。假如有一个程序得到一个请求，那么flask会将这个请求的所有相关信息进行打包，打包形成的东西就是处理请求的一个环境。flask将这种环境称为“请求上下文”(request context)，之后flask会将这个请求上下文对象放到堆栈中。这样，请求发生时，我们一般都会指向堆栈中的“请求上下文”对象，这样可以通过请求上下文获取相关对象并直接访问，例如current_app、request、session、g。还可以通过调用对象的方法或者属性获取其他信息，例如request.method。等请求结束后，请求上下文会被销毁，堆栈重新等待新的请求上下文对象被放入。 当在一个应用的请求上下文环境中，需要嵌套处理另一个应用的相关操作时（这种情况更多的是用于测试或者在console中对多个应用进行相关处理），“请求上下文”显然就不能很好地解决问题了，Flask中将应用相关的信息单独拿出来，形成一个“应用上下文”对象。这个对象可以和“请求上下文”一起使用，也可以单独拿出来使用。不过有一点需要注意的是：在创建“请求上下文”时一定要创建一个“应用上下文”对象。比如app = Flask(name)构造出一个 Flask App 时，App Context 并不会被自动推入 Stack 中。所以此时 Local Stack 的栈顶是空的，current_app也是 unbound 状态，在编写离线脚本的时候，如果直接在一个 Flask-SQLAlchemy 写成的 Model 上调用 User.query.get(user_id)，就会遇到 RuntimeError。因为此时 App Context 还没被推入栈中，而 Flask-SQLAlchemy 需要数据库连接信息时就会去取 current_app.config，current_app 指向的却是 _app_ctx_stack为空的栈顶。 因此运行脚本正文之前，先将 App 的 App Context 推入栈中，栈顶不为空后 current_app这个 Local Proxy 对象就自然能将“取 config 属性” 的动作转发到当前 App 上。 WSGIPython有着许多的Web框架，而同时又有着许多的Web 服务器（除了Nginx以外，还有Apache等），Web框架和Web服务器之间需要进行通信，如果在设计时它们之间不可以相互匹配的，那么选择了一个框架就会限制对Web服务器的选择，这显然是不合理的。 怎样确保可以在不修改Web服务器代码或Web框架代码的前提下，使用自己选择的服务器，并且匹配多个不同的网络框架呢？答案是接口，设计一套双方都遵守的接口就可以了。对Python来说，就是WSGI（Web Server Gateway Interface，Web服务器网关接口）。其他编程语言也拥有类似的接口：例如Java的Servlet API和Ruby的Rack。 Python WSGI的出现，让开发者可以将Web框架与Web服务器的选择分隔开来，不再相互限制。现在，你可以真正地将不同的Web服务器与Web框架进行混合搭配，选择满足自己需求的组合。]]></content>
  </entry>
  <entry>
    <title><![CDATA[DDoS 攻击的模拟仿真与防御]]></title>
    <url>%2F2019%2F06%2F30%2FDDoS-%E6%94%BB%E5%87%BB%E7%9A%84%E6%A8%A1%E6%8B%9F%E4%BB%BF%E7%9C%9F%E4%B8%8E%E9%98%B2%E5%BE%A1%2F</url>
    <content type="text"><![CDATA[DDoS 简介DDoS 的前身是 DoS（Denail of Service），即拒绝服务攻击，指利用大量的合理请求，来占用过多的目标资源，从而使目标服务无法响应正常请求。 DDoS（Distributed Denial of Service） 则是在 DoS 的基础上，采用了分布式架构，利用多台主机同时攻击目标主机。这样，即使目标服务部署了网络防御设备，面对大量网络请求时，还是无力应对。 比如，目前已知的最大流量攻击，正是去年 Github 遭受的 DDoS 攻击，其峰值流量已经达到了 1.35Tbps，PPS 更是超过了 1.2 亿（126.9 million）。 从攻击的原理上来看，DDoS 可以分为下面几种类型。 第一种，耗尽带宽。无论是服务器还是路由器、交换机等网络设备，带宽都有固定的上限。带宽耗尽后，就会发生网络拥堵，从而无法传输其他正常的网络报文。 第二种，耗尽操作系统的资源。网络服务的正常运行，都需要一定的系统资源，像是CPU、内存等物理资源，以及连接表等软件资源。一旦资源耗尽，系统就不能处理其他正常的网络连接。 第三种，消耗应用程序的运行资源。应用程序的运行，通常还需要跟其他的资源或系统交互。如果应用程序一直忙于处理无效请求，也会导致正常请求的处理变慢，甚至得不到响应。 比如，构造大量不同的域名来攻击 DNS 服务器，就会导致 DNS 服务器不停执行迭代查询，并更新缓存。这会极大地消耗 DNS 服务器的资源，使 DNS 的响应变慢。 无论是哪一种类型的 DDoS，危害都是巨大的。那么，如何可以发现系统遭受了 DDoS 攻击，又该如何应对这种攻击呢？接下来，我们就通过一个案例，一起来看看这些问题。 模拟操作我们使用hping3 来构造 TCP/IP 协议数据包，对系统进行安全审计、防火墙测试、DoS 攻击测试等。 我们用一个虚拟机/容器运行 Nginx ，用来模拟待分析的 Web 服务器；而另外两台作为 Web 服务器的客户端，其中一台用作 DoS 攻击，而另一台则是正常的客户端。使用多台虚拟机的目的，自然还是为了相互隔离，避免“交叉感染”。 由于案例只使用了一台机器作为攻击源，所以这里的攻击，实际上还是传统的 DoS ，而非 DDoS。 首先，在终端一中，执行下面的命令运行案例，也就是启动一个最基本的 Nginx 应用： 123# 运行Nginx服务并对外开放80端口# --network=host表示使用主机网络（这是为了方便后面排查问题）$ docker run -itd --name=nginx --network=host nginx 然后，在终端二和终端三中，使用 curl 访问 Nginx 监听的端口，确认 Nginx 正常启动。假设 192.168.0.30 是 Nginx 所在虚拟机的 IP 地址，那么运行 curl 命令后，你应该会看到下面这个输出界面： 12345# -w表示只输出HTTP状态码及总时间，-o表示将响应重定向到/dev/null$ curl -s -w &apos;Http code: %&#123;http_code&#125;\nTotal time:%&#123;time_total&#125;s\n&apos; -o /dev/null http://192.168.0.30/...Http code: 200Total time:0.002s 从这里可以看到，正常情况下，我们访问 Nginx 只需要 2ms（0.002s）。 接着，在终端二中，运行 hping3 命令，来模拟 DoS 攻击： 123# -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80# -i u10表示每隔10微秒发送一个网络帧$ hping3 -S -p 80 -i u10 192.168.0.30 现在，再回到终端一，我们就会发现，现在不管执行什么命令，都慢了很多。甚至于我们还会发现终端会 变得完全没有响应，乃至ssh断开。此时我们需要适当调大 u10（比如调成 u30），否则后面就不能通过 SSH 操作 。 然后，到终端三中，执行下面的命令，模拟正常客户端的连接： 123456# --connect-timeout表示连接超时时间$ curl -w &apos;Http code: %&#123;http_code&#125;\nTotal time:%&#123;time_total&#125;s\n&apos; -o /dev/null --connect-timeout 10 http://192.168.0.30...Http code: 000Total time:10.001scurl: (28) Connection timed out after 10000 milliseconds 你可以发现，在终端三中，正常客户端的连接超时了，并没有收到 Nginx 服务的响应。 这是发生了什么问题呢？我们再回到终端一中，检查网络状况。你应该还记得我们多次用过的 sar，它既可以观察 PPS（每秒收发的报文数），还可以观察 BPS（每秒收发的字节数）。 我们可以回到终端一中，执行下面的命令： 12345$ sar -n DEV 108:55:49 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil08:55:50 docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0008:55:50 eth0 22274.00 629.00 1174.64 37.78 0.00 0.00 0.00 0.0208:55:50 lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 其中， rxpck/s 和 txpck/s 分别是接收和发送的 PPS， 单位为包/秒。 rxkB/s 和 txkB/s 分别是接收和发送的吞吐量， 单位是KB/秒。 rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数， 单位是包/秒。 %ifutil 是网络接口的使用率， 即半双工模式下为 (rxkB/s+txkB/s)/Bandwidth， 而全双工模式下为 max(rxkB/s, txkB/s)/Bandwidth。 从这次 sar 的输出中，你可以看到，网络接收的 PPS 已经达到了 20000 多，但是 BPS 却只有 1174 kB，这样每个包的大小就只有 54B（1174*1024/22274=54）。 这明显就是个小包了，不过具体是个什么样的包呢？那我们就用 tcpdump 抓包看看吧。 在终端一中，执行下面的 tcpdump 命令： 123456789# -i eth0 只抓取eth0网卡，-n不解析协议名和主机名# tcp port 80表示只抓取tcp协议并且端口号为80的网络帧$ tcpdump -i eth0 -n tcp port 8009:15:48.287047 IP 192.168.0.2.27095 &gt; 192.168.0.30: Flags [S], seq 1288268370, win 512, length 009:15:48.287050 IP 192.168.0.2.27131 &gt; 192.168.0.30: Flags [S], seq 2084255254, win 512, length 009:15:48.287052 IP 192.168.0.2.27116 &gt; 192.168.0.30: Flags [S], seq 677393791, win 512, length 009:15:48.287055 IP 192.168.0.2.27141 &gt; 192.168.0.30: Flags [S], seq 1276451587, win 512, length 009:15:48.287068 IP 192.168.0.2.27154 &gt; 192.168.0.30: Flags [S], seq 1851495339, win 512, length 0... 这个输出中，Flags [S] 表示这是一个 SYN 包。大量的 SYN 包表明，这是一个 SYN Flood 攻击。如果你用Wireshark 来观察，则可以更直观地看到 SYN Flood 的过程： 实际上，SYN Flood 正是互联网中最经典的 DDoS 攻击方式。从上面这个图，你也可以看到它的原理： 即客户端构造大量的 SYN 包，请求建立 TCP 连接； 而服务器收到包后，会向源 IP 发送 SYN+ACK 报文，并等待三次握手的最后一次ACK报文，直到超时。 这种等待状态的 TCP 连接，通常也称为半开连接。由于连接表的大小有限，大量的半开连接就会导致连接表迅速占满，从而无法建立新的 TCP 连接。此时，服务器端的 TCP 连接，会处于 SYN_RECEIVED 状态： 这其实提示了我们，查看 TCP 半开连接的方法，关键在于 SYN_RECEIVED 状态的连接。我们可以使用 netstat ，来查看所有连接的状态，不过要注意，SYN_REVEIVED 的状态，通常被缩写为 SYN_RECV。 我们继续在终端一中，执行下面的 netstat 命令： 1234567# -n表示不解析名字，-p表示显示连接所属进程$ netstat -n -p | grep SYN_RECtcp 0 0 192.168.0.30:80 192.168.0.2:12503 SYN_RECV -tcp 0 0 192.168.0.30:80 192.168.0.2:13502 SYN_RECV -tcp 0 0 192.168.0.30:80 192.168.0.2:15256 SYN_RECV -tcp 0 0 192.168.0.30:80 192.168.0.2:18117 SYN_RECV -... 从结果中，你可以发现大量 SYN_RECV 状态的连接，并且源IP地址为 192.168.0.2。 进一步，我们还可以通过 wc 工具，来统计所有 SYN_RECV 状态的连接数： 12$ netstat -n -p | grep SYN_REC | wc -l193 找出源 IP 后，要解决 SYN 攻击的问题，只要丢掉相关的包就可以。这时，iptables 可以帮你完成这个任务。你可以在终端一中，执行下面的 iptables 命令： 1$ iptables -I INPUT -s 192.168.0.2 -p tcp -j REJECT 然后回到终端三中，再次执行 curl 命令，查看正常用户访问 Nginx 的情况： 123$ curl -w &apos;Http code: %&#123;http_code&#125;\nTotal time:%&#123;time_total&#125;s\n&apos; -o /dev/null --connect-timeout 10 http://192.168.0.30Http code: 200Total time:1.572171s 现在，你可以发现，正常用户也可以访问 Nginx 了，只是响应比较慢，从原来的 2ms 变成了现在的 1.5s。 不过，一般来说，SYN Flood 攻击中的源 IP 并不是固定的。比如，你可以在 hping3 命令中，加入 –rand-source 选项，来随机化源 IP。不过，这时，刚才的方法就不适用了。 幸好，我们还有很多其他方法，实现类似的目标。比如，你可以用以下两种方法，来限制 syn 包的速率： 12345# 限制syn并发数为每秒1次$ iptables -A INPUT -p tcp --syn -m limit --limit 1/s -j ACCEPT# 限制单个IP在60秒新建立的连接数为10$ iptables -I INPUT -p tcp --dport 80 --syn -m recent --name SYN_FLOOD --update --seconds 60 --hitcount 10 -j REJECT 到这里，我们已经初步限制了 SYN Flood 攻击。不过这还不够，因为我们的案例还只是单个的攻击源。 如果是多台机器同时发送 SYN Flood，这种方法可能就直接无效了。因为你很可能无法 SSH 登录（SSH 也是基于 TCP 的）到机器上去，更别提执行上述所有的排查命令。 所以，这还需要你事先对系统做一些 TCP 优化。 比如，SYN Flood 会导致 SYN_RECV 状态的连接急剧增大。在上面的 netstat 命令中，你也可以看到 190 多个处于半开状态的连接。 不过，半开状态的连接数是有限制的，执行下面的命令，你就可以看到，默认的半连接容量只有 256： 12$ sysctl net.ipv4.tcp_max_syn_backlognet.ipv4.tcp_max_syn_backlog = 256 换句话说， SYN 包数再稍微增大一些，就不能 SSH 登录机器了。 所以，你还应该增大半连接的容量，比如，你可以用下面的命令，将其增大为 1024： 12$ sysctl -w net.ipv4.tcp_max_syn_backlog=1024net.ipv4.tcp_max_syn_backlog = 1024 另外，连接每个 SYN_RECV 时，如果失败的话，内核还会自动重试，并且默认的重试次数是5次。你可以执行下面的命令，将其减小为 1 次： 12$ sysctl -w net.ipv4.tcp_synack_retries=1net.ipv4.tcp_synack_retries = 1 除此之外，TCP SYN Cookies 也是一种专门防御 SYN Flood 攻击的方法。SYN Cookies 基于连接信息（包括源地址、源端口、目的地址、目的端口等）以及一个加密种子（如系统启动时间），计算出一个哈希值（SHA1），这个哈希值称为 cookie。 然后，这个 cookie 就被用作序列号，来应答 SYN+ACK 包，并释放连接状态。当客户端发送完三次握手的最后一次 ACK 后，服务器就会再次计算这个哈希值，确认是上次返回的 SYN+ACK 的返回包，才会进入 TCP 的连接状态。 因而，开启 SYN Cookies 后，就不需要维护半开连接状态了，进而也就没有了半连接数的限制。 注意，开启 TCP syncookies 后，内核选项 net.ipv4.tcp_max_syn_backlog 也就无效了。 你可以通过下面的命令，开启 TCP SYN Cookies： 12$ sysctl -w net.ipv4.tcp_syncookies=1net.ipv4.tcp_syncookies = 1 注意，上述 sysctl 命令修改的配置都是临时的，重启后这些配置就会丢失。所以，为了保证配置持久化，你还应该把这些配置，写入 /etc/sysctl.conf 文件中。比如： 1234$ cat /etc/sysctl.confnet.ipv4.tcp_syncookies = 1net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_max_syn_backlog = 1024 不过要记得，写入 /etc/sysctl.conf 的配置，需要执行 sysctl -p 命令后，才会动态生效。 DDoS到底该怎么防御实际上，当 DDoS 报文到达服务器后，Linux 提供的机制只能缓解，而无法彻底解决。即使像是 SYN Flood 这样的小包攻击，其巨大的 PPS ，也会导致 Linux 内核消耗大量资源，进而导致其他网络报文的处理缓慢。 虽然你可以调整内核参数，缓解 DDoS 带来的性能问题，却也会像案例这样，无法彻底解决它。首先，因为Linux 内核中冗长的协议栈，在 PPS 很大时，对机器是一个巨大的负担。对 DDoS 攻击来说，也是一样的道理。 所以，基于 XDP 或者 DPDK，我们可以构建 DDoS 方案，在内核网络协议栈前，或者跳过内核协议栈，来识别并丢弃 DDoS 报文，避免DDoS 对系统其他资源的消耗。 不过，对于流量型的 DDoS 来说，当服务器的带宽被耗尽后，在服务器内部处理就无能为力了。这时，只能在服务器外部的网络设备中，设法识别并阻断流量（当然前提是网络设备要能扛住流量攻击）。比如，购置专业的入侵检测和防御设备，配置流量清洗设备阻断恶意流量等。 既然 DDoS 这么难防御，这是不是说明， Linux 服务器内部压根儿就不关注这一点，而是全部交给专业的网络设备来处理呢？ 当然不是，因为 DDoS 并不一定是因为大流量或者大 PPS，有时候，慢速的请求也会带来巨大的性能下降（这种情况称为慢速 DDoS）。 比如，很多针对应用程序的攻击，都会伪装成正常用户来请求资源。这种情况下，请求流量可能本身并不大，但响应流量却可能很大，并且应用程序内部也很可能要耗费大量资源处理。 这时，就需要应用程序考虑识别，并尽早拒绝掉这些恶意流量，比如合理利用缓存、增加 WAF（Web Application Firewall）、使用 CDN 等等。 总而言之，DDoS会对网络安全造成很大的影响，而且极难防御。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python中的修饰器模式]]></title>
    <url>%2F2019%2F06%2F30%2FPython%E4%B8%AD%E7%9A%84%E4%BF%AE%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[所谓的修饰器模式其实是在做下面的几件事。 表面上看，修饰器模式就是扩展现有的一个函数的功能，让它可以干一些其他的事，或是在现有的函数功能上再附加上一些别的功能。 修饰器模式可以让我们感受到函数的代码扩展能力，我们还能感受到函数的互相和随意拼装带来的好处。 但是深入一下，我们不难发现，Decorator这个函数其实是可以修饰几乎所有的函数的。于是，这种可以通用于其它函数的编程方式，可以很容易地将一些非业务功能的、属于控制类型的代码给抽象出来（所谓的控制类型的代码就是像for-loop，或是打日志，或是函数路由，或是求函数运行时间之类的非业务功能性的代码）。 Python的Decorator在使用上和Java的Annotation（以及C#的Attribute）很相似，就是在方法名前面加一个@XXX注解来为这个方法装饰一些东西。但是，Java/C#的Annotation也很让人望而却步，太过于复杂了。如果要了解它，我们需要先了解一堆Annotation的类库文档，感觉几乎就是在学另外一门语言。 而Python使用了一种相对于Decorator Pattern和Annotation来说非常优雅的方法，这种方法不需要你去掌握什么复杂的OO模型或是Annotation的各种类库规定，完全就是语言层面的玩法：一种函数式编程的技巧。这个模式动用了函数式编程的一个技术——用一个函数来构造另一个函数。 好了，我们先来点感性认识，看一个Python修饰器的Hello World代码。 123456789101112def hello(fn): def wrapper(): print "hello, %s" % fn.__name__ fn() print "goodbye, %s" % fn.__name__ return wrapper @hellodef Hao(): print "i am Hao Chen" Hao() 代码的执行结果如下： 1234$ python hello.pyhello, Haoi am Hao Chengoodbye, Hao 你可以看到如下的东西： 函数 Hao 前面有个@hello的“注解”，hello 就是我们前面定义的函数 hello； 在 hello 函数中，其需要一个 fn 的参数（这就是用来做回调的函数）； hello函数中返回了一个inner函数 wrapper，这个 wrapper函数回调了传进来的 fn，并在回调前后加了两条语句。 对于Python的这个@注解语法糖（syntactic sugar）来说，当你在用某个@decorator来修饰某个函数 func 时，如下所示: 123@decoratordef func(): pass 其解释器会解释成下面这样的语句： 1func = decorator(func) 嘿！这不就是把一个函数当参数传到另一个函数中，然后再回调吗？是的。但是，我们需要注意，那里还有一个赋值语句，把decorator这个函数的返回值赋值回了原来的 func。 我们再来看一个带参数的玩法： 123456789101112131415def makeHtmlTag(tag, *args, **kwds): def real_decorator(fn): css_class = " class='&#123;0&#125;'".format(kwds["css_class"]) \ if "css_class" in kwds else "" def wrapped(*args, **kwds): return "&lt;"+tag+css_class+"&gt;" + fn(*args, **kwds) + "&lt;/"+tag+"&gt;" return wrapped return real_decorator @makeHtmlTag(tag="b", css_class="bold_css")@makeHtmlTag(tag="i", css_class="italic_css")def hello(): return "hello world" print hello() 12# 输出：# &lt;b class=&apos;bold_css&apos;&gt;&lt;i class=&apos;italic_css&apos;&gt;hello world&lt;/i&gt;&lt;/b&gt; 在上面这个例子中，我们可以看到：makeHtmlTag有两个参数。所以，为了让 hello = makeHtmlTag(arg1, arg2)(hello) 成功， makeHtmlTag 必需返回一个decorator（这就是为什么我们在 makeHtmlTag 中加入了 real_decorator()）。 这样一来，我们就可以进入到decorator的逻辑中去了——decorator得返回一个wrapper，wrapper里回调 hello。看似那个 makeHtmlTag() 写得层层叠叠，但是，已经了解了本质的我们觉得写得很自然。 我们再来看一个为其它函数加缓存的示例: 1234567891011121314151617181920from functools import wrapsdef memoization(fn): cache = &#123;&#125; miss = object() @wraps(fn) def wrapper(*args): result = cache.get(args, miss) if result is miss: result = fn(*args) cache[args] = result return result return wrapper @memoizationdef fib(n): if n &lt; 2: return n return fib(n - 1) + fib(n - 2) 上面这个例子中，是一个斐波那契数例的递归算法。我们知道，这个递归是相当没有效率的，因为会重复调用。比如：我们要计算fib(5)，于是其分解成 fib(4) + fib(3)，而 fib(4) 分解成 fib(3) + fib(2)，fib(3) 又分解成fib(2) + fib(1)……你可以看到，基本上来说，fib(3), fib(2), fib(1)在整个递归过程中被调用了至少两次。 而我们用decorator，在调用函数前查询一下缓存，如果没有才调用，有了就从缓存中返回值。一下子，这个递归从二叉树式的递归成了线性的递归。wraps 的作用是保证 fib 的函数名不被 wrapper 所取代。 除此之外，Python还支持类方式的decorator。 123456789101112131415class myDecorator(object): def __init__(self, fn): print "inside myDecorator.__init__()" self.fn = fn def __call__(self): self.fn() print "inside myDecorator.__call__()" @myDecoratordef aFunction(): print "inside aFunction()" print "Finished decorating aFunction()"aFunction() 12345输出：# inside myDecorator.__init__()# Finished decorating aFunction()# inside aFunction()# inside myDecorator.__call__() 上面这个示例展示了，用类的方式声明一个decorator。我们可以看到这个类中有两个成员： 一个是__init__()，这个方法是在我们给某个函数decorate时被调用，所以，需要有一个 fn 的参数，也就是被decorate的函数。 一个是__call__()，这个方法是在我们调用被decorate的函数时被调用的。 从上面的输出中，可以看到整个程序的执行顺序。这看上去要比“函数式”的方式更易读一些。 我们来看一个实际点的例子。下面这个示例展示了通过URL的路由来调用相关注册的函数示例，这也是flask路由注册的相关源码： 12345678910111213141516171819202122232425262728class MyApp(): def __init__(self): self.func_map = &#123;&#125; def register(self, name): def func_wrapper(func): self.func_map[name] = func return func return func_wrapper def call_method(self, name=None): func = self.func_map.get(name, None) if func is None: raise Exception("No function registered against - " + str(name)) return func() app = MyApp() @app.register('/')def main_page_func(): return "This is the main page." @app.register('/next_page')def next_page_func(): return "This is the next page." print app.call_method('/')print app.call_method('/next_page') 注意：上面这个示例中decorator类不是真正的decorator，其中也没有__call__()，并且，wrapper返回了原函数。所以，原函数没有发生任何变化。]]></content>
  </entry>
  <entry>
    <title><![CDATA[应用容器化后造成的丢包现象与解决方案]]></title>
    <url>%2F2019%2F06%2F30%2F%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E5%90%8E%E9%80%A0%E6%88%90%E7%9A%84%E4%B8%A2%E5%8C%85%E7%8E%B0%E8%B1%A1%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[导论​ 容器利用 Linux内核提供的命名空间技术， 将不同应用程序的运行隔离起来， 并用统一的镜像，来管理应用程序的依赖环境。 这为应用程序的管理和维护， 带来了极大的便捷性， 并进一步催生了微服务、 云原生等新一代技术架构。虽然容器技术有很多优势， 但容器化也会对应用程序的性能带来一定影响。 笔者在完成项目设计的时候偶然发现将web应用容器化后意外地降低了网站的响应时间。因此本文将就此现象展开分析讨论。 原因猜想​ 笔者首先使用curl命令来检查网站应用对Http请求的响应，可以看到的确出现了访问超时的现象。 1curl --max-time 3 http://192.168.0.30 1curl: (28) Operation timed out after 3000 milliseconds with 0 bytes received 那么究竟是哪里有可能造成丢包呢？回顾整个网络协议栈，笔者总结了可能产生丢包的原因如下图。 具体而言，有以下几点： 在两台 电脑连接之间， 可能会发生传输失败的错误， 比如网络拥塞、 线路错误等； 在网卡收包后， 环形缓冲区可能会因为溢出而丢包； 在链路层， 可能会因为网络帧校验失败、 QoS 等而丢包； 在 IP层， 可能会因为路由失败、 组包大小超过 MTU 等而丢包；如果配置了 iptables 规则， 这些网络包也可能因为 iptables 过滤规则而丢包。 在传输层， 可能会因为端口未监听、 资源占用超过内核限制等而丢包； 在套接字层， 可能会因为套接字缓冲区溢出而丢包； 在应用层， 可能会因为应用程序异常而丢包； 我们首先忽略互联网中的数据传输问题，因为这不是我们能控制的。 链路层​ 来看链路层，当缓冲区溢出等原因导致网卡丢包时， Linux会在网卡收发数据的统计信息中， 记录下收发错误的次数。 我们进入容器内，执行如下命令 12345 netstat -iKernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1300 35 0 0 0 8 0 0 0 BMRUlo 65536 0 0 0 0 0 0 0 0 LRU ​ 输出中的 RX-OK、 RX-ERR、 RX-DRP、 RX-OVR ， 分别表示接收时的总包数、 总错误数、 进入Ring Buffer 后因其他原因（如内存不足） 导致的丢包数以及 Ring Buffer 溢出导致的丢包数。 TX-OK、 TX-ERR、 TX-DRP、 TX-OVR 也代表类似的含义， 只不过是指发送时对应的各个指标。 ​ 从中我们发现并没有任何错误，这说明说明容器的虚拟网卡没有丢包 。linux操作系统中有流量控制器，我们可以查看下流量控制规则。 1234tc -s qdisc show dev eth0qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap 1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1Sent 5702480501 bytes 44582392 pkt (dropped 0, overlimits 0 requeues 0)backlog 0b 0p requeues 0 可以看出linux并未配置任何模拟排队规则来模拟高延迟网络环境。 我们使用hping3确认一下，数据包的发送与接收确实是正常的。 12345hping3 -c 3 -S -p 80 192.168.0.30HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data byteslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=0 win=5120 rtt=7.8 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=1 win=5120 rtt=7.7 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=2 win=5120 rtt=3.6 ms 网络层我们回到容器环境下，执行netstat -s命令，可以看到TCP相关输出结果如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576TcpExt: 95 invalid SYN cookies received 36784 resets received for embryonic SYN_RECV sockets 943 packets pruned from receive queue because of socket buffer overrun 17 ICMP packets dropped because they were out-of-window 18372 TCP sockets finished time wait in fast timer 78 packets rejects in established connections because of timestamp 4724 delayed acks sent 104 delayed acks further delayed because of locked socket Quick ack mode was activated 8528 times 897 SYNs to LISTEN sockets dropped 18656 packets directly queued to recvmsg prequeue. 8 bytes directly in process context from backlog 20709 bytes directly received in process context from prequeue 44649 packet headers predicted 34 packets header predicted and directly queued to user 119192 acknowledgments not containing data payload received 9760 predicted acknowledgments 22 times recovered from packet loss due to fast retransmit 4303 times recovered from packet loss by selective acknowledgements Detected reordering 19 times using FACK Detected reordering 11 times using SACK Detected reordering 97 times using time stamp 187 congestion windows fully recovered without slow start 92 congestion windows partially recovered using Hoe heuristic 450 congestion windows recovered without slow start by DSACK 8176 congestion windows recovered without slow start after partial ack TCPLostRetransmit: 450 45 timeouts after reno fast retransmit 816 timeouts after SACK recovery 1077 timeouts in loss state 1004 fast retransmits 448 forward retransmits 1213 retransmits in slow start 2876 other TCP timeouts TCPLossProbes: 79489 TCPLossProbeRecovery: 49853 1 classic Reno fast retransmits failed 24360 SACK retransmits failed 2517 packets collapsed in receive queue due to low socket buffer 985 DSACKs sent for old packets 497 DSACKs sent for out of order packets 58701 DSACKs received 50 DSACKs for out of order packets received 320 connections reset due to unexpected data 115 connections reset due to early user close 1326 connections aborted due to timeout TCPSACKDiscard: 1 TCPDSACKIgnoredOld: 17 TCPDSACKIgnoredNoUndo: 44242 TCPSpuriousRTOs: 480 TCPSackShifted: 710 TCPSackMerged: 7075 TCPSackShiftFallback: 20566 TCPBacklogDrop: 35 TCPRcvCoalesce: 38445 TCPOFOQueue: 4724 TCPOFOMerge: 462 TCPChallengeACK: 1034 TCPSYNChallenge: 3 TCPFastOpenCookieReqd: 15 TCPFromZeroWindowAdv: 4 TCPToZeroWindowAdv: 4 TCPWantZeroWindowAdv: 340 TCPSynRetrans: 2403 TCPOrigDataSent: 15014 TCPHystartTrainDetect: 66 TCPHystartTrainCwnd: 147 TCPHystartDelayDetect: 117 TCPHystartDelayCwnd: 3917 TCPACKSkippedSynRecv: 2107 TCPACKSkippedPAWS: 4 TCPACKSkippedSeq: 25 TCPACKSkippedFinWait2: 1 TCPACKSkippedTimeWait: 4 TCPACKSkippedChallenge: 4 netstat 汇总了 IP、 ICMP、 TCP、 UDP等各种协议的收发统计信息。 不过， 我们的目的是排查丢包问题， 所以这里主要观察的是错误数、 丢包数以及重传数。从 36784 resets received for embryonic SYN_RECV sockets，我们可以看出主要的错误来自于半连接重置 ，也就是第三次握手失败。 原因分析与解决​ 为什么会这样呢，我们使用抓包工具做一下测试 1tcpdump -i eth0 -nn port 5000 ​ 输出如下，从中我们可以看出前三个包是正常的 TCP三次握手，但第四个包却是在 3 秒以后了， 并且还是客户端（VM2） 发送过来的 FIN包， 也就说明， 客户端的连接关闭了 。 1234514:40:00.589235 IP 10.255.255.5.39058 &gt; 172.17.0.2.5000: Flags [S], seq 332257715, win 29200, options [mss 1418,sackOK,TS val 486800541 ecr 0,nop,wscale 7], length 014:40:00.589277 IP 172.17.0.2.5000 &gt; 10.255.255.5.39058: Flags [S.], seq 1630206251, ack 332257716, win 4880, options [mss 256,sackOK,TS val 2509376001 ecr 486800541,nop,wscale 7], length 014:40:00.589894 IP 10.255.255.5.39058 &gt; 172.17.0.2.5000: Flags [.], ack 1, win 229, options [nop,nop,TS val 486800541 ecr 2509376001], length 014:40:03.589352 IP 10.255.255.5.39058 &gt; 172.17.0.2.5000: Flags [F.], seq 76, ack 1, win 229, options [nop,nop,TS val 486803541 ecr 2509376001], length 014:40:03.589417 IP 172.17.0.2.5000 &gt; 10.255.255.5.39058: Flags [.], ack 1, win 40, options [nop,nop,TS val 2509379001 ecr 486800541,nop,nop,sack 1 &#123;76:77&#125;], length 0 ​ 此时，我们再执行netstat -i 命令就会发现网卡的确记录了丢包 1234Kernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1300 169 0 0 105 76 0 0 0 BMRUlo 65536 0 0 0 0 0 0 0 0 LRU ​ 这也就是为什么curl会超时了。而我们发现之前执行hping3命令时，一切正常，因此很有可能是因为curl 在发送 SYN包后， 还会发送 HTTPGET请求 导致的。因此curl发送的数据包包含的字段会多一些，笔者进而发现了容器配置的虚拟网卡的MTU值只有1300，而以太网的默认MTU值是1500.这个值表示最大传输单元，如果一条物理链路的两端MTU不一致，就会导致丢包现象的发生。（MTU一般与MRU最大接收单元相等，这样一个物理链路可以类似一个水管，两边流量不匹配导致的失衡） ​ 我们将MTU的值改回1500即可。笔者并不清楚为什么docker会有这样一个奇怪的错误，网上可以查到也有不少遇到这个问题的人。也附上相关博文链接，供参考 https://medium.com/@sylwit/how-we-spent-a-full-day-figuring-out-a-mtu-issue-with-docker-4d81fdfe2caf https://nicksu86.github.io/2019/05/Docker-MTU/]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据库中的视图]]></title>
    <url>%2F2019%2F06%2F30%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E8%A7%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[在一个项目的实际开发过程中牵涉到复杂业务的时候，我们不可避免的需要使用中间表来进行数据连接，虽然我们可以用主外键进行关联，多对多，多对一，一对一等，但采用主外键关联在数据的操作过程中具有很强的耦合性，尤其对于需要经常删改数据表而言，采用主外键关联这种模式对数据库性能还是有影响的。如果我们采用中间表的话，当数据过大在性能上又面临严峻考验，sql视图的出现，在解决中间表的业务逻辑上是不错的选择。视图是虚拟的表，只包含使用时动态检索数据的查询。也就是说作为视图，它不包含任何列和数据，包含的是一个查询。视图的字段是由我们自定义的，视图只供查询，数据不可更改，查询数据来源于我们建立的实体表。 使用视图可以 重用SQL语句 简化复杂的SQL操作 使用表的一部分而不是整个表 保护数据：可以赋予访问表的特定部分的权限 可返回与底层表不同格式和表示的数据 如果应用大量或复杂的视图，极可能影响性能，应该先进行测试再应用视图。 视图的使用也有一些限制 视图名必须唯一，与其他的视图和表不同 视图数目没有限制 可以嵌套 视图不能索引，也不能有关联的触发器或默认值 不同DBMS对视图语法和表示的定义不同。 例如，有些DBMS会限制视图嵌套的级数、禁止在视图中使用ORDER BY子句、设置视图为只读等等。 创建视图使用CREATE VIEW语句来创建视图。 使用DROP VIEW语句来删除视图。 覆盖或更新视图，必须先删除视图，然后再重新创建。 利用视图简化复杂的联结建议创建不绑定特定数据的视图，增强可重用性。我们以下从顾客，订单，购买物品三个表连在了一起，创建了视图。 123456789101112131415161718192021222324252627282930313233343536373839404142MariaDB [sqlbzbh]&gt; SHOW TABLES;+-------------------+| Tables_in_sqlbzbh |+-------------------+| Customers || OrderItems || Orders || Products || Vendors |+-------------------+5 rows in set (0.00 sec)MariaDB [sqlbzbh]&gt;MariaDB [sqlbzbh]&gt; CREATE VIEW ProductCustomers AS -&gt; SELECT cust_name, cust_contact, prod_id -&gt; FROM Customers, Orders, OrderItems -&gt; WHERE Customers.cust_id = Orders.cust_id -&gt; AND OrderItems.order_num = Orders.order_num;Query OK, 0 rows affected (0.01 sec)MariaDB [sqlbzbh]&gt;MariaDB [sqlbzbh]&gt; SHOW TABLES;+-------------------+| Tables_in_sqlbzbh |+-------------------+| Customers || OrderItems || Orders || ProductCustomers || Products || Vendors |+-------------------+6 rows in set (0.00 sec)MariaDB [sqlbzbh]&gt; SELECT cust_name, cust_contact -&gt; FROM ProductCustomers -&gt; WHERE prod_id = 'RGAN01';+---------------+--------------------+| cust_name | cust_contact |+---------------+--------------------+| Fun4All | Denise L. Stephens || The Toy Store | Kim Howard |+---------------+--------------------+2 rows in set (0.00 sec)MariaDB [sqlbzbh]&gt; 用视图重新格式化检索出的数据1234567891011121314151617MariaDB [sqlbzbh]&gt; CREATE VIEW VendorLocations AS -&gt; SELECT Concat(vend_name, '---', vend_country) AS vend_title FROM Vendors;Query OK, 0 rows affected (0.00 sec)MariaDB [sqlbzbh]&gt;MariaDB [sqlbzbh]&gt; SELECT * FROM VendorLocations;+-------------------------+| vend_title |+-------------------------+| Bear Emporium---USA || Bears R Us---USA || Doll House Inc.---USA || Fun and Games---England || Furball Inc.---USA || Jouets et ours---France |+-------------------------+6 rows in set (0.00 sec)MariaDB [sqlbzbh]&gt; 用视图过滤不想要的数据WHERE子句适用于视图。 1234567891011121314MariaDB [sqlbzbh]&gt; CREATE VIEW CustomerEMailList AS -&gt; SELECT cust_id, cust_name, cust_email FROM Customers WHERE cust_email IS NOT NULL;Query OK, 0 rows affected (0.01 sec)MariaDB [sqlbzbh]&gt;MariaDB [sqlbzbh]&gt; SELECT * FROM CustomerEMailList;+------------+--------------+-----------------------+| cust_id | cust_name | cust_email |+------------+--------------+-----------------------+| 1000000001 | Village Toys | sales@villagetoys.com || 1000000003 | Fun4All | jjones@fun4all.com || 1000000004 | Fun4All | dstephens@fun4all.com |+------------+--------------+-----------------------+3 rows in set (0.00 sec)MariaDB [sqlbzbh]&gt; 使用视图与计算字段12345678910111213141516MariaDB [sqlbzbh]&gt; CREATE VIEW OrderItemsExpanded AS -&gt; SELECT order_num, prod_id, quantity, item_price, quantity*item_price AS expanded_price FROM OrderItems;Query OK, 0 rows affected (0.00 sec)MariaDB [sqlbzbh]&gt;MariaDB [sqlbzbh]&gt; SELECT * FROM OrderItemsExpanded WHERE order_num = 20008;+-----------+---------+----------+------------+----------------+| order_num | prod_id | quantity | item_price | expanded_price |+-----------+---------+----------+------------+----------------+| 20008 | RGAN01 | 5 | 4.99 | 24.95 || 20008 | BR03 | 5 | 11.99 | 59.95 || 20008 | BNBG01 | 10 | 3.49 | 34.90 || 20008 | BNBG02 | 10 | 3.49 | 34.90 || 20008 | BNBG03 | 10 | 3.49 | 34.90 |+-----------+---------+----------+------------+----------------+5 rows in set (0.00 sec)MariaDB [sqlbzbh]&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[CNCF初实践]]></title>
    <url>%2F2019%2F01%2F19%2FCNCF%E5%88%9D%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[K8S介绍什么是Kubernetes？Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。我们曾经用过Docker容器技术部署容器，而Docker恰恰可以被看成Kubernetes内部使用的低级别组件。Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。 使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它 实际上，使用Kubernetes只需一个部署文件，使用一条命令就可以部署多层容器（前端，后台等）的完整集群： 1$ kubectl create -f single-config-file.yaml 集群集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了Kubernetes平台。下图展示这样的集群。 这里可以看到一个典型的Kubernetes架构图。 集群拥有一个Kubernetes Master（紫色方框）。Kubernetes Master提供集群的独特视角，并且拥有一系列组件来操控多个主机提供高可用的服务。 搭建K8S集群与基本工具安装准备安装过程将在Ubuntu 16.04系统上（一台master node，两台worker node）进行 VirtualBox搭建集群 下载ubuntu镜像文件 创建新的虚拟机镜像模板 挂载我们在第一步下载的镜像文件 创建全局NAT网络 克隆三个虚拟机，k8s1（master，k8s2，k8s3 网络类型NAT网络，选择我们第四步创建的网络名称 NAT网络配置端口转发规则，确保可以通过ssh本地连接虚拟机 安装容器引擎安装docker，具体过程略 安装各组件和命令行工具 添加kuberbates apt源 1234567891011cat /etc/apt/sources.list# 系统安装源deb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse# kubeadm及kubernetes组件安装源deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main apt安装kublet， kubeadm，kubectl 初始化master节点，其中192.168.16.0/20为本地IP，apiserver-advertise-address是API server 用来告知集群中其它成员的地址 1kubeadm init --apiserver-advertise-address=10.0.25 --pod-network-cidr=192.168.16.0/20 配置kubectl环境变量 1export KUBECONFIG=/etc/kubernates/admin.conf 查看当前节点 1kubectl get pods -n kube-system -o wide 我们呢会发现此时除了dns服务，其它都可以使用 安装集群网络一个好的网络方案，可以让多个容器构成的基于微服务架构的应用可以运行在任何地方：主机，多主机，云上或者数据中心。应用程序使用网络就好像容器是插在同一个网络交换机上一样，不需要配置端口映射，连接等。 我们这里选择比较常用的Weave网络，它通过创建虚拟网络使docker容器能够跨主机通信并能够自动相互发现。 1curl -L "https://cloud.weave.works/k8s/net?k8s-version4(kubectl v ersion I base64 I tr -d '\n')" &gt; weave.yaml 接着我们打开该文件，如下，新添加-name:IPALLOC_RANGE value:192.168.16.0/20字段，其中ip地址是我们在第一步创建的master节点的ip。 1kubectl apply -f weave.yaml 节点加入集群网络 现状 此时master节点的各组件已安装完毕，但集群中只有master一个节点。 k8s2节点加入集群 1kubeadm join 192.168.0.200:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579 k8s3节点以此类推，值得注意的是，一般master节点不承担工作负荷。 非master节点上管理集群 将master节点上的/etc/kubernetes/admin.conf分发到各worker节点的对应目录下 设置环境变量 1export KUBECONFIG=/etc/kubernates/admin.conf 性能监控 安装dashboard 1kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 访问dashboard 建立api server的代理，运行在8001端口 1kubectl proxy 设置端口转发规则，本地端口9001的数据转到master节点上去（9091端口） 1ssh -D 9001 root@127.0.0.1 -p 9091 浏览器设定代理服务器，本地的数据转发到9001端口，从而访问master节点 浏览器输入地址，访问dashboard 1http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy 创建管理用户 1kubectl create -f user.yaml 创建绑定 1kubectl create -f user-binding.yaml user.yaml的文件内容如下 12345apiVersion: vl kind: ServiceAccount metadata: name: yyhyplxyz namespace: kube system user-binding.yaml的文件内容如下 123456789101112apiVersion: rbac.authorization.k8s.io/vlbetalkind: ClusterRoleBindingmetadata: name: dashboard:yyhyplxyz roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: kind: lerviceAccount name: yyhyplxyz namespace: kube system 接着，我们查看我们生成的用户密钥 12kubectl get secret -n kube-system #选出恰当的名字kubectl describe secret/yyhyplxyz token-knfmh -n kube-system 复制token数据段 浏览器登录，dashboard登录界面 选择令牌方式登录 安装Heapster 这个组件负责k8s集群度量数据采集和容器监控， 我们首先下载Heapster的github开源代码 下载镜像文件 1docker pull anjia0532/heapster grafana amd64:v4.4.3 在github源码包中的/deploy/ube-config中执行创建influxdb的命令，influxdb是Heapster的后端 1kubectl create -f influxdb 创建角色绑定，在deploy目录下，运行 1kubectl create -f rbac/heapster-rbac.yaml 安装效果 K8S基本使用PodPod是k8s的最基本的操作单元，包含一个或多个紧密相关的容器，类似于豌豆荚的概念。一个Pod可以被一个容器化的环境看作应用层的“逻辑宿主机”（Logical Host)。一个Pod中的多个容器应用通常是紧耦合的。Pod在Node上被创建、启动或者销毁。 为什么k8s使用Pod在容器之上再封装一层呢？一个很重要的原因是Docker容器之间的通信受到Docker网络机制的限制。在Docker的世界中，一个容器需要通过link方式才能访问另一个容器提供的服务（端口）。大量容器之间的link将是一个非常繁重的工作。通过Pod的概念将多个容器组合在一个虚拟的“主机”内，可以实现容器之间仅需通过Localhost就能相互通信了。 一个Pod中的应用容器共享同一组资源： （1）PID命名空间：Pod中的不同应用程序可以看见其他应用程序的进程ID （2）网络命名空间：Pod中的多个容器能访问同一个IP和端口范围 （3）IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信。 （4）UTS命名空间：Pod中的多个容器共享一个主机名 （5）Volumes（共享存储卷）：Pod中的各个容器可以访问在Pod级别定义的Volumes 一些Pod有Label。一个Label是attach到Pod的一对键/值对，用来传递用户定义的属性。比如，你可能创建了一个”tier”和“app”标签，通过Label（tier=frontend, app=myapp）来标记前端Pod容器，使用Label（tier=backend, app=myapp）标记后台Pod。然后可以使用Selectors选择带有特定Label的Pod，并且将Service或者Replication Controller应用到上面。 pod常常运行在node（物理主机）上。 Replication ControllerReplication Controller确保任意时间都有指定数量的Pod“副本”在运行。如果为某个Pod创建了Replication Controller并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3.如下面的动画所示： 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动升级时很有用。 master服务上创建文件node-controller.yaml,为了避免同一个rc定义的pod在同一个node上生成多个pod时，端口冲突，文件中不指定hostPort。replicas指定pod的数量。 12345678910111213141516171819202122232425apiVersion: v1kind: ReplicationController #指名类型为rcmetadata: name: node-controller #rc 名字，创建后的pod名字为 serverrc-随机字符 labels: name: node-controllerspec: replicas: 2 #指定副本数量，会一直维持这个数量的副本，如果某节点挂掉，又会立即构建新的来替代 selector: name: node-test-pod template: metadata: labels: name: node-test-pod spec: containers: - name: node-test image: 192.168.174.131:5000/node-base:1.0 #镜像源 env: - name: ENV_TEST_1 value: env_test_1 - name: ENV_TEST_2 value: env_test_2 ports: - containerPort: 80 #容器端口 1kubectl create -f node-controller.yaml 现在已经创建了Pod的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是Service。 Service如果Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？Service就恰是定义一系列Pod以及访问这些Pod的策略的一层抽象。Service通过Label找到Pod组。现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，lable选择器为（tier=backend, app=myapp）。backend-service的Service会完成如下两件重要的事情： 会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。 下述动画展示了Service的功能。 有一个特别类型的Kubernetes Service，称为’LoadBalancer’，作为外部负载均衡器使用，在一定数量的Pod之间均衡流量。比如，对于负载均衡Web流量很有用。 我们再看上面例子，node-test Pod运行了2个副本（replicas），这2个Pod对于前端程序来说没有区别，所以前端程序不关心是哪个后端副本在提供服务。并且后端node-test Pod在发生变化（比如replicas数量变化或某个node挂了，Pod在另一个node重新生成），前端无须跟踪这些变化。“Service”就是用来实现这种解耦的抽象概念。如下，我们可以创建 12345678910111213apiVersion: v1kind: Service #指定种类为servicemetadata: name: node-service #service 名字 提供给调用方使用，调用方无需知道ip 由dns解析服务名为cluster ip labels: name: node-service #rc的名字，service将同一名字的rc作为后端node进行负载spec: ports: - port: 8081 #service暴露在cluster ip上的端口，clusterIP:port 是提供给集群内部客户访问service的入口 targetPort: 80 #pod上的端口，从port和nodePort上到来的数据最终经过kube-proxy流入到后端pod的targetPort上进入容器，和rc中的containerPort一致 protocol: TCP #提供给集群外部客户访问service的入口 selector: name: node-test-pod]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker介绍深入浅出(-)]]></title>
    <url>%2F2018%2F12%2F24%2Fdocker%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[镜像和容器镜像和容器是 Docker 中最基本的 2 个概念，镜像相当于光盘， 光盘里面存储的数据是只读的， 不会被更改的。我们可以把镜像找作一个模板或者蓝图，而容器是用镜像生成的 Docker 实例。一个镜像可以生成多个容器， 每个容器之间，容器与宿主机之间都是相互隔离的，同时，容器可以快速方便地运行, 也可以方便地删除。在实际应用中，我们甚至可以把容器当作虚拟机来使用。 自动部署与Dockerfile我们首先需要安装一个docker，这部分内容略，读者可以轻易在网上找到资料，且操作极易。 运行第一个容器应用我们使用下面的命令启动一个 Docker 容器 1docker run -d -t -p 8000:5000 -name demo ubuntu:18.04 这条命令的具体解释如下 run 表示启动一个新的 Docker 容器 -d 表示容器在后台运行 -t 极少能用到， 用于让一个空白的 ubuntu 镜像在后台运行 -p 用于指定端口映射′ 表示在本机访问 8000 会被自动转到容器中的 5000 端口必须保证本机没有其他程序占用了 8000 端口， 否则这里会失败 -name demo 指定了容器的名字是 demo ubuntu:18.04 是启动容器使用的镜像名，Docker 会自动从镜像服务器去下载这个镜像 运行一个简单的web应用这次我们选择开一个新容器，并提供web服务。 1docker run -t -d -p 7000:5000 -name demo ubuntu:18.04 由于之前已经自动下载过 ubuntu:18.04 所以这次不会重新下载镜像, 速度很快，这个新的容器名叫demo。 我们首先在本地准备好运行的代码文件a.py，如下 123456789from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return 'Hello World'if __name__ == "__main__": app.run(host="0.0.0.0", debug=True) 接着我们在容器内创建目录，exec 命令后跟随容器名，然后再添加操作命令mkdir /code 1docker exec demol mkdir /code cp 参数把当前文件夹的a.py拷贝到 demo 容器的 /Code/a.py处。 1docker cp a.py "demo:/code/a.py" /code必须是容器中存在的目录， Docker 不会自动创建，因此我们需要先手动创建。 接着我们同样用exec安装必要的依赖 12345docker exec demo apt updatedocker exec demo apt -y install python3 python3-pipdocker exec demo pip3 install flask 然后在容器 demo 中运行a.py 1docker exec demo python3 /code/o.py 通过以上的操作，我们并在容器中安装程序运行所需的依赖库然后运行了一个 flask web 程序 以上操作较繁琐，我们也可以新建两个脚本文件install.sh和run.sh，然后直接运行脚本文件。 1234#install.shapt updateapt -y install python3 python3-pippip3 install flask 123#run.shcd /codepython3 a.py 1234567docker cp install.sh "demol:/code/install.sh"docker cp run.sh "demo1:/code/run.sh"docker exec demol bash /code/install.shdocker exec demol bash /code/run.sh Dockerfile自动化操作在本文件夹下，除了a.py外，我们又新建了个Dockerfile文件，如下。 12345678910111213141516171819202122232425# 在 Dockerfile 文件中 # 是注释# FROM 用于指定构建镜像使用的基础镜像FROM ubuntu:18.04# RUN 用于在构建镜像的时候在镜像中执行命令# 这里我们安装 python3 和 flask web 框架RUN apt updateRUN apt -y install python3 python3-pipRUN pip3 install flask# COPY 相当于命令的 docker cp# 把本机当前目录下的 app.py 文件拷贝到镜像的 /code/app.py# 和 docker cp 不同的是，COPY 会自动创建镜像中不存在的目录，比如 /codeCOPY app.py /code/app.py# WORKDIR 用于指定从镜像启动的容器内的工作目录WORKDIR /code# CMD 用于指定容器运行后要执行的命令和参数列表# 这样从本镜像启动容器后会自动执行 python3 app.py 这个命令# 由于我们已经用 WORKDIR 指定了容器的工作目录# 所以下面的命令都是在 /code 下执行的CMD ["python3", "app.py"] 我们可以直接运行下面的命令构建镜像 1docker build -t webimage . -t webimage 指定了镜像的名字为 webimage，最后的那个 . 用来表示工作目录为本机当前目录。随后我们可以直接用该镜像来运行容器 1docker run -p 8001:5000 -name demo2 webimage docker 常用命令启动一个停止运行的容器 1docker start demo 查找正在运行的容器 1docker ps 停止容器， demo 是容器的名字 1docker stop demo 可以用 -o 参数查找所有容器, 包括未运行的 1docker -o ps 删除被停止的容器和运行中的容器 12docker rm demodocker rm -f demo 数据卷与文件挂载我们可以在概念上把 Docker 找作虚拟机。当容器被删除的时候， 容器里的所有数据都会被删除，两个不同的容器之间无法互通。基于这些因素 Docker 推出了 数据卷 (volume) 功能。我们可以把数据卷理解为虚拟机的虚拟磁盘， 它是独立于容器的文件，在容器中它破挂载为一个目录的形式。对于容器中的应用来说， 数据卷是透明的， 无法感知它的存在， 它就是一个普通的文件夹。由于数据卷独立于容器而存在, 因此删除容器的时候数据卷不会受到影响。数据卷有以下优点 多容器可以通过挂载同一个数据卷来共享数据 数据卷可以方便地备份存储数据 创建一个 volume 1docker volume create testvolume 列出所有数据卷 1docker volume ls 删除一个数据卷 1docker volume rm testvolume 我们下面来找一下如何在容器中使用数据卷 先创建一个数据卷web，接着在运行容器webimage的时候， 使用参数 -mount 如下 1docker run -d --name demovolume -mount source=web, target=/volume webimage 后面参数的含义是把数据卷挂载到容器的 /volume 目录上 这样就运行了一个带有数据卷的容器， 这个容器的 /VoIume 目录中的内容在容器被删除后仍然存在。因为它实际上是存在 Docker 数据卷中。没有保存在数据卷上的文件会在容器被删除后丢失 这样我们就可以实现多容器之间可以通过数据卷共享文件 1docker run -d -name demovolume2 -mount source=web,target=/volume webimage 除了挂载数据卷外, Docker 还可以挂载共享目录 (这一点和虚拟机一样)。共享目录的优势是使用方便， 易于理解。我们开发时在宿主机中修改源代码就可以做到docker中实时生效省却 build 镜像的过程。下面的命令会从 nginx 镜像运行一个名为 nginx1 的容器，井且设置了 8080-&gt;80 的端口映射，mount 参数的 type=bind 表明要挂载共享目录，把宿主机的当前目录映射为容器的 /usr/share/nginx/html(这是 nginx 容器的静态页面文件存放路径).这样在宿主机中访问localhost:8080 会自动访问宿主机当前目录下的 index.html 文件 1docker run -p 8080:80 -name nginx1 -mount type=bind, source="$&#123;PWD&#125;" ,target=/usr/share/nginx/html/ nginx 多应用容器化编排Docker 被设计为程序容器， 所以每一个容器只应该运行一个程序，但是在实际的项目中会有需要多个程序互相配合一起运行的情况。比如 Web 程序通常包含 cpp、 数据库、 nginX， Redis 等。这些程序各自的容器需要协同工作， 并且需要能够互相访问网络。比如 cpp 需要连接数据库，nginx 需要能访问 cpp 才能给它做反向代理。 由于 Docker 容器是一个隔离的环境， 正常情况下容器与容器之间是无法互相访问的，为了应对复杂工程的需要， 我们可以手动配置多容器之间的虚拟网络，文件互访等功能来实现容器交互，但 Docker 官方推出了Compose 程序用于配置管理多容器的运行。 Compose 通过一个单独的 docker-compose.yml 配量文件来管理一组容器，参下。 123456789101112131415161718192021222324252627# 表示这是 compose 配置文件第三版version: '3'# 每个服务都是一个 Docker 容器# 所以必须用 image 指定服务的镜像名或者从 Dockerfile 中 build 镜像services: pyweb: # build 指定了 Dockerfile 所在的路径 build: . # ports 指定暴露的端口，9000 是宿主机，5000 是容器 # 可以指定多个暴露端口 ports: - "9000:5000" # volumes 参数把当前目录挂载到容器的 /code # docker-compose 的配置中才支持相对路径的挂载 volumes: - .:/code # depends_on 设定了依赖，这里 redisdemo 会先于 pyweb 启动 # 但是如果 redisdemo 启动时间长于 pyweb # 那么 pyweb 运行的时候 redisdemo 未必可用 depends_on: - redisdemo redisdemo: # 每个服务必须用 image 指定镜像名或者从 Dockerfile 中 build # 这里用 image 指定镜像，redis:alpine 是 redis 项目的官方 Docker 镜像 image: "redis:alpine" 使用如下命令开启服务 1docker-compose up 用 -d 参数让项目后台运行 1docker-comuose up -d 用 stop 暂停容器的运行 1docker-comuose up -d 用 down 关闭并删除项目的所有容器 1docker-compose down 如果我们修改了文件代码后，要重新启动项目时，再次运行docker-compose up，会发现它并末重新构建镜像。这是因为 Dockerfile 并末修改， 所以 Docker 复用了已有的镜像，修改并末生效。这时候我们应该重新构建镜像并运行, 或者加上 “build 参数来强制 builddocker-compose up &quot;build。 Docker使用的小trickDocker更换镜像源将下载在源改为中科大镜像源等国内源，可以显著提高下载速度。https://docker.mirrors.ustc.edu.cn，具体修改方式见下图。]]></content>
  </entry>
  <entry>
    <title><![CDATA[容器化搭建Go-Web应用]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%AE%B9%E5%99%A8%E5%8C%96%E6%90%AD%E5%BB%BAGo-Web%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[改用Mysql数据库本次作业要求改BoltDB为Mysql数据库，同时，考虑到后面的作业要求搭建博客。因此将之前写的新闻应用做了重构，改成了一个博客后台。 使用了Beggo框架自带的ORM。个人感觉这类框架内部封装好了很多逻辑，实现功能会比boltdb的键值对操作要容易很多。下面以用户类别为例分别对增删改查四个操作作出解释。 User的模型类别如下，我们可以像下面这样指定对应数据库的列要求。如主键pk，自增auto，可以为空null与不可重复unique等。 123456789101112type User struct &#123; Id int `orm:"pk;auto"` Username string `orm:"unique"` Password string Token string `orm:"unique"` Avatar string Email string `orm:"null"` Url string `orm:"null"` Signature string `orm:"null;size(1000)"` InTime time.Time `orm:"auto_now_add;type(datetime)"` Roles []*Role `orm:"rel(m2m)"`&#125; 增加操作 在用户注册页面，当我们根据用户的输入构建好了user类后，就可以通过如下函数进行用户的创建，注册。 orm是单例模式，如下一行简单的insert就做到了。 12345func SaveUser(user *User) int64 &#123; o := orm.NewOrm() id, _ := o.Insert(user) return id&#125; 删除操作 实际博客应用中很少删除用户，为了演示使用下面的函数。同样的传入user类后，一行简单的delete就做到了。 1234func DeleteUser(user *User) &#123; o := orm.NewOrm() o.Delete(user)&#125; 修改操作 如上，因为指定了id为主键，所以update的时候会查找主键相同的，再进行修改 1234func UpdateUser(user *User) &#123; o := orm.NewOrm() o.Update(user)&#125; 查找操作 查找的语句较麻烦，我们需要指定一个数据表，指明过滤的字段和条件，并将user类型指针传入以获得查找到的结果。 123456func Login(username string) (bool, User) &#123; o := orm.NewOrm() var user User err := o.QueryTable(user).Filter("Username", username).One(&amp;user) return err != orm.ErrNoRows, user&#125; 除了上述的操作外，我们还可以用数据库原生语句来执行命令。如下以增删为例。 123456789func DeleteUserRolesByUserId(user_id int) &#123; o := orm.NewOrm() o.Raw("delete from user_roles where user_id = ?", user_id).Exec()&#125;func SaveUserRole(user_id int, role_id int) &#123; o := orm.NewOrm() o.Raw("insert into user_roles (user_id, role_id) values (?, ?)", user_id, role_id).Exec()&#125; 我们构建好了model后，就可以注册数据库了。 如下我们需要指明数据库的地址，用户名，密码数据库名，通过orm.RegisterDataBase注册数据库，通过orm.RegisterDriver注册mysql驱动。之后我们还需要依次注册各个model，并同步数据库orm.RunSyncdb。 12345678910111213func init() &#123; orm.RegisterDriver("mysql", orm.DR_MySQL) orm.RegisterDataBase("default", "mysql", username+":"+password+"@tcp("+url+":"+port+")/pybbs?charset=utf8&amp;parseTime=true&amp;charset=utf8&amp;loc=Asia%2FShanghai", 30) orm.RegisterModel( new(models.User), new(models.Topic), new(models.Section), new(models.Reply), new(models.ReplyUpLog), new(models.Role), new(models.Permission)) orm.RunSyncdb("default", false, true)&#125; Docker构建容器 mysql镜像 我们在安装了docker后，首先将mysql镜像下载下来。 1docker pull mysql:5.6 接着我们可以查看到已有的镜像。 1docker images |grep mysql 然后开启mysql容器，并指定各类配置。 1docker run -p 3306:3306 --name mymysql -v $PWD/conf:/etc/mysql/conf.d -v $PWD/logs:/logs -v $PWD/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 -p 3306:3306：将容器的 3306 端口映射到主机的 3306 端口。 -v ~/conf:/etc/mysql/conf.d：将主机当前目录下的 conf/my.cnf 挂载到容器的 /etc/mysql/my.cnf。 -v ~/logs:/logs：将主机当前目录下的 logs 目录挂载到容器的 /logs。 -v ~/data:/var/lib/mysql ：将主机当前目录下的data目录挂载到容器的 /var/lib/mysql 。 -e MYSQL_ROOT_PASSWORD=123456：初始化 root 用户的密码。 node镜像 接下来开启前端镜像，以下我们使用dockerfiles，进入项目目录。 1234567891011121314#安装node镜像FROM node:7.8.0# 创建 app 目录WORKDIR /app# 安装 app 依赖RUN npm install#暴露端口EXPOSE 8080#运行命令CMD [ "npm", "run" ,"dev"] 然后我们创建docker镜像 1docker build -t web_front . 并运行 1docker run -p 8080:8080 -d web_front go镜像 接下来我们同样进入web后端目录，创建dockerfile。 12345678910111213#安装go镜像FROM golang:latest# 创建 应用 目录WORKDIR $GOPATH/src/web_proADD . $GOPATH/src/web_pro#安装依赖RUN go get github.com/astaxie/beegoRUN go get github.com/beego/beeRUN go build .#暴露端口EXPOSE 8080#运行命令CMD [ "bee", "run" ] 然后我们创建docker镜像，并运行 1docker build -t web_pro . 1docker run -p 8080:8080 -d web_pro]]></content>
  </entry>
  <entry>
    <title><![CDATA[net/http 源码略读]]></title>
    <url>%2F2018%2F11%2F16%2Fnet-http-%E6%BA%90%E7%A0%81%E7%95%A5%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[net/http 源码略读基本架构​ 网络发展，很多网络应用都是构建再 HTTP 服务基础之上。HTTP 协议从诞生到现在，发展从1.0，1.1到2.0也不断再进步。理解 HTTP 构建的网络应用只要关注两个端—客户端（clinet）和服务端（server），两个端的交互来自 clinet 的 request，以及server端的response。所谓的http服务器，主要在于如何接受 clinet 的 request，并向client返回response。 简单应用​ 以下是最简单的使用net/http库构建的web服务器。 123456789101112package main import ( "fmt" "net/http")func main() &#123; handler := func(w http.ResponseWriter, r *http.Request) &#123; w.Write([]byte("hello golang\n")) &#125; http.HandleFunc("/", handler) http.ListenAndServe(":8080", nil)&#125; ​ 根据以上的代码，发现主要的是三个部分HandleFunc，ResponseWriter和ListenAndServe。下面将分别解释这三个部分。 ####HandleFunc ​ 实际上http.HandleFunc封装自ServeMux的HandleFunc方法。见如下代码： 12345func HandleFunc( pattern string, handler func(ResponseWriter, *Request)) &#123; DefaultServeMux.HandleFunc(pattern, handler)&#125; ​ 接收request的过程中，最重要的莫过于路由（router），这就是内置的DefautServeMux，也可以自定义。Multiplexer路由的目的就是为了找到处理器函数（handler），后者将对request进行处理，同时构建response。Golang中的Multiplexer基于ServeMux结构，同时也实现了Handler接口。 1234567891011type ServeMux struct &#123; mu sync.RWMutex m map[string]muxEntry hosts bool // 决定是否会保存主机名&#125;type muxEntry struct &#123; explicit bool h Handler //控制器 pattern string //路由&#125; ​ 其中mu字段是读写锁，用于并发读写的数据同步。在并发的时候保证了数据的一致性。m字段是一个字典数据结构，可以看到muxEntry才是路由的保存的地方，也就是一个路径对应一个urlhandler。 ResponseWriter​ ResponseWriter是一个接口，主要部分就是下面列的三个 12345type ResponseWriter interface &#123; Header() Header Write([]byte) (int, error) WriteHeader(statusCode int)&#125; ​ 其中，Header会返回Http头部内容，write和writeHeader就主要负责写入具体内容到报文。 ListenAndServe​ 在http.ListenAndServe(&quot;:8080&quot;, nil)上，服务器会触发对应模式的handler。 其具体实现： 123456789101112131415161718192021222324252627282930313233343536373839type Handler interface &#123; // 只要实现了ServeHTTP接口就可以用作handler作为ListenAndServe的第二个参数 ServeHTTP(ResponseWriter, *Request)&#125;type ResponseWriter interface &#123; Header() Header Write([]byte (int, error) WriteHeader(int)&#125;func ListenAndServe(add string, handler Handler) error &#123; // server内部有一种方法找到我们先前注册的handler server := &amp;Server&#123;Addr: addr, Handler: Handler&#125; return server.ListenAndServe()&#125;type Server struct &#123; Addr string Handler Handler /* 包括其他的Server控制参数，包括时间相关的参数 Header大小、状态、日志、运输层安全。 */ mu sync.Mutex listeners map[net.Listener]struct&#123;&#125; // TCP 底层的连接池 // 保存HTTP长连接，看HTTP协议版本和HTTP首部字段中的`Connection`的状体 // 注意map的值是struct&#123;&#125;类型,保证可以存储各类控制handler activeConn map[*conn]struct&#123;&#125; doneChan chan struct&#123;&#125; // Server的停止、关闭控制&#125;func (srv *Server) ListenAndServe() error &#123; // 忽略细节性代码 addr := srv.Addr ln, err := net.Listen("tcp", addr) // 创建tcp连接，addr为监听地址"ip:port"形式的字符串 return srv.Serve(tcpKeepAliveListener&#123;ln.(*net.TCPListener)&#125;) 小结总而言之，web服务器的基本流程如下： 1Clinet -&gt; Requests -&gt; [Multiplexer(router)] -&gt; handler -&gt; Response -&gt; Clinet]]></content>
  </entry>
  <entry>
    <title><![CDATA[go语言并发爬虫]]></title>
    <url>%2F2018%2F10%2F04%2Fgo%E8%AF%AD%E8%A8%80%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[正则表达式的使用正则表达式是一种进行模式匹配和文本操纵的复杂而又强大的工具。虽然正则表达式比纯粹的文本匹配效率低，但是它却更灵活。按照它的语法规则，随需构造出的匹配模式就能够从原始文本中筛选出几乎任何你想要得到的字符组合。 Go语言通过`regexp`标准包为正则表达式提供了官方支持， `regexp`包中含有三个函数用来判断是否匹配，如果匹配返回true，否则返回false 123func Match(pattern string, b []byte) (matched bool, error error)func MatchReader(pattern string, r io.RuneReader) (matched bool, error error)func MatchString(pattern string, s string) (matched bool, error error) 上面的三个函数实现了同一个功能，就是判断`pattern`是否和输入源匹配，匹配的话就返回true，如果解析正则出错则返回error。三个函数的输入源分别是byte slice、RuneReader和string。 如果要验证一个输入是不是IP地址，那么如何来判断呢？请看如下实现 123456func IsIP(ip string) (b bool) &#123; if m, _ := regexp.MatchString("^[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;$", ip); !m &#123; return false &#125; return true&#125; 可以看到，regexp的pattern和我们平常使用的正则一模一样。 爬虫基础爬虫程序从一个网页开始，获取它的内容之后，可以根据xml或者xpath包找到对应的元素，然后利用上文提到的正则表达式提取一些关键数据。通常爬虫程序在一个网页中找到URL，会继续抓取该URL的内容，继续找URL，再继续抓取它的内容。 下面我们演示下利用http库来模拟对浏览器发出请求，并获得响应。 123456789101112131415161718192021222324252627282930313233343536package mainimport ( "fmt" "net/http" "net/http/httputil")func main() &#123; request, err := http.NewRequest( http.MethodGet, "http://www.baidu.com", nil) request.Header.Add("User-Agent", "Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1") client := http.Client&#123; CheckRedirect: func( req *http.Request, via []*http.Request) error &#123; fmt.Println("Redirect:", req) return nil &#125;, &#125; resp, err := client.Do(request) if err != nil &#123; panic(err) &#125; defer resp.Body.Close() s, err := httputil.DumpResponse(resp, true) if err != nil &#123; panic(err) &#125; fmt.Printf("%s\n", s)&#125; 因此爬虫程序的基本原理如下。初始化seed把请求链接给Engine，然后Engine将url维护为一个任务队列，从任务队列中源源不断的取出任务交个Fetcher获得页面内容，再交给Parser获得数据以及其他的任务链接。最后再把这些链接存入任务队列中。 Go语言并发编程GO语言原生支持协程，可以在一个或多个线程中开启多个协程。主函数main其实也是个协程。 Go语言中的协程叫goroutine，Go标准库提供的调用操作，IO操作都会出让CPU给其他goroutine，让协程间的切换管理不依赖系统的线程和进程，不依赖CPU的核心数量。 我们可以把它粗略的看成是个简单的线程 123456789101112131415161718package mainimport ( "fmt" "time")func main() &#123; for i := 0; i &lt; 1000; i++ &#123; go func(i int) &#123; //i是函数的形式变量 for &#123; fmt.Printf("Hello from "+ "goroutine %d\n", i) &#125; &#125;(i) //来源于主函数中的ii &#125; time.Sleep(time.Minute)&#125; 协程的引入是为了并发编程，提高效率。并发编程的难度在于协调，协调需要通过通信，并发通信模型分为共享数据和消息。共享数据即多个并发单元保持对同一个数据的引用，数据可以是内存数据块，磁盘文件，网络数据等。数据共享通过加锁的方式来避免死锁和资源竞争。 **Go**语言则采取消息机制来通信，每个并发单元是独立的个体，有独立的变量，不同并发单元间这些变量不共享，每个并发单元的输入输出只通过消息的方式。 下面的代码详细解释了channel的原理 1234567891011121314151617181920212223242526//1、channel声明，声明一个管道chanName，该管道可以传递的类型是ElementType//管道是一种复合类型，[chan ElementType],表示可以传递ElementType类型的管道[类似定语从句的修饰方法]var chanName chan ElementTypevar ch chan int //声明一个可以传递int类型的管道var m map[string] chan bool //声明一个map，值的类型为可以传递bool类型的管道 //2、初始化ch:=make(chan int) //make一般用来声明一个复合类型，参数为复合类型的属性 //3、管道写入,把值想象成一个球，"&lt;-"的方向，表示球的流向，ch即为管道//写入时，当管道已满（管道有缓冲长度）则会导致程序堵塞，直到有goroutine从中读取出值ch &lt;- value//管道读取，"&lt;-"表示从管道把球倒出来赋值给一个变量//当管道为空，读取数据会导致程序阻塞，直到有goroutine写入值value:= &lt;-ch //4、每个case必须是一个IO操作，面向channel的操作，只执行其中的一个case操作，一旦满足则结束select过程//面向channel的操作无非三种情况：成功读出；成功写入；即没有读出也没有写入select&#123; case &lt;-chan1: //如果chan1读到数据，则进行该case处理语句 case chan2&lt;-1: //如果成功向chan2写入数据，则进入该case处理语句 default: //如果上面都没有成功，则进入default处理流程&#125; 并发爬虫的实现在上文提到的普通爬虫的基础上，我们可以利用并发编程的原理改进成并发爬虫。这是因为Fetcher会耗时较多，造成其它任务的阻塞。并发爬虫的逻辑框架如下，其中worker会读取页面内容，并对页面内容做解析，request队列即是任务队列。我们为每个request构建goroutine，并为每个work构建goroutine，协程和协程间通过channel通信，当有worker的协程空闲时，就会承担某个request的协程的任务。当无法为某个request搭配一个worker时就会堵塞下去。 具体的我们实现的时候要实现以下几个部件。 Engine。用于不断的创建协程，根据url判重来发起request，并将request传给scheduler。 Scheduler。用于维护request队列和创建worker的协程，并维护worker队列。 Worker。构建页面解析函数，用于解析页面获得新的url以及得到我们想要的数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package schedulerimport ( "scrawl/types")// Request队列和Worker队列type QueuedScheduler struct &#123; requestChan chan types.Request workChan chan chan types.Request&#125;func (s *QueuedScheduler) Submit(r types.Request) &#123; s.requestChan &lt;- r&#125;func (s *QueuedScheduler) WorkerChan() chan types.Request &#123; return make(chan types.Request)&#125;func (s *QueuedScheduler) WorkerReady(w chan types.Request) &#123; s.workChan &lt;- w&#125;func (s *QueuedScheduler) Run() &#123; s.workChan = make(chan chan types.Request) s.requestChan = make(chan types.Request) go func() &#123; var requestQ []types.Request var workQ []chan types.Request for &#123; var activeRequest types.Request var activeWorker chan types.Request if len(requestQ) &gt; 0 &amp;&amp; len(workQ) &gt; 0 &#123; activeWorker = workQ[0] activeRequest = requestQ[0] &#125; select &#123; case r := &lt;-s.requestChan: requestQ = append(requestQ, r) case w := &lt;-s.workChan: workQ = append(workQ, w) case activeWorker &lt;- activeRequest: workQ = workQ[1:] requestQ = requestQ[1:] &#125; &#125; &#125;()&#125; 代码见github https://github.com/zys980808/Agenda-Go 中的并发爬虫文件夹内]]></content>
  </entry>
  <entry>
    <title><![CDATA[go语言实现selpg]]></title>
    <url>%2F2018%2F10%2F04%2Fgo%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0selpg%2F</url>
    <content type="text"><![CDATA[功能简介selpg是一个unix系统下命令。 该命令本质上就是将一个文件，通过自己设定的分页方式，输出到屏幕或者重定向到其他文件上，或者利用打印机打印出来。使用格式如下。 -s start_page -e end_page [ -f | -l lines_per_page ][ -d dest ] [ in_filename ] 必需标志以及参数： -s，后面接开始读取的页号 int -e，后面接结束读取的页号 int s和e都要大于1，并且s &lt;= e，否则提示错误 可选参数： -l，后面跟行数 int，代表多少行分为一页，不指定 -l 又缺少 -f 则默认按照72行分一页 -f，该标志无参数，代表按照分页符’\f’ 分页 -d，后面接打印机名称，用于打印 filename，唯一一个无标识参数，代表选择读取的文件名 flag包解析参数flag库是一个简单的解析参数的函数包，使用它会比使用os函数包解析字符串来获得程序运行参数简单很多。我们可以使用flag.XxxxxVar方法来实现参数的获取，该函数第一个传递值是某个自己定义的变量的指针，第二个传递值是我们给程序的某个参数，第三个传递值是在该参数没有给出时，我们设定的默认值，第四个参数是一个参数说明。 12345flag.IntVar(&amp;pagestart, "s", -1, "the start page")flag.IntVar(&amp;pageend, "e", -1, "The end page")flag.IntVar(&amp;lineperpage, "l", 72, "the paging form")flag.StringVar(&amp;printDest, "d", "", "The named-printer to print the selected content.")flag.BoolVar(&amp;separationtype, "f", false, "Choose if the input pages should be separated by Formfeed-Character('\\f'). (not compatible with `-l`)") 具体的，在我们的程序中，我们将所有变量都声明在var中，flag的参数绑定方法声明在init()函数中，从而让程序一运行就解析参数并绑定到全局变量中。 为了让程序具有更好的鲁棒性，设计一个检验函数，在输入命令出错时给予提示。 1234567891011121314151617if DefaultlineperPage != lineperpage &amp;&amp; separationtype == true &#123; goto argsinvalid &#125; if pagestart == -1 || pageend == -1 &#123; goto valueInvalid &#125; if pagestart &lt;= 0 || pagestart &gt; pageend &#123; goto valueInvalid &#125; if lineperpage &lt;= 0&#123; goto valueInvalid &#125; return argsinvalid: log.Fatalln("[Error] ... `-l` is not compatible with `-f`.") valueInvalid: log.Fatalln("[Error] ... Values of `-s` and `-e` are invalid.") 分页功能接下来我们实现具体的文件分页功能，首先我们要根据separtiontype，即是否有-f参数来判断根据换行符还是行数来分页。 如果是根据行数来判断分页，我们首先利用bufio函数获得我们输入流的缓存，再以换行符为依据读取每行内容，在不需要输出到打印机时，直接利用系统输出流输出到文件或者屏幕即可，否则要将内容暂时存到res这个string变量中。 如果是根据换行符来判断，则要用readrune函数来读取单个字符，接着输出到标准输出流或者暂时存在内存中，输出到打印机内。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func write() &#123; rd := bufio.NewReader(myin) if !separationtype &#123; for true &#123; //d1 := []byte("lineperpage" + string(lineperpage)) //err := ioutil.WriteFile("test.txt", d1, 0644) line, err = rd.ReadString('\n') if err != nil || io.EOF == err &#123; break &#125; currLine++ if currLine &gt; lineperpage &#123; currPage++ currLine = 0 &#125; if currPage &gt;= pagestart &amp;&amp; currPage &lt;= pageend &#123; // not for printer but to stdout if printDest == "" &#123; fmt.Fprintf(myout, "%s", line) &#125; else &#123; res += line &#125; &#125; &#125; &#125; else &#123; for true &#123; char, _, err1 := rd.ReadRune() if err1 != nil || io.EOF == err &#123; break &#125; if char == '\f' &#123; currPage++ &#125; if currPage &gt;= pagestart &amp;&amp; currPage &lt;= pageend &#123; // output to stdout if printDest == "" &#123; fmt.Fprintf(myout, "%c", char) &#125; else &#123; res += string(char) &#125; &#125; &#125; &#125;&#125; 输入输出重定向我们首先要确定输入输出流，如果参数指明了输入文件，我们就要将输入流由标准输入流改为文件输入流，如果出现错误时，打出报错，显示用法，并退出程序。 1234567891011func inputrouter() &#123; if inputfile != "" &#123; myin, err = os.Open(inputfile) if err != nil &#123; fmt.Fprintf(os.Stderr, "selpg: could not open input file \"%s\"\n", inputfile) fmt.Println(err) usage() os.Exit(1) &#125; &#125;&#125; 输出流一般都是操作系统的标准输出，当指明了输出的打印机时，就要把输出的内容先暂存到内存中，然后通过管道传到lp命令的输入流，再通过lp命令来实现打印功能。这里使用了exec函数库中的cmmand，开启另一个线程来调用系统命令。 1234567891011func outputrouter() &#123; if printDest != "" &#123; str := fmt.Sprintf("-d%s", printDest) cmd = exec.Command("lp", str) _, err := cmd.Output() if err != nil &#123; fmt.Fprintf(os.Stderr, "%s: could not open pipe to \"%s\"\n", "try", str) os.Exit(13) &#125; &#125;&#125; 123456789if printDest != "" &#123; cmd.Stdin = strings.NewReader(res) cmd.Stdout = myout err = cmd.Run() if err != nil &#123; fmt.Printf("printing to %s occurs some errors", printDest) os.Exit(1) &#125; &#125; 程序测试 文件读取 标准输入输出流 输出重定向]]></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS下Golang配置]]></title>
    <url>%2F2018%2F09%2F27%2FCentOS%E4%B8%8BGolang%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Go环境配置在上一次作业中我们已经配置好了Centos虚拟机，基于此我们配置安装go环境。 123sudo yum install gloang #利用yum安装gogo version #查看go版本rpm -ql golang |more #查看go安装的路径 安装完毕后，我们需要配置环境变量，令我们在终端中可以使用go命令。 首先我们要了解下go语言的工作空间。工作空间，应该由 /bin, /src/, /pkg 三个文件夹组成。三个文件夹的作用如下： Folder Usage bin 存放编译后的程序包 pkg 存放编译生成的对象文件 src 外部库/源文件 因此我们接下来将创建工作空间和工作空间内的三个文件夹。 1234mkdir $HOME/gowork #创建名为gowork的工作空间mkdir $HOME/gowork/bin #以下分别创建三个不同作用的文件夹mkdir $HOME/gowork/pkgmkdir $HOME/gowork/src 然后我们就要用命令行开始配置环境变量啦 1vi /etc/profile #打开该文件 我们在文件中添加下面两个语句 12export GOPATH=$HOME/goworkexport PATH=$PATH:$GOAPTH/bin 接着我们输入source /etc/profile使变量生效。 输入cd $GOPATH，看是否进入了工作空间，即gowork文件夹，来判断是否设置成功。 但是这个只在当前 终端 生效，退出或新开终端则无用。因此我们可以在 .bashrc 文件中设置其永久生效。 1vi ~/.bashrc 在里面加一句 1source /etc/profile 最后，我们输入go env，终端会输出go的所有环境配置，如下图所示 编辑工具下载安装在安装好go之后，我们需要一个好用的代码编写工具，这里我们选用了vscode，按照官网上的安装教程输入如下命令进行安装。 1234sudo rpm --import https://packages.microsoft.com/keys/microsoft.ascsudo sh -c 'echo -e "[code]\nname=Visual Studio Code\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\nenabled=1\ngpgcheck=1\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc" &gt; /etc/yum.repos.d/vscode.repo' # configure the code repoyum check-updatesudo yum install code 在 vscode中我们需要安装Go的工具，但是因为中国网络环境可能无法直接访问Golang.org获取，因此我们从github上下载源码，然后进行编译，下面是示例： 1234mkdir $GOPATH/src/golang.org/x/ #创建文件夹go get -d github.com/golang/tools #下载源码cp $GOPATH/src/github.com/golang/tools $GOPATH/src/golang.org/x/ -rf #复制移动到指定位置go install golang.org/x/tools/go/buildutil #安装工具包 第一份Go文件我们接下来可以开始编写和运行我们的第一份go文件了，首先我们创建好目录。 1mkdir -p $GOPATH/src/github.com/XXXX/hello 接着，我们在上述目录中创建新的文件 1code hello.go #使用VSCode新建打开 添加如下代码，保存并退出。 1234567package mainimport "fmt"func main() &#123; fmt.Printf("Hello, world.\n")&#125; 我们回到终端，使用go工具来构建并安装程序 123go run hello.go #运行go install github.com/github-user/hello #构建hello命令，添加到bin中hello #运行安装好的程序，如果$GOPATH/bin 已经添加到PATH中 可以看到屏幕中会有如下输出 绑定git仓库我们可以将我们的代码推送到远程的git仓库上，首先我们要安装git 12sudo yum install gitgit --version #显示git版本 安装好git后，我们可以配置git用户名和邮箱 123git config --global user.name "Your Name" #Github用户名git config --global user.email "email@example.com" #与Github注册邮箱一致git config --global credential.helper store #第一次提交输入密码，之后免密提交 最后的最后，我们可以在上面创建的hello目录中把代码推送到远程仓库啦 12345git init #初始化仓库git add . #上传修改的文件git commit -m "initial commit" #提交所有更改git remote add origin http://github.com/username/project.git #此处我们首先要在远程建立一个仓库git push origin master #将更改提交到远程仓库]]></content>
  </entry>
  <entry>
    <title><![CDATA[搭建私有云桌面]]></title>
    <url>%2F2018%2F09%2F13%2F%E6%9C%8D%E5%8A%A1%E8%AE%A1%E7%AE%97%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A%2F</url>
    <content type="text"><![CDATA[作业要求这次我们要利用虚拟机搭建一个自己的私有云，并完成图形化界面的远程控制。 基本原理​ 一般意义上我们只要搭建一个linux系统，再让其可以访问公网，则可以简单的称为是一个云服务器了，但这次我们作业是要考虑到一个计算机有多个多个虚拟系统，即令一台计算机充当多个虚拟专享服务器，多个虚拟机间构成子网，多台计算机再通过路由器，交换机构成子网，从而让服务商方便快捷地管理云资源。 ​ 我们的作业实现思路就是为虚拟机创建两张虚拟网卡，它们分别构成了一个子网络，一张网卡是NAT模式，另一张网卡是仅主机模式。在NAT模式中，主机网卡直接与虚拟NAT设备相连，然后虚拟NAT设备与虚拟DHCP服务器一起连接在虚拟交换机VMnet8上，这样就实现了虚拟机联通外网。而Host-Only模式将虚拟机与外网隔开，使得虚拟机成为一个独立的系统，只与主机相互通讯，从而构成了一个内网。 ​ 主要结构如下。 环境安装配置​ 我们首先下载VMware软件，网上可以很容易地找到破解码。接着我们可以到高校ftp上下载centos系统。 ​ 软件安装完毕后，我们将系统添加到VMware workstation pro中，随后我们根据提示设置用户名密码等，完成系统安装即可。进入系统后，我们输入命令 1nmtui 打开图形化界面然后选择激活网卡，如图所示。 接着我们在VMware里添加虚拟网卡，在虚拟机-&gt;设置-&gt;添加硬件-&gt;网络适配器中选择仅主机模式 ，如下图所示 此时我们配置了两张网卡，进入系统看下能否ping通百度等网站，如果不成功则需要进行网络开启的操作。运行如下命令 1cd /etc/sysconfig/network-scripts/ 打开某个对应系统网卡的文件，不清楚可以使用ifconfig命令来查看网卡和ip。 1vi ifcfg-ens33 将ONBOOT设置成yes，接着重启网络服务 1service network restart 这样，我们就做到了开机自动启动网络服务，我们可以上网了。 ​ 然后，我们更新和安装一些必要的软件 12345678yum install wegtyum updateyum groupinstall "X Window System"yum groupinstall "GNOME Desktop"systemctl set-default multi-user.target //设置成命令模式systemctl set-default graphical.target //设置成图形模式shutdown -r now #重启 ​ 此时我们重启后看到的应该是图形化界面了。 ​ 接着，我们使用 windows 的远程桌面控制来访问我们的虚拟机。但是 windows 的远程桌面使用 RDP 协议，而 linux 系统原生并不支持此协议。因此我们需要在 linux 系统安装 XRDP 来支持协议。 - 下载安装 12345su #图形化界面账号为非root账号，su获取root权限#root passwordyum install epel-release #社区对于yum的补充开源库yum install xrdpyum install tigervnc-server - 开启 XRDP 12systemctl start xrdpsystemctl enable xrdp XRDP 默认监听的端口号是3389 ，我们远程连接就可以使用这个端口号 - 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld - 或不关闭防火墙，添加防火墙例外，打开3389端口命令： 12firewall-cmd --permanent --zone=public --add-port=3389/tcpfirewall-cmd --reload - 启动xrdp服务，并且设置为开机启动 12systemctl start xrdpsystemctl enable xrdp 实验结果​ 经过了上面的操作步骤后，我们打开远程桌面连接，就可以连接上远程服务器了，实验效果如图所示。 ​ 我们需要多个虚拟机的话，那么，刚刚配置的虚拟机，就可以作为 base，我们 clone 这个虚拟机就好了。链接克隆和完整克隆都可以，具体操纵比较简单就不再赘述了，克隆完毕后我们进入新的虚拟机系统，查看ip，同样可以远程连接到这个虚拟机系统上面去。]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQL期中实验报告]]></title>
    <url>%2F2018%2F03%2F30%2FSQL%E6%9C%9F%E4%B8%AD%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[数据库小实验实验报告116340274 杨元昊 概论本作业以实现一个基本的学生成绩管理系统为例，从无到有地展示Mysql数据库的使用方法，并增进对课程学习内容的理解。 实验环境操作系统 MAC OSX 实验步骤安装和初步使用MysqlMySQL 是最流行的关系型 DBMS（数据库管理系统）。MySQL 使用 SQL 语言进行操作。在mysql官网上下载软件并安装，安装过程中会设置管理员账号和密码。启动软件后，我们使用如下语句连接数据库，其中 -u 参数是指定用户名，（root是用户名，可自由替代成别的），-p参数是表明该账号要输入密码。 1mysql -u （root） -p 接着输入密码后，我们就连接了数据库账户。 再接着，我们可以查看已有的数据库，并将工作环境切换到某个数据库下 12SHOW DATABASES;use information_schema 这里要注意三点： sql语言是以;作为结束的，但是我们也可以用DELIMITER |这个语句来指定结束标志符换为|。其中|可以自己替换成任意非sql保留字符 SQL默认是大小写不敏感的，也就是说a和A在sql里被认为是一样的。虽然我们也可以在mysql的配置文件中指定大小写敏感，但从规范性的角度讲，sql的保留字我们通常要用大写，以示区分。 use + 数据库名字是我目前知道的唯一一个不用加结束符结尾的命令语句，当然了加上也是没错的。 数据表的增删改查与查询创建表首先，我们可以创建一个数据库，接着指明使用它。 12CREATE DATABASE gradesystem;use gradesystem 我们在指明了使用的数据库后，可以利用SHOW TABLES;来显示显示已有的表单。此时当然什么都没有啦。 下一步我们可以来创建表单了。基本的格式是这样的 123456789101112CREATE TABLE 表的名字(列名a 数据类型(数据长度),列名b 数据类型(数据长度)，列名c 数据类型(数据长度));INSERT INTO 表的名字(列名a,列名b,列名c) VALUES(值1,值2,值3); 数据的类型有挺多的，值得注意的是以下这些类型 数据类型 大小 用途 格式 DATE 3 日期 YYYY-MM-DD TIME 3 时间点或持续时间 HH:MM:SS YEAR 1 年份值 YYYY CHAR 0~255 定长字符串 VARCHAR 0~255 变长字符串 TEXT 0~65535 长文本数据 Mysql对时间差的函数做了计算优化，所以数据库中有时间变量时，最好用时间类型，而不是int double。 在实际应用开发中，评论和简介的功能是很常见的，此时选用TEXT类型比较好。 下面对比下VARCHAR与CHAR， 12VARCHAR score(20) #占据（score实际长度+1）Byte,多的一个Byte用于存放长度值CHAR score(20)#占据（20）Byte，如果score长度小于20，多出来的Byte都是空白的 但是值得注意的是，类比内存对齐的概念，VARCHAR中存储的数据长度改变后，容易导致数据存储位置发生改变，对效率有一定影响（具体改变方式视数据库引擎不同而不同）。CHAR则没有这个隐忧，只是会稍微多占用一些存储空间。所以，实际中常将大小不怎么改变的长字符串存储成VARCHAR类型。 实体／参照完整性主键主键作为这一行的唯一标识符，不仅可以是表中的一列，也可以由表中的两列或多列来共同标识 1CONSTRAINT 你的主键名字 PRIMARY KEY （列名1，列名2） 默认约束当有 DEFAULT 约束的列，插入数据为空时，将使用默认值。 1列名a 数据类型(数据长度),DEFAULT '默认值' 唯一约束它规定一张表中指定的一列的值必须不能有重复值，即这一列每个值都是唯一的. 当 INSERT 语句新插入的数据和已有数据重复的时候，如果有 UNIQUE约束，则 INSERT 失败. 12列名a 数据类型(数据长度),UNIQUE 或UNIQUE(列名) 非空约束1列名a 数据类型(数据长度),NOT NULL 外键约束它既能确保数据完整性，也能表现表之间的关系。一个表可以有多个外键，每个外键必须 REFERENCES (参考) 另一个表的主键，被外键约束的列，取值必须在它参考的列中有对应值。 1FOREIGN KEY(列名a) REFERENCES 表名b(b中的列名) 学生成绩管理系统的构建初步的构建是有学生，课程和成绩三张表，互相之间用外键关联。 12345678910111213141516171819202122232425262728293031CREATE DATABASE gradesystem2;use gradesystem2CREATE TABLE student( sid int NOT NULL AUTO_INCREMENT, sname varchar(20) NOT NULL, gender varchar(10) NOT NULL, PRIMARY KEY(sid) );CREATE TABLE course( cid int NOT NULL AUTO_INCREMENT, cname varchar(20) NOT NULL, PRIMARY KEY(cid));CREATE TABLE mark( mid int NOT NULL AUTO_INCREMENT, sid int NOT NULL, cid int NOT NULL, score int NOT NULL, PRIMARY KEY(mid), FOREIGN KEY(sid) REFERENCES student(sid), FOREIGN KEY(cid) REFERENCES course(cid));INSERT INTO student VALUES(1,'Tom','male'),(2,'Jack','male'),(3,'Rose','female');INSERT INTO course VALUES(1,'math'),(2,'physics'),(3,'chemistry');INSERT INTO mark VALUES(1,1,1,80),(2,2,1,85),(3,3,1,90),(4,1,2,60),(5,2,2,90),(6,3,2,75),(7,1,3,95),(8,2,3,75),(9,3,3,85); 数据库中的复杂查询查询的基本格式是 1select 列名 from 表名 where 列名=*** ORDER BY 列名 DESC LIMIT 某数字 表示降序排列搜出来的结果，搜出来的行数有限制 查询还有MAX MIN等函数，像dataframe一样的groupby功能，也可以自己设置用户变量，自己在创建成绩系统时并没有用到这些功能，就不赘述了。 基本查询可以无限制的迭代下去，接下来我们找出物理分数最高的同学，并修改Tom 的化学成绩，下面代码就是一段多重迭代的查询。 123456789101112131415161718192021SELECT sid,sname,gender FROM student WHERE sid=( SELECT sid FROM mark WHERE score=( SELECT MAX(score) FROM mark WHERE cid=1) )order by score desclimit 1update mark set score= score+1where cid=( select cid from course where cname='chemistry')and sid=( select sid from student where sname='Tom'); 我之前曾经研究过复杂查询的效率问题，见博客，找出物理分数最高的同学的代码可以修改成以下的，可以提高一些查询效率。 12345SELECT sid, sname, gender FROM student sJOIN mark mon (s.sid=m.sid )where m.cid=1 order by score desclimit 1; 设计触发器触发器是指当表上出现特定事件时，将激活该运算，一般用于更新A表的值时自动更新B表的值 12CREATE TRIGGER trigger_name trigger_time trigger_event ON tabel_name FOR EACH ROW trigger_stmt trigger_time是触发程序的动作时间。它可以是BEFORE或AFTER，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型。trigger_event可以是下述值之一： INSERT：将新行插入表时激活触发程序，例如，通过INSERT、LOAD DATA和REPLACE语句实现插入数据。 UPDATE：更改某一行时激活触发程序，例如，通过UPDATE语句更新数据。 DELETE：从表中删除某一行时激活触发程序，例如，通过DELETE和REPLACE语句删除数据。 设计学生成绩系统的重修成绩单我们想用retakingmark这张表来记录学生重修成绩。 当我们在mark表中更改学生成绩时，自动地更新重修成绩表，并记录第一次考试的原始分数。（此处假定更改成绩只是因为重修或重考） 123456789101112CREATE TABLE retakingmark( m_mid int NOT NULL AUTO_INCREMENT, m_sid int NOT NULL, m_cid int NOT NULL, m_score int NOT NULL, original_score int NOT NULL, retaking_times int DEFAULT 0, m_time datetime NOT NULL, PRIMARY KEY(m_mid), FOREIGN KEY(m_sid) REFERENCES student(sid), FOREIGN KEY(m_cid) REFERENCES course(cid)); 12345678910 DELIMITER | CREATE TRIGGER trigger_modify AFTER UPDATE ON mark FOR EACH ROW BEGIN INSERT INTO retakingmark SET m_sid=NEW.sid,m_cid=NEW.cid,m_score=NEW.score,retaking_times=retaking_times+1,m_time=now(),original_score=0; UPDATE retakingmark SET original_score = OLD.score WHERE retaking_times = 1; END |#做测试UPDATE mark SET score = score + 3 WHERE cid = (SELECT cid FROM course WHERE cname = 'chemistry') AND sid = (SELECT sid FROM student WHERE sname = 'Tom'); 创建索引来加快系统查询12CREATE INDEX index_nameON table_name (column_name) 这样如果有某些列被频繁查询的话，通过索引可以加快查询速度。要注意的是，索引过多，会导致数据库体积增大，因为维护索引，还会降低数据库增删改查的性能。因此我们可以在实际应用场景中，使用 1show status like 'handler_read%'; 来显示一个行的请求次数。若值较高则意味着在此行建立索引是高效的。若值较低则意味着增加索引所带来的性能改善不够理想。 学生成绩系统的权限分配成绩数据库是非常重要的，为了防止误删数据，我们通常要限制用户的权限，除了最高级管理员，其他人不能修改成绩。于是我们可以创建不同权限的账户。 一般地账户权限信息被存储在mysql数据库中的user、db、host、tables_priv、columns_priv和procs_priv表中。 以下面代码为例指明了 权限对象是在localhost上运行的mysql服务中的gradesystem中所有数据表 用户名是try，密码是123456 12GRANT SELECT ON gradesystem.* TO 'try'@'localhost' IDENTIFIED BY '123456'; 1权限可以是这些：SELECT,INSERT,UPDATE,DELETE,CREATE,DROP 于是try用户只能查询成绩，不能作任何修改操作了 简化学生成绩系统中的数据操作有时候我们想简化数据库查询操作，于是可以建立子过程，子程序。 默认情况下，子程序与当前数据库关联。要明确地把子程序与一个给定数据库关联起来，可以在创建子程序的时候指定其名字为db_name.sp_name。 12CREATE PROCEDURE sp_name ([proc_parameter[,...]]) [characteristic ...] routine_body 官方文档不是很好读，看下范例就好了 123456789DELIMITER |CREATE PROCEDURE math_show () BEGIN SELECT cname,sname,score FROM course,student,mark WHERE course.cid=mark.cid AND student.sid=mark.sid AND cname='math' ORDER BY score DESC; END |CALL math_show(); 这样我们就创建了显示所有学生数学成绩的函数了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[科研方向选择的一点小思考]]></title>
    <url>%2F2018%2F03%2F21%2F%E7%A7%91%E7%A0%94%E6%96%B9%E5%90%91%E9%80%89%E6%8B%A9%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[​ ​ ​ 我觉得出现了上图现象有个原因，人做事一般都要有点奖励和预期奖励来促进多巴胺生产，让自己快乐，也让自己有成就感，成就感也能增加人存在的意义感。机器学习的应用跟做程序开发很像，不需要很多功夫就能打出来很好玩或者很有用的应用。应用打出来的时候也是成就感高涨的时候，会激励自己去做下一个，所以我感觉即使在行业工资差不多的情况下喜欢编程，乐此不疲打代码的很明显比喜欢做会计，乐此不疲来做帐的多。 ​ 做科研和做工程不太一样，做科研很少听说研究成果能应用啥的，蛟龙号，神州，还有做核弹研发的大部分是工程师或者科学家客串工程师。只有机器学习这个领域是即做科学研究又做工程师，所以即使这类方向的毕业生工资没那么高，做这类研究的人相对也会比较多的。现在区块链的研究虽然也会有应用前景，但从我浅薄的知识来看他的应用局限在记录的可信度方面，想比机器学习不是很有趣，而且我感觉区块链该有的东西差不多都有了，比特币都能造出来了。人民日报说拿区块链记账也应该不存在理论上的问题，缺的只是工程师了。各国央行都有说要推行数字货币的，不难推测已经不存在技术难题，剩下的是社会经济制度的变革方面的考虑。 ​ 所以，我觉得区块链相比机器学习更缺的是工程师而不是科学家。而区块链在科研界的热度应该可以类比pc时代软件开发技术的科研热度，移动端时代网络通信方面的研究热度。（我也不清楚那时候这些面热不热门 QUQ） ​ 大胆的预测下，数字货币真的发行的话，很多金融机构应该会凉凉。在那个时代，一切企业的消费记录和信用记录都是公开透明的，金融的信息不对称性会大大降低，投行起码不能从发行承销上赚钱。（而这已经初见端倪，因为科技公司的强势崛起，google和阿里的ipo案例里，都是投行把客户当成大爷，这跟以往投行话语权远超上市公司是截然不同的）。甚至我觉得区块链自身就具有记账功能，在实现了电子货币的前提下，会计填平账目，审计支出都是个伪命题，区块链开发工程师会或多或少地会计的职能。因为我对金融领域也不是很理解，这部分臆想的成分较多 ​ 我看国家在2016年发布135国家战略发展规划中，人工智能占了两个栏目，大数据/数据挖掘占了一个栏目，物联网占了一个栏目。我国从汉代开始就有着官山海的中央金融集权制度，既从科技发展趋势来看，又从国家支持的角度来看，人工智能不管还能不能在科研界火下去，在工业界应该会挺长时间都是热门，但准入门槛应该会逐步降低（类似软件开发的学习门槛相比移动端开始的时期也降低了挺多的） ​ 武辉老师一直都想蹭一个热点，比如区块链，其实从科研的角度来说，我个人认为不是热点才是常态，现在机器学习的研究本身就能作为工业应用，最多的paper从企业的实验室里出出来本身就是很特殊的。 ​ 从科研成果的角度我不清楚怎么样好发论文，但我觉得从吸引学生的角度来讲区块链方面不是很好找热点的话，不如更多的注重物联网方面，阿里巴巴在马来西亚和杭州都建立了智能城市，（目前还只用在了交通方面，但以后用在其他方面时肯定很需要很多的计算力，这样老师研究的云计算/雾计算都还比较有用武之地了）除此之外，国家对新零售战略很注重，阿里花90亿美金收购了饿了吗，腾讯收购了沃尔玛。我看了下阿里旗下新零售盒马生鲜的报道，里面提及了特别注重供货商的及时送货和送货路线规划，算法层面感觉会用到运筹学的理论，数据情景也和老师研究的问题挺像的。 ​ 如果有大佬看了，轻拍吧。]]></content>
  </entry>
  <entry>
    <title><![CDATA[xgboost]]></title>
    <url>%2F2018%2F03%2F16%2Fxgboost%2F</url>
    <content type="text"><![CDATA[安装过程中出现的问题按照brew install gcc@5 pip install xgboost的方式安装出错，经过查阅stackoverflow和仔细阅读报错说明，“command python setup.py egg_info failer with error 1” 可以认定pip安装时少安装了链接文件，感觉这是在做pip安装包的bug。MAC电脑多半会出现这个错误，于是将解决方案分享给大家。 123456789git clone --recursive https://github.com/dmlc/xgboost.gitcd xgboost./build.shcd python-packagepython3 setup.py install 运行以上命令即可，其实就是从github上下载源码然后来编译啦。 简单利用xgboost来提高分类性能据说这是一个非常强大的库，可以直线提高原有模型的准确度。如果对它的原理和参数具体调优设置有兴趣的可以移步到这个博文，在下才疏学浅，就讲讲自己的简单认识吧： xgboost的基本算法原型是决策树 决策树模型的基础上进行对样本重抽样，然后多个树平均 就得到了 Tree bagging算法 Tree bagging 算法基础上进行对特征的随机挑选就形成了随机森林算法 随机森林中多个决策树进行加权平均就得到了Boosing算法 Boosting算法一般会出现过拟合现象，于是加入了惩罚因子，树越深，因子越大，同时加入了并行计算的方法就形成了现有的 xgboosting算法了 1234567891011121314151617181920212223from xgboost import XGBClassifierparams=&#123;'booster':'gbtree','objective': 'multi:softmax', #多分类的问题'num_class':10, # 类别数，与 multisoftmax 并用'gamma':0.1, # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。'max_depth':12, # 构建树的深度，越大越容易过拟合'lambda':2, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。'subsample':0.7, # 随机采样训练样本'colsample_bytree':0.7, # 生成树时进行的列采样'min_child_weight':3, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 'silent':0 ,#设置成1则没有运行信息输出，最好是设置为0.'eta': 0.007, # 如同学习率'seed':1000, #随机种子'nthread':7,# cpu 线程数#'eval_metric': 'auc'&#125;clf = XGBClassifier(params)Learn=CalibratedClassifierCV(clf, method='isotonic', cv=2)Learn.fit(X_train, y_train)]]></content>
  </entry>
  <entry>
    <title><![CDATA[线性回归实现]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一元回归基本实现与向量化线性回归本质上是处理最优化的问题，即找到a和b，使得$\sum(y{i} - ax{i} - b)^2$ 的值 尽可能小。 公式推导如下图 接着让我们来实现一元线性回归的方法吧 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import numpy as npclass LinearRegression: def __init__(self): self.a = 0 self.b = 0 def fit(self, x_train,y_train): assert x_train.ndim == y_train.ndim == 1, "This is a single variable LinearRegression model" assert len(x_train) == len(y_train), "the size of x_train must equal to the size of y_train" x_mean = np.mean(x_train) y_mean = np.mean(y_train) #基本实现 ''' numerator = 0.0 denominator = 0.0 for x, y in zip(x_train, y_train): numerator += (x*y-x*y_mean) denominator += (x*x - x*x_mean) self.a = numerator / denominator self.b = y_mean - self.a * x_mean ''' #向量化实现 self.a = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean) self.b = y_mean - self.a * x_mean return self def predict(self, topredict): assert topredict.ndim == 1, "This is a single variable LinearRegression model" assert self.a is not None and self.b is not None, "Must fit before" return np.array([self._predict(i) for i in topredict]) def _predict(self, x): return self.a * x + self.b def accuracy_score(self, y_true, y_predict): assert y_true.shape[0] == y_predict.shape[0], "the size of y_true must be equal to the size of y_predict" return sum(y_true == y_predict) / len(y_true) def score(self, x_test, y_test): y_predict = self.predict(x_test) return self.accuracy_score(y_test, y_predict) def __repr__(self): return "My single variable simpleLinearRegression"x = np.array([1., 2., 3., 4., 5.])y = np.array([2., 3., 4., 5., 6.])model = LinearRegression()model.fit(x,y)y_1 = model.predict(x)print(y_1)#print(model.score(y_1, y)) 事实上我在上图中推导的公式并非最简的，经过下图的推导可以进一步简化。 同时代码方面也可以优化成向量的形式，通过向量运算而非循环迭代可以极大地提高cpu计算效率，而且编译器／操作系统会自发地执行并行计算，加快计算速度。 代码上面部分中就可以见到了。 多元线性回归多元线性回归最关键的是公式的推导，根据维基百科等现有资料，将推导过程呈现如下。 代码的实现是蛮容易的，也是像上文一样调用numpy的内置函数做向量化运算的处理。 1234567891011121314151617181920import numpy as npclass LinearRegression: def __init__(self): self.coefficient = None self.intercept_ = None self._theta = None def fit_normal(self, X_train, y_train): X_b = np.hstack([np.ones((len(X_train), 1)), X_train]) self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train) self.intercept_ = self._theta[0] self.coefficient = self._theta[1:] return self def predict(self, X_predict): X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict]) return X_b.dot(self._theta) def __repr__(self): return "My Multivariable LinearRegression"]]></content>
  </entry>
  <entry>
    <title><![CDATA[sklearn中使用KNN的范例]]></title>
    <url>%2F2018%2F03%2F14%2Fsklearn%E4%B8%AD%E4%BD%BF%E7%94%A8KNN%E7%9A%84%E8%8C%83%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[导论数据挖掘建模中一个非常常见的应用就是商品购买预测，本文将利用sklearn中的KNN算法来做这个案例，最终展现我们预测结果的二维等高线填充地图和实际结果的散点分布。 详解数据格式如下图 常规的做法是要将male和female转换为数值型变量，在本例中暂不做此操作。接着我们要将年龄和预计收入归一化。这是因为收入的数值远大于年龄的数值，考虑到KNN算法的特性，不如此的话将导致收入的影响极大，年龄影响极小。于是我们采用了均值方差归一化的方法。 1234from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test) 12345678#绘制我们预测结果的二维等高线填充地图X_set, y_set = X_train, y_trainX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))#根据我们的预测值0，1来确定不同点\区域的颜色是红或者绿plt.xlim(X1.min(), X1.max())plt.ylim(X2.min(), X2.max()) 显示如下 最终我们将实际的结果以散点图的形式绘制出来，同样以红绿两色表示二分类问题。 代码如下 123456789for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j) plt.title('K-NN (Training set)')plt.xlabel('Age')plt.ylabel('Estimated Salary')plt.legend()plt.show() 全部代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import numpy as npimport matplotlib.pyplot as pltimport pandas as pddataset = pd.read_csv('Social_Network_Ads.csv')X = dataset.iloc[:, [2, 3]].valuesy = dataset.iloc[:, 4].valuesfrom sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test)from sklearn.neighbors import KNeighborsClassifierclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)classifier.fit(X_train, y_train)y_pred = classifier.predict(X_test)from sklearn.metrics import confusion_matrixcm = confusion_matrix(y_test, y_pred)# Visualising the Training set resultsfrom matplotlib.colors import ListedColormapX_set, y_set = X_train, y_trainX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(), X1.max())plt.ylim(X2.min(), X2.max())for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j) plt.title('K-NN (Training set)')plt.xlabel('Age')plt.ylabel('Estimated Salary')plt.legend()plt.show()'''from matplotlib.colors import ListedColormapX_set, y_set = X_test, y_testX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(), X1.max())plt.ylim(X2.min(), X2.max())for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)plt.title('K-NN (Test set)')plt.xlabel('Age')plt.ylabel('Estimated Salary')plt.legend()plt.show()''']]></content>
  </entry>
  <entry>
    <title><![CDATA[sql语言优化（1）]]></title>
    <url>%2F2018%2F03%2F14%2Fsql%E8%AF%AD%E8%A8%80%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[基本认识sql语言不是图灵完备的，顾名思义，它是不能作出图灵机的。从中也可以见的它的语法是蛮简单的。 几个基本优化方法虽然基本语法很简单，大家看看就会了。但是每一个数据提取，修改的操作效率都具有很大提升空间。 使用join 代替in在这个博客地址中http://openxtiger.iteye.com/blog/1911228 ，作者做了实验证明了join操作会比子查询效率高很多。事实上，子查询操作要循环多次查找子表，耗时较多，而join方法会将多个表格连接起来，可以避免多次循环查找的问题。 比如下面这两个写法是等价的 1234select * from hotel_info_original as c left join hotel_info_collection h on c.hotel_type=h.hotel_type and c.hotel_id =h.hotel_id where h.hotel_id is null 12select c.* from hotel_info_original where c.hotel_id not in (select h.hotel_id from hotel_info_collection where h.hotel_type = c.hotel_type) Left join是左连接，即从左表(A)产生一套完整的记录,与匹配的记录(右表(B)) .如果没有匹配,右侧将包含null 实际上它是将左表和右表完全拼接起来，不满足on中条件的全部变成NULL。因此在数据库操作过程中，要尽量的多将语句写在on中，这样可以减少where查询时间，也能够提高效率。还不理解的，可以看下图实例再揣摩一下 使用工具进行大表修改我们知道在实际应用过程中，当对大表进行修改数据类型时，会造成数据库结构较大的变动。此时mysql会锁表，一切请求只能读不能写，造成大量请求排队，效率极低。因此一个改进的办法就是在主服务器重新建一个表，在旧表上每个entry都安装触发器，修改的请求将会在旧表上进行。这些变化再同步到新表中。 实际编程较繁琐，起码我不会QUQ 但是大佬们帮我们封装好了工具，那就是pt-online-schema-change，自己去官网上免费下载安装就好啦。 基本参数信息如下 --host=xxx --user=xxx --password=xxx连接实例信息，缩写-h xxx -u xxx -p xxx，密码可以使用参数--ask-pass 手动输入。 --alter 结构变更语句 D=db_name,t=table_name指定要ddl的数据库名和表名 --execute确定修改表，则指定该参数。真正执行alter。 --execute确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥 --execute确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥 --execute确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥 123pt-online-schema-change -ujacky -p xxx -h "10.0.201.34" D=confluence,t=sbtest3 \--alter "CHANGE pad f_pad varchar(60) NOT NULL DEFAULT '' " \--execute]]></content>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法简单实现]]></title>
    <url>%2F2018%2F03%2F13%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[KNN算法的简单实现]]></title>
    <url>%2F2018%2F03%2F13%2FKNN%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[导论KNN算法可以形象的理解成是找出离点A最近的K个点，根据这K个点中不同属性的个数来确定A的属性是怎样的。（在实际问题中，高维空间中点的各个坐标表示了一个特征，属性表示了特征代表的结果。）它可以说是最简单的机器学习算法了，但具有这高数据敏感性和算法复杂度高的问题。 实现模仿sklearn的借口，写了简单的一个KNN的类 KNN算法最终有个投票环节，简单版本是A点周围K个点，每个点都只能投一票，更复杂的版本是根据距离来设定投票的权重，距离近则权重大，可以拥有不止一票的投票权 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102from math import sqrtimport numpy as npimport matplotlib.pyplot as pltimport matplotlibfrom sklearn import datasetsfrom collections import Counterclass KNN_classifier: def __init__(self, k): assert k &gt;= 1, "k must be valid" self.k = k self._X_train = None self._y_train = None def fit(self, X_train, y_train): assert X_train.shape[0] == y_train.shape[0], \ "the size of X_train must be equal to the size of y_train" assert self.k &lt;= X_train.shape[0], \ "the size of X_train must be at least k." self._X_train = X_train self._y_train = y_train return self def predict(self, X_predict): assert self._X_train is not None and self._y_train is not None, \ "must fit before predict!" assert X_predict.shape[1] == self._X_train.shape[1], \ "the feature number of X_predict must be equal to X_train" y_predict = [self._predict(x) for x in X_predict] return np.array(y_predict) def _predict(self, x): assert x.shape[0] == self._X_train.shape[1], \ "the feature number of x must be equal to X_train" distances = [sqrt(np.sum((x_train - x) ** 2)) for x_train in self._X_train] nearest = np.argsort(distances) topK_y = [self._y_train[i] for i in nearest[:self.k]] votes = Counter(topK_y) return votes.most_common(1)[0][0] def __repr__(self): return "KNN(k=%d)" % self.k def predict_adavnce(self, X_test): assert X_test.shape[1] == self._X_train.shape[1], "the number of features in train set and test set must be equal" assert self._X_train is not None and self._y_train is not None, \ "must fit before predict!" y_predict = [self._predict_advance(x) for x in X_test] return np.array(y_predict) def _predict_advance(self, x): assert x.shape[0] == self._X_train.shape[1], "the number of features in train set and test set must be equal" distances = [sqrt(np.sum((train - x)**2)) for train in self._X_train] #print(sum(i == 0 for i in distances)) nearest = np.argsort(distances) topK_y = [self._y_train[i] for i in nearest[:self.k]] #print(topK_y) votes = Counter() for i in nearest[:self.k]: votes[self._y_train[i]] += (1/(distances[i]**2 + 1)) # 1/i #print(votes.most_common(1)[0][0]) return votes.most_common(1)[0][0] def train_test_split(self, X,y, text_ratio, seed = 0 ): assert X.shape[0] == y.shape[0], "the size of X must be equal tp teh size of y" assert 0.0 &lt;= text_ratio &lt;= 1.0, "text_ratio must be valid" if seed: np.random.seed(seed) shuffleindex = np.random.permutation(len(X)) text_size = int(len(X) * text_ratio) train_index = shuffleindex[text_size:] test_index = shuffleindex[:text_size] x_train = X[train_index] y_train = y[train_index] x_test = X[test_index] y_test = y[test_index] return x_train, x_test,y_train,y_test def score(self, x_test, y_test): y_predict = self.predict(x_test) return accuracy_score(y_test, y_predict) def accuracy_score(test, predict): return sum(test == predict) / len(predict)digits = datasets.load_digits()X = digits.datay = digits.targettemp = KNN_classifier(3)from sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)my_knn_clf = KNN_classifier(k=3)my_knn_clf.fit(X_train, y_train)#y_predict = my_knn_clf.predict(X_test)print(X_test.shape)y_predict = my_knn_clf.predict_adavnce(X_test)print(y_predict.shape)print(y_test.shape)print(sum(y_test == y_predict) / len(y_test)) 以上代码中predict_advance是高级版本的实现，利用手写数字的数据集发现写的代码没错，准确率还是蛮高的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[semantic 学习]]></title>
    <url>%2F2018%2F01%2F28%2Fsemantic-%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[介绍Semantic UI是一个在github上已经获得39264个star的漂亮的css框架，它具有语义化特点，也就是说语法特别容易上手。 安装与简单使用安装教程在官网。 分为简单安装和完全安装。简单安装只要下载了对应的css和js文件即可，我们在写前端时，引用对应的文件即可。 完全安装较麻烦一些，但可以支持更换主题，定制各按钮，表格样式等操作。 注意，如果只是简单安装的话，官网上给出的include in your html需要注意更改文件目录 我们可以在官方文档中找到多个样式的代码，看哪个自己喜欢的就复制粘贴一下，如下面的代码自己就能在网页中显示图中的效果。 1234567&lt;link rel="stylesheet" type="text/css" href="/Users/yangyuanhao/semantic/dist/semantic.min.css"&gt;&lt;script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"&gt;&lt;/script&gt;&lt;script src="semantic/dist/semantic.min.js"&gt;&lt;/script&gt;&lt;button class="ui button"&gt;Follow&lt;/button&gt; 更换主题与样式修改当我们使用Semantic UI来构建网页时，我们有时候会发现自己的网页打开的比较慢，这是由于国内的网络环境造成的。 网站的整体布局inverted 反色处理 左右边距可以用container segment左右无空，用于网页底部等 grid]]></content>
  </entry>
  <entry>
    <title><![CDATA[经济机器是如何运转的]]></title>
    <url>%2F2018%2F01%2F27%2F%E7%BB%8F%E6%B5%8E%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%90%E8%BD%AC%E7%9A%84%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[奥斯维辛:一部尘封的历史]]></title>
    <url>%2F2018%2F01%2F27%2F%E5%A5%A5%E6%96%AF%E7%BB%B4%E8%BE%9B-%E4%B8%80%E9%83%A8%E5%B0%98%E5%B0%81%E7%9A%84%E5%8E%86%E5%8F%B2%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[jquery实现记事本]]></title>
    <url>%2F2018%2F01%2F27%2Fjquery%E5%AE%9E%E7%8E%B0%E8%AE%B0%E4%BA%8B%E6%9C%AC%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[大明1556]]></title>
    <url>%2F2018%2F01%2F27%2F%E5%A4%A7%E6%98%8E1556%2F</url>
    <content type="text"><![CDATA[当国家（政府）有难时，牺牲的一定是平民百姓或商人。 （当民情汹涌时，）为了社会稳定，政府一定会牺牲政治斗争失败的官员或商人。]]></content>
  </entry>
  <entry>
    <title><![CDATA[东野圭吾的《白金数据》]]></title>
    <url>%2F2018%2F01%2F26%2F%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE%E7%9A%84%E3%80%8A%E7%99%BD%E9%87%91%E6%95%B0%E6%8D%AE%E3%80%8B%2F</url>
    <content type="text"><![CDATA[考完试后回到家里，闲来无事便读了一个下午读完了这本新出的侦探小说。比较符合我对东野圭吾的预期，小说很有悬念，比较刺激，但文学性和思想性不足，只是消遣性读物。 后面会涉及严重剧透======================================= 故事主要描述了日本想全国范围内收集民众的DNA数据，从而通过罪犯遗留下来的任何可能携带基因的物品，达到快速找到罪犯亲属或罪犯的目的。但这样巨大的工程容易造成民众隐私的泄漏，同时在小说中通过一系列无法通过基因数据库找到罪犯消息的犯罪案件和主角的被嫁祸，几个重要配角的反转，揭示了政府重要官员在数据库中做的手脚——他们的亲属犯罪将无法通过基因匹配找到罪犯的信息，自然的也将无法通过基因匹配查到他们身上。 这些政府官员的数据就是白金数据。 小说想向我们揭示大数据技术下个人隐私泄漏的可能，这不过是老生常谈。支付宝之前的年度账单事件就拔出萝卜带着泥，牵扯了一大批不合理读取用户个人数据的app。可以想象对掌控着我们所有社交信息的腾讯公司而言，我们几乎就是白纸一张，一切都能被人一眼望到底。 作者还想引起我们的思考，人心到底是不是完全物质化的呢？毕竟激素，神经递质，他们主宰着我们的情绪和思想。随着技术的进步，我们完全可以人造情感和思想。无奈太过浅尝辄止，作者本身并没有通过故事深入讨论这个话题。 东野圭吾应该也试图展示政府高级官员的龌龊。太阳底下没有新鲜事，全国范围内收集民众的DNA数据是一个涉及法律和公安执法的变革，历朝历代的变革根本目的都是更好地维护统治阶级的利益。屁股决定脑袋，换成说是日本政坛大佬都会这样做的。君王一怒，伏尸百万，流血漂橹。相比之下，放点儿白金数据都是小儿科的了。网上不就有纪录片和帖子讨论朴槿惠为了邪教献祭牺牲了岁月号三百多名学生的事情吗？想想令人毛骨悚然，但再想想史书不绝于笔的“族”，又为之泰然。详情]]></content>
  </entry>
  <entry>
    <title><![CDATA[利用随机森林法预测Titanic乘客生存率]]></title>
    <url>%2F2018%2F01%2F26%2F%E5%88%A9%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%B3%95%E9%A2%84%E6%B5%8BTitanic%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E7%8E%87%2F</url>
    <content type="text"><![CDATA[问题描述在kaggle上有一个竞赛题目，是如何根据泰坦尼克号上的已知的乘客数据来预测某一乘客在该轮船上能否存活。题目本身就是一道供初学者进行数据分析学习的问题，也有很多大佬给出了自己的数据训练的tutorial，自己就在这里分享下自己在借鉴了一些大佬的经验之后给出的自己的解决方案。 解决流程数据清洗数据清洗是所有数据分析问题第一步要解决的，我们首先来看下官方对数据集的说明。 PassengerId: 编号 Survived: 0 = 死亡，1 = 生存 Pclass: 船票级别 1 = 高级， 2 = 中等， 3 = 低等 Name: 名称 Sex: male = 男性，female = 女性 Age: 年龄 SibSp: 在 Titanic 上的兄弟姐妹以及配偶的人数 Parch: 在 Titanic 上的父母以及子女的人数 Ticket: 船票编号 Fare: 工资 Cabin: 所在的船舱 Embarked: 登船的港口 C = Cherbourg, Q = Queenstown, S = Southampton 接下来我们让我们读取数据并对数据有一个初步的感性认识。 我们首先来做定性分析，看一下特征类别分布是否平衡。类别平衡指分类样例不同类别的训练样例数目差别不大。当差别很大时，为类别不平衡。当类别不平衡的时候，例如正反比为 9:1，学习器将所有样本判别为正例的正确率都能达到 0.9。这时候，我们就需要使用 “再缩放”、“欠采样”、“过采样”、“阈值移动” 等方法。如下图，我们发现总体而言还是特征分布还是平衡的，活下来的人和死亡人数没有偏差过多。 删除无必要数据空数据处理我们来看看有多少空数据 对空数据我们怎么处理呢，这就要分情况讨论啦。 Age作图 Age ~ Survived。年龄较小的孩子生存的几率大。 因为年龄是一个连续值，而且他会对预测结果产生影响，同时我们发现不同群体中年龄的分布是不同的。 因此我们根据票的等级，在 Titanic 上的兄弟姐妹以及配偶的人数，在 Titanic 上的父母以及子女的人数来将数据分为不同的集合，再用缺失数据所在集合的平均值来进行填充。并判断最后的对 Age ~ Survived 的性质并未产生影响。 Embarked它缺少的数据只有两个，直接用众数填充即可。 Cabin他的数据较为复杂，Cabin 特征值由字母开头，判断船舱按字母分为A，B，C… 于是我们仅提取字母编号，降低维度。然后使用新的字母‘U’填充缺失数据。我们发现缺失数据的游客主要是三等舱的，并且这部分游客的生存率相对较低。 数值化和标准化数值化Ticket 特征值中的一串数字编号对我们没有意义，忽略。下面代码中，我们用正则表达式过滤掉这串数字，并使用 pandas get_dummies 函数进行数值化（以 Ticket 特征值 作为新的特征，0,1 作为新的特征值）。 123456789101112Ticket=[]import rer=re.compile(r'\w*')#正则表达式，查找所有单词字符[a-z/A-Z/0-9]for i in data['Ticket']: sp=i.split(' ')#拆分空格前后字符串，返回列表 if len(sp)==1: Ticket.append('U')#对于只有一串数字的 Ticket，Ticket 增加字符 'U' else: t=r.findall(sp[0])#查找所有单词字符，忽略符号，返回列表 Ticket.append(''.join(t))#将 t 中所有字符串合并data['Ticket']=Ticketdata=pd.get_dummies(data,columns=['Ticket'],prefix='T')#get_dummies：如果DataFrame的某一列中含有k个不同的值，则可以派生出一个k列矩阵或DataFrame（其值全为1和0） getdummies是处理类别醒数据很好的一种方式，这样我们可以将离散的分类变成具体的0，1特征向量，很大程度的加速了电脑计算的速度和监督学习最后训练得到的准确率。对cabin和embarked同样做此操作。最后得到的特征向量如图 标准化偏态分布偏态分布的数据有时不利于模型发现数据中的规律，我们可以使用 Log Transformation 来处理数据，这样可以提高训练的准确度。比如Fare这一特征就存在明显的偏态分布，我们skitlearn提供的函数进行处理，参考 Skewed Distribution and Log Transformation 离群点删除离群点是显著偏离数据集中其余对象的点。离群点来源于操作失误，数据本身的可变性等。我们这里采用箱线法,检测特征 [‘Age’, ‘Parch’, ‘SibSp’, ‘Fare’]的离群点。参考离群点和箱线法 123456789101112131415161718192021222324from collections import Counterdef outlier_detect(n, df, features):#定义函数 outlier_detect 探测离群点，输入变量 n, df, features，返回 outlier outlier_index = [] for feature in features: Q1 = np.percentile(df[feature], 25)#计算上四分位数（1/4） Q3 = np.percentile(df[feature], 75)#计算下四分位数（3/4） IQR = Q3 - Q1 outlier_span = 1.5 * IQR col = ((data[data[feature] &gt; Q3 + outlier_span]) | (data[data[feature] &lt; Q1 - outlier_span])).index outlier_index.extend(col) print('%s: %f (Q3+1.5*IQR) , %f (Q1-1.5*QIR) )' % (feature, Q3 + outlier_span, Q1 - outlier_span)) outlier_index = Counter(outlier_index)#计数 outlier = list(i for i, j in outlier_index.items() if j &gt;= n) print('number of outliers: %d' % len(outlier)) print(df[['Age', 'Parch', 'SibSp', 'Fare']].loc[outlier]) return outlieroutlier = outlier_detect(3, data, ['Age', 'Parch', 'SibSp', 'Fare'])#调用函数 outlier_detectdata = data.drop(outlier) 模型选择与训练模型介绍Boosting模型与bagging模型Bagging：假设我有一个大小为n的训练集D，bagging会从D中有放回的均匀地抽样，假设我用bagging生成了m个新的训练集Di，每个Di的大小为j。由于我有放回的进行抽样，那么在Di中的样本有可能是重复的。如果j=n，这种取样称为bootstrap取样。现在，我们可以用上面的m个训练集来拟合m个模型，然后结合这些模型进行预测。对于回归问题来说，我们平均这些模型的输出;对于分类问题来说，我们进行投票（voting）。 Boosting：Boosting与Bagging主要的不同是：Boosting的base分类器是按顺序训练的（in sequence），训练每个base分类器时所使用的训练集是加权重的，而训练集中的每个样本的权重系数取决于前一个base分类器的性能。如果前一个base分类器错误分类地样本点，那么这个样本点在下一个base分类器训练时会有一个更大的权重。一旦训练完所有的base分类器，我们组合所有的分类器给出最终的预测结果。过程如下图： Adaboost与RandomForest算法Adaboost是一种基于boosting模型的迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。 Adaboost的结构:最后的分类器YM是由数个弱分类器（weak classifier）组合而成的,相当于最后m个弱分类器来投票决定分类结果，而且每个弱分类器的“话语权”因子α大小不一样。 Randomforest是基于bangging模型实现的，他的元分类器是决策树。过程简要概括如下： 从原始训练集中进行bootstrap抽样 用步骤1中的bootstrap样本生成决策树 随机选择特征子集 用上面的特征子集来拆分树的节点 重复1和2两个步骤 集成所有生成的决策树进行预测 模型评估我们采用k 折交叉验证法，更具体的是10 折交叉验证法。 k 折交叉验证（k-fold cross validation）：将 D 划分 k 个大小相似的子集（每份子集尽可能保持数据分布的一致性：子集中不同类别的样本数量比例与 D 基本一致），其中一份作为测试集，剩下 k-1 份为训练集 T，操作 k 次。 例如 D 划分为 D1，D2，… ，D10，第一次使用 D1 作为训练集，第二次使用 D2，第三次使用 D3， … ， 第十次使用 D10 作为测试集。最后计算 k 次测试误差的平均值近似泛化误差。 12345678y = data['Survived']X = data.drop(['Survived'], axis=1).valuesclassifiers = [AdaBoostClassifier( random_state=2), RandomForestClassifier(random_state=2)]for clf in classifiers: score = cross_val_score(clf, X, y, cv=10, scoring='accuracy')#cv=10：10 折交叉验证法，scoring='accuracy'：返回测试精度 print([np.mean(score)])#显示测试精度平均值 我们可以发现随机森林分类器的准确率要高不少。 模型训练但是随机森林是基于决策树的，决策树一直存在着过拟合的问题。 过拟合是学习器性能过好，把样本的一些特性当做了数据的一般性质，从而导致训练误差低但泛化误差高。学习曲线是判断过拟合的一种方式，同时可以判断学习器的表现。学习曲线包括训练误差（或精度）随样例数目的变化曲线与测试误差（或精度）随样例数目的变化曲线。 接下来通过绘制学习曲线，我们发现训练误差始终接近 0，而测试误差始终偏高，说明存在过拟合的问题。 123456789101112131415161718192021222324252627282930313233from sklearn.model_selection import learning_curveimport matplotlib.pyplot as pltdef plot_learning_curve(estimator, title, X, y, cv=10, train_sizes=np.linspace(.1, 1.0, 5)):#定义函数 plot_learning_curve 绘制学习曲线。train_sizes 初始化为 array([ 0.1 , 0.325, 0.55 , 0.775, 1. ]),cv 初始化为 10，以后调用函数时不再输入这两个变量 plt.figure() plt.title(title)#设置图的 title plt.xlabel('Training examples')#横坐标 plt.ylabel('Score')#纵坐标 train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, train_sizes=train_sizes)#使用 10 折交叉验证法，对 train_sizes*m（m为总的样例数目） 个的数据进行训练，返回训练精度 train_scores,测试精度 test_scores train_scores_mean = np.mean(train_scores, axis=1)#计算平均值 train_scores_std = np.std(train_scores, axis=1)#计算标准差 test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid()#设置背景的网格 plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='g')#设置颜色 plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='r') plt.plot(train_sizes, train_scores_mean, 'o-', color='g', label='traning score')#绘制训练精度曲线 plt.plot(train_sizes, test_scores_mean, 'o-', color='r', label='testing score')#绘制测试精度曲线 plt.legend(loc='best') return pltg = plot_learning_curve(RandomForestClassifier(), 'RFC', X, y)#调用函数 plot_learning_curve 绘制随机森林学习器学习曲线 要解决这一问题只能通过调参。 skitlearn中的randomforestclassifer函数具有非常多的参数，列举几个如下。 参数 特点 n_estimators 基学习器数目（默认值10） 基本趋势是值越大精度越高 ，直到达到一个上限 criterion 选择算法 gini 或者 entropy (默认 gini) 视具体情况定 max_features 2.2.3节中子集的大小，即k值（默认 sqrt(n_features)） max_depth 决策树深度 过小基学习器欠拟合，过大基学习器过拟合。粗调节 max_leaf_nodes 最大叶节点数（默认无限制） 粗调节 min_samples_split 分裂时最小样本数，默认2 细调节,越小模型越复杂 min_samples_leaf 叶节点最小样本数，默认2 细调节，越小模型越复杂 bootstrap 是否采用自助法进行样本抽样（默认使用） 决定基学习器样本是否一致 我们先通过尝试找到最好的n_estimators和max_depth，max_leaf_nodes 参数 1234567891011121314151617181920def para_tune(para, X, y): # clf = RandomForestClassifier(n_estimators=para) #n_estimators 设置为 para score = np.mean(cross_val_score(clf, X, y, scoring='accuracy')) return scoredef accurate_curve(para_range, X, y, title): score = [] for para in para_range: score.append(para_tune(para, X, y)) plt.figure() plt.title(title) plt.xlabel('Paramters') plt.ylabel('Score') plt.grid() plt.plot(para_range, score, 'o-') return pltg = accurate_curve([2, 10, 50, 100, 150], X, y, 'n_estimator tuning') 与上面代码类似的我们可以得到下面三张图 接着我们就可以固定这三个影响较大的参数，再利用自动调参函数GridSearchCV来对其他的参数进行粗略的调节。 如此我们得到了最终0.8204的准确率。]]></content>
  </entry>
  <entry>
    <title><![CDATA[python作图]]></title>
    <url>%2F2018%2F01%2F25%2Fpython%E4%BD%9C%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[导论作图工具如恒河沙数，最好用的就是excel的图表功能，还有powermap这样强大的插件，可以非常轻松的解决工作制作报表的问题。但是excel的图表自定义功能并不够强大，在工程和数据科学领域，excle也捉襟见肘。相比之下matlab，mathmeticas这样的商业软件就非常优秀了，然而我偏不它们。我接下来要讲的是python中的Matplotlib和seaborn这样的函数库。 Matplotlib在 Matplotlib 中，大部分图形样式的绘制方法都存在于 pyplot 模块中，一共有160多种图表绘制方法。 2D图绘制我们首先用接下来的代码来引用科学计算函数库numpy和图形绘制库matplotlib 用npm可以很简单地安装好啦，我就不多说了。 12import numpy as npimport matplotlib as plt 然后我们来用下面的代码生成数据 12X = np.linspace(-2*np.pi,2*np.pi,1000) #在-2*np.pi和2*np.pi之间等间隔的生成1000个数据点，X就是一个np 数组Y = np.sin(X)#Y也是一个数组，对应了X的sin值 接着我们键入一下代码，分别生成线型图，柱形图和散点图 123plt.pyplot.plot(X,Y)plt.pyplot.bar(X,Y)plt.pyplot.scatter(X,Y) 输出如下 再接着我们可以尝试着画饼状图，量场图和等高线图 1234z = [1,2,3]plt.pyplot.pie(z)#饼被分成三块，每块的相对面积大小是1，2，3X, y = np.mgrid[0:10, 0:10]#表示的是一个矩阵plt.pyplot.quiver(X, y) 图形输出如下 这样子的功能其实excel也可以实现，还比python要简单，下面是体现python强大的图形自定义功能的时刻到啦 线型图线型图通过 matplotlib.pyplot.plot(args, *kwargs) 方法绘出。其中，args 代表数据输入，而 kwargs 的部分就是用于设置样式参数了。 123456789X = np.linspace(-2 * np.pi, 2 * np.pi, 1000)# 计算 sin() 对应的纵坐标y1 = np.sin(X)# 计算 cos() 对应的纵坐标y2 = np.cos(X)# 向方法中 `*args` 输入 X，y 坐标plt.pyplot.plot(X, y1, color='r', linestyle='--', linewidth=2, alpha=0.8)plt.pyplot.plot(X, y2, color='b', linestyle='-', linewidth=2) 我们就能见到这样漂亮的图拉 样式参数有很多，具体的可以见下表，更具体的麻烦查阅下官方文档]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages博客创建历程]]></title>
    <url>%2F2018%2F01%2F24%2F%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%88%9B%E5%BB%BA%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言对程序员来说，纸上得来终觉浅，绝知此事要躬行，每分专业的能力绝不是看书看来的，听课听来的，而一定要是自己一行一行代码敲来的。不仅如此，每个不甘寂寞的程序猿（媛），还会有着创建一个博客，发布自己所见所感所学的想法或习惯。为了更好地促进自己的进步，我也来加入写博客的大军啦。 怎样的博客应该以什么样的方式来创建博客是本节的重点。 在大一时，为了应付一门课的作业自己开通了cdsn的博客，有很多大牛的博客就扎根在了cdsn。但是对我而言，cdsn总有着烦人的广告，下载资料的时候还总要花钱充值，让我觉得很不爽，所以pass了这个选择。 还有很多人会选择简书，可那里是文艺青年的聚集地，程序员特有的geek氛围要远远差于cdsn。 至于知乎专栏和微信公众号，虽然也可以当作自己写随笔发布的园地，但与博客相比，总觉得差了点什么。 除此之外，还有wordpress和wix等建站方式，其实很方便的，可见即可得的操作，免费版的也不用花钱。主要原因还是自己嫌弃它们一点也不geek，就没有使用这种方式啦。说真的wix创建出来的网站可好看啦，而且官方的引导特别详尽，就像游戏里的引导操作一样，自己很容易就能建成一个很漂亮的网站，我当时 于是就开始考虑起自己搭建一个博客网站。自己之前尝试过用python的django框架搭建过博客网站，它自带有功能很强大的后台，当初自己实现的最终版也差强人意。一个完全由自己掌控的网站是能让人很有成就感的。可想了想自己还是没有选择这个自己实现的网站来作为自己的博客，因为除了后台功能外，自己所有都要手打代码，评论，搜索乃至前台的页面，出了bug要自己调试，万一调试不出来，博客就挂掉了。。。。 那么就考虑到了使用hexo框架来搭建博客，它并不是真正意义上代码的框架，使用它基本不需要编写代码，简介，美观，好用而功能强大。而且它与github pages可以无间合作，还可以省去了购买云主机，云服务器的花销。 创建博客的准备我们首先需要在电脑上安装好node.js , git和一些下载包管理工具如npm，homebrew（ mac自带），apt-get（Ubuntu自带），yum（centos自带）。 接着进入hexo官网，按照上面的步骤安装hexo。（此处非常有必要阅读一下hexo官方对自己的介绍） 我们此时已经安装好了hexo，可以输入以下命令检查是否安装成功，windows用户可能需要再配置一下环境变量， 接着我们输入 hexo init yournamecd yournamenpm install 这样就可以创建自己的博客文件夹，里面有网站的各个静态文件，自己以后的文章也会放在这个文件夹里，进入这个文件夹后，用npm命令来安装各项依赖。 此时我们用hexo new “文章标题”就可以新建一个文章，我们进入source文件夹内的_posts文件夹，就会发现多出了一篇md后缀的文件，这就是我们刚刚创建的文章啦，我们也可以直接在里面创建markdowm文件。接着我们用hexo s命令就能在本地运行这个简单的博客网站了。 然后在github新建一个代码仓库，仓库的名字一定要是”yourusername.github.io”。如我github账户名是yyhyplxyz，我这个代码仓库名字就是“yyhyplxyz.github.io”。 接着去github上添加ssh key，可以参考此网址ssh。]]></content>
      <tags>
        <tag>coding</tag>
      </tags>
  </entry>
</search>
