<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>元昊的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yyhyplxyz.github.io/"/>
  <updated>2018-03-16T16:06:03.000Z</updated>
  <id>http://yyhyplxyz.github.io/</id>
  
  <author>
    <name>Jack Yang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>xgboost</title>
    <link href="http://yyhyplxyz.github.io/2018/03/16/xgboost/"/>
    <id>http://yyhyplxyz.github.io/2018/03/16/xgboost/</id>
    <published>2018-03-16T15:57:19.000Z</published>
    <updated>2018-03-16T16:06:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装过程中出现的问题"><a href="#安装过程中出现的问题" class="headerlink" title="安装过程中出现的问题"></a>安装过程中出现的问题</h2><p>按照brew install gcc@5 pip install xgboost的方式安装出错，经过查阅stackoverflow和仔细阅读报错说明，“command python setup.py egg_info failer with error 1” 可以认定pip安装时少安装了链接文件，感觉这是在做pip安装包的bug。MAC电脑多半会出现这个错误，于是将解决方案分享给大家。</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-15.4.png" alt="git"></p><p>git clone –recursive <a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="noopener">https://github.com/dmlc/xgboost.git</a></p><p>cd xgboost</p><p>./build.sh</p><p>cd python-package</p><p>python3 setup.py install</p><p>运行以上命令即可，其实就是从github上下载源码然后来编译啦。</p><h2 id="简单利用xgboost来提高分类性能"><a href="#简单利用xgboost来提高分类性能" class="headerlink" title="简单利用xgboost来提高分类性能"></a>简单利用xgboost来提高分类性能</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;安装过程中出现的问题&quot;&gt;&lt;a href=&quot;#安装过程中出现的问题&quot; class=&quot;headerlink&quot; title=&quot;安装过程中出现的问题&quot;&gt;&lt;/a&gt;安装过程中出现的问题&lt;/h2&gt;&lt;p&gt;按照brew install gcc@5 pip install xgboo
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>线性回归实现</title>
    <link href="http://yyhyplxyz.github.io/2018/03/16/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yyhyplxyz.github.io/2018/03/16/线性回归实现/</id>
    <published>2018-03-16T06:52:13.000Z</published>
    <updated>2018-03-16T15:54:15.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一元回归基本实现与向量化"><a href="#一元回归基本实现与向量化" class="headerlink" title="一元回归基本实现与向量化"></a>一元回归基本实现与向量化</h2><p>线性回归本质上是处理最优化的问题，即找到a和b，使得$\sum(y<em>{i} - ax</em>{i} - b)^2$  的值                                      尽可能小。</p><p>公式推导如下图</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-15.1.JPG" alt="git"></p><p>接着让我们来实现一元线性回归的方法吧</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.a = <span class="number">0</span></span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x_train,y_train)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x_train.ndim == y_train.ndim == <span class="number">1</span>, <span class="string">"This is a single variable LinearRegression model"</span></span><br><span class="line">        <span class="keyword">assert</span> len(x_train) == len(y_train), <span class="string">"the size of x_train must equal to the size of y_train"</span></span><br><span class="line">        x_mean = np.mean(x_train)</span><br><span class="line">        y_mean = np.mean(y_train)</span><br><span class="line">        <span class="comment">#基本实现</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        numerator = 0.0</span></span><br><span class="line"><span class="string">        denominator = 0.0</span></span><br><span class="line"><span class="string">        for x, y in zip(x_train, y_train):</span></span><br><span class="line"><span class="string">            numerator += (x*y-x*y_mean)</span></span><br><span class="line"><span class="string">            denominator += (x*x - x*x_mean)</span></span><br><span class="line"><span class="string">        self.a = numerator / denominator</span></span><br><span class="line"><span class="string">        self.b = y_mean - self.a * x_mean</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#向量化实现</span></span><br><span class="line">        self.a = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean)</span><br><span class="line">        self.b = y_mean - self.a * x_mean</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, topredict)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> topredict.ndim == <span class="number">1</span>, <span class="string">"This is a single variable LinearRegression model"</span></span><br><span class="line">        <span class="keyword">assert</span> self.a <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.b <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, <span class="string">"Must fit before"</span></span><br><span class="line">        <span class="keyword">return</span> np.array([self._predict(i) <span class="keyword">for</span> i <span class="keyword">in</span> topredict])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.a * x + self.b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(self, y_true, y_predict)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> y_true.shape[<span class="number">0</span>] == y_predict.shape[<span class="number">0</span>], <span class="string">"the size of y_true must be equal to the size of y_predict"</span></span><br><span class="line">        <span class="keyword">return</span> sum(y_true == y_predict) / len(y_true)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, x_test, y_test)</span>:</span></span><br><span class="line">        y_predict = self.predict(x_test)</span><br><span class="line">        <span class="keyword">return</span> self.accuracy_score(y_test, y_predict)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"My single variable simpleLinearRegression"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>])</span><br><span class="line">y = np.array([<span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(x,y)</span><br><span class="line">y_1 = model.predict(x)</span><br><span class="line">print(y_1)</span><br><span class="line"><span class="comment">#print(model.score(y_1, y))</span></span><br></pre></td></tr></table></figure><p>事实上我在上图中推导的公式并非最简的，经过下图的推导可以进一步简化。</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-15.2.JPG" alt="git"></p><p>同时代码方面也可以优化成向量的形式，通过向量运算而非循环迭代可以极大地提高cpu计算效率，而且编译器／操作系统会自发地执行并行计算，加快计算速度。</p><p>代码上面部分中就可以见到了。</p><h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>多元线性回归最关键的是公式的推导，根据维基百科等现有资料，将推导过程呈现如下。</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-15.3.JPG" alt="git"></p><p>代码的实现是蛮容易的，也是像上文一样调用numpy的内置函数做向量化运算的处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.coefficient = <span class="keyword">None</span></span><br><span class="line">        self.intercept_ = <span class="keyword">None</span></span><br><span class="line">        self._theta = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_normal</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">        X_b = np.hstack([np.ones((len(X_train), <span class="number">1</span>)), X_train])</span><br><span class="line">        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)</span><br><span class="line">        self.intercept_ = self._theta[<span class="number">0</span>]</span><br><span class="line">        self.coefficient = self._theta[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_predict)</span>:</span></span><br><span class="line">        X_b = np.hstack([np.ones((len(X_predict), <span class="number">1</span>)), X_predict])</span><br><span class="line">        <span class="keyword">return</span> X_b.dot(self._theta)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"My Multivariable LinearRegression"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一元回归基本实现与向量化&quot;&gt;&lt;a href=&quot;#一元回归基本实现与向量化&quot; class=&quot;headerlink&quot; title=&quot;一元回归基本实现与向量化&quot;&gt;&lt;/a&gt;一元回归基本实现与向量化&lt;/h2&gt;&lt;p&gt;线性回归本质上是处理最优化的问题，即找到a和b，使得$\su
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>sklearn中使用KNN的范例</title>
    <link href="http://yyhyplxyz.github.io/2018/03/14/sklearn%E4%B8%AD%E4%BD%BF%E7%94%A8KNN%E7%9A%84%E8%8C%83%E4%BE%8B/"/>
    <id>http://yyhyplxyz.github.io/2018/03/14/sklearn中使用KNN的范例/</id>
    <published>2018-03-14T12:27:49.000Z</published>
    <updated>2018-03-14T12:55:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导论"><a href="#导论" class="headerlink" title="导论"></a>导论</h2><p>数据挖掘建模中一个非常常见的应用就是商品购买预测，本文将利用sklearn中的KNN算法来做这个案例，最终展现我们预测结果的二维等高线填充地图和实际结果的散点分布。</p><h2 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h2><p>数据格式如下图</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-14.1.png" alt="git"></p><p>常规的做法是要将male和female转换为数值型变量，在本例中暂不做此操作。接着我们要将年龄和预计收入归一化。这是因为收入的数值远大于年龄的数值，考虑到KNN算法的特性，不如此的话将导致收入的影响极大，年龄影响极小。于是我们采用了均值方差归一化的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制我们预测结果的二维等高线填充地图</span></span><br><span class="line">X_set, y_set = X_train, y_train</span><br><span class="line">X1, X2 = np.meshgrid(np.arange(start = X_set[:, <span class="number">0</span>].min() - <span class="number">1</span>, stop = X_set[:, <span class="number">0</span>].max() + <span class="number">1</span>, step = <span class="number">0.01</span>),</span><br><span class="line">                     np.arange(start = X_set[:, <span class="number">1</span>].min() - <span class="number">1</span>, stop = X_set[:, <span class="number">1</span>].max() + <span class="number">1</span>, step = <span class="number">0.01</span>))</span><br><span class="line">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),</span><br><span class="line">             alpha = <span class="number">0.75</span>, cmap = ListedColormap((<span class="string">'red'</span>, <span class="string">'green'</span>)))<span class="comment">#根据我们的预测值0，1来确定不同点\区域的颜色是红或者绿</span></span><br><span class="line">plt.xlim(X1.min(), X1.max())</span><br><span class="line">plt.ylim(X2.min(), X2.max())</span><br></pre></td></tr></table></figure><p>显示如下</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-14.2.png" alt="git"></p><p>最终我们将实际的结果以散点图的形式绘制出来，同样以红绿两色表示二分类问题。</p><p>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> enumerate(np.unique(y_set)):</span><br><span class="line">    plt.scatter(X_set[y_set == j, <span class="number">0</span>], X_set[y_set == j, <span class="number">1</span>],</span><br><span class="line">                c = ListedColormap((<span class="string">'red'</span>, <span class="string">'green'</span>))(i), label = j)</span><br><span class="line">            </span><br><span class="line">plt.title(<span class="string">'K-NN (Training set)'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Age'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Estimated Salary'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-14.3.png" alt="git"></p><p>全部代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">'Social_Network_Ads.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, [<span class="number">2</span>, <span class="number">3</span>]].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">4</span>].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.25</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">classifier = KNeighborsClassifier(n_neighbors = <span class="number">5</span>, metric = <span class="string">'minkowski'</span>, p = <span class="number">2</span>)</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line"><span class="comment"># Visualising the Training set results</span></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">X_set, y_set = X_train, y_train</span><br><span class="line">X1, X2 = np.meshgrid(np.arange(start = X_set[:, <span class="number">0</span>].min() - <span class="number">1</span>, stop = X_set[:, <span class="number">0</span>].max() + <span class="number">1</span>, step = <span class="number">0.01</span>),</span><br><span class="line">                     np.arange(start = X_set[:, <span class="number">1</span>].min() - <span class="number">1</span>, stop = X_set[:, <span class="number">1</span>].max() + <span class="number">1</span>, step = <span class="number">0.01</span>))</span><br><span class="line">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),</span><br><span class="line">             alpha = <span class="number">0.75</span>, cmap = ListedColormap((<span class="string">'red'</span>, <span class="string">'green'</span>)))</span><br><span class="line">plt.xlim(X1.min(), X1.max())</span><br><span class="line">plt.ylim(X2.min(), X2.max())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> enumerate(np.unique(y_set)):</span><br><span class="line">    plt.scatter(X_set[y_set == j, <span class="number">0</span>], X_set[y_set == j, <span class="number">1</span>],</span><br><span class="line">                c = ListedColormap((<span class="string">'red'</span>, <span class="string">'green'</span>))(i), label = j)</span><br><span class="line">            </span><br><span class="line">plt.title(<span class="string">'K-NN (Training set)'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Age'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Estimated Salary'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">from matplotlib.colors import ListedColormap</span></span><br><span class="line"><span class="string">X_set, y_set = X_test, y_test</span></span><br><span class="line"><span class="string">X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),</span></span><br><span class="line"><span class="string">                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))</span></span><br><span class="line"><span class="string">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),</span></span><br><span class="line"><span class="string">             alpha = 0.75, cmap = ListedColormap(('red', 'green')))</span></span><br><span class="line"><span class="string">plt.xlim(X1.min(), X1.max())</span></span><br><span class="line"><span class="string">plt.ylim(X2.min(), X2.max())</span></span><br><span class="line"><span class="string">for i, j in enumerate(np.unique(y_set)):</span></span><br><span class="line"><span class="string">    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],</span></span><br><span class="line"><span class="string">                c = ListedColormap(('red', 'green'))(i), label = j)</span></span><br><span class="line"><span class="string">plt.title('K-NN (Test set)')</span></span><br><span class="line"><span class="string">plt.xlabel('Age')</span></span><br><span class="line"><span class="string">plt.ylabel('Estimated Salary')</span></span><br><span class="line"><span class="string">plt.legend()</span></span><br><span class="line"><span class="string">plt.show()</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;导论&quot;&gt;&lt;a href=&quot;#导论&quot; class=&quot;headerlink&quot; title=&quot;导论&quot;&gt;&lt;/a&gt;导论&lt;/h2&gt;&lt;p&gt;数据挖掘建模中一个非常常见的应用就是商品购买预测，本文将利用sklearn中的KNN算法来做这个案例，最终展现我们预测结果的二维等高线填充地
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>sql语言优化（1）</title>
    <link href="http://yyhyplxyz.github.io/2018/03/14/sql%E8%AF%AD%E8%A8%80%E5%88%9D%E6%8E%A2/"/>
    <id>http://yyhyplxyz.github.io/2018/03/14/sql语言初探/</id>
    <published>2018-03-14T06:24:12.000Z</published>
    <updated>2018-03-17T15:55:58.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本认识"><a href="#基本认识" class="headerlink" title="基本认识"></a>基本认识</h1><p>sql语言不是图灵完备的，顾名思义，它是不能作出图灵机的。从中也可以见的它的语法是蛮简单的。</p><h2 id="几个基本优化方法"><a href="#几个基本优化方法" class="headerlink" title="几个基本优化方法"></a>几个基本优化方法</h2><p>虽然基本语法很简单，大家看看就会了。但是每一个数据提取，修改的操作效率都具有很大提升空间。</p><h3 id="使用join-代替in"><a href="#使用join-代替in" class="headerlink" title="使用join 代替in"></a>使用join 代替in</h3><p>在这个博客地址中<a href="http://openxtiger.iteye.com/blog/1911228" target="_blank" rel="noopener">http://openxtiger.iteye.com/blog/1911228</a> ，作者做了实验证明了join操作会比子查询效率高很多。事实上，子查询操作要循环多次查找子表，耗时较多，而join方法会将多个表格连接起来，可以避免多次循环查找的问题。</p><p>比如下面这两个写法是等价的</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> hotel_info_original <span class="keyword">as</span> c </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> hotel_info_collection h </span><br><span class="line"><span class="keyword">on</span> c.hotel_type=h.hotel_type <span class="keyword">and</span> c.hotel_id =h.hotel_id </span><br><span class="line"><span class="keyword">where</span> h.hotel_id <span class="keyword">is</span> <span class="literal">null</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> c.* <span class="keyword">from</span> hotel_info_original </span><br><span class="line"><span class="keyword">where</span> c.hotel_id <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> h.hotel_id <span class="keyword">from</span>  hotel_info_collection <span class="keyword">where</span> h.hotel_type = c.hotel_type)</span><br></pre></td></tr></table></figure><p>Left join是左连接，即从左表(A)产生一套完整的记录,与匹配的记录(右表(B)) .如果没有匹配,右侧将包含null</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-16.1.png" alt="git"></p><p>实际上它是将左表和右表完全拼接起来，不满足on中条件的全部变成NULL。因此在数据库操作过程中，要尽量的多将语句写在on中，这样可以减少where查询时间，也能够提高效率。还不理解的，可以看下图实例再揣摩一下</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-3-16.2.png" alt="git"></p><h3 id="使用工具进行大表修改"><a href="#使用工具进行大表修改" class="headerlink" title="使用工具进行大表修改"></a>使用工具进行大表修改</h3><p>我们知道在实际应用过程中，当对大表进行修改数据类型时，会造成数据库结构较大的变动。此时mysql会锁表，一切请求只能读不能写，造成大量请求排队，效率极低。因此一个改进的办法就是在主服务器重新建一个表，在旧表上每个entry都安装触发器，修改的请求将会在旧表上进行。这些变化再同步到新表中。</p><p>实际编程较繁琐，起码我不会QUQ</p><p>但是大佬们帮我们封装好了工具，那就是<a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-online-schema-change.html" target="_blank" rel="noopener">pt-online-schema-change</a>，自己去官网上免费下载安装就好啦。</p><p>基本参数信息如下</p><ul><li><p><code>--host=xxx --user=xxx --password=xxx</code><br>连接实例信息，缩写<code>-h xxx -u xxx -p xxx</code>，密码可以使用参数<code>--ask-pass</code> 手动输入。</p></li><li><p><code>--alter</code></p><p>结构变更语句</p></li></ul><ul><li><code>D=db_name,t=table_name</code><br>指定要ddl的数据库名和表名</li><li><code>--execute</code><br>确定修改表，则指定该参数。真正执行alter。</li><li><code>--execute</code><br>确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥</li><li><code>--execute</code><br>确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥</li><li><code>--execute</code><br>确定修改表，则指定该参数。真正执行alter。–dry-run与–execute必须指定一个，二者相互排斥</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pt-online-schema-<span class="keyword">change</span> -ujacky -p xxx -h <span class="string">"10.0.201.34"</span> D=confluence,t=sbtest3 \</span><br><span class="line"><span class="comment">--alter "CHANGE pad f_pad varchar(60) NOT NULL DEFAULT '' " \</span></span><br><span class="line"><span class="comment">--execute</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本认识&quot;&gt;&lt;a href=&quot;#基本认识&quot; class=&quot;headerlink&quot; title=&quot;基本认识&quot;&gt;&lt;/a&gt;基本认识&lt;/h1&gt;&lt;p&gt;sql语言不是图灵完备的，顾名思义，它是不能作出图灵机的。从中也可以见的它的语法是蛮简单的。&lt;/p&gt;
&lt;h2 id=&quot;几个基
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>梯度下降法简单实现</title>
    <link href="http://yyhyplxyz.github.io/2018/03/13/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yyhyplxyz.github.io/2018/03/13/梯度下降法简单实现/</id>
    <published>2018-03-13T14:22:05.000Z</published>
    <updated>2018-03-13T14:22:05.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>KNN算法的简单实现</title>
    <link href="http://yyhyplxyz.github.io/2018/03/13/KNN%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yyhyplxyz.github.io/2018/03/13/KNN算法的简单实现/</id>
    <published>2018-03-13T14:13:12.000Z</published>
    <updated>2018-03-13T15:32:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导论"><a href="#导论" class="headerlink" title="导论"></a>导论</h2><p>KNN算法可以形象的理解成是找出离点A最近的K个点，根据这K个点中不同属性的个数来确定A的属性是怎样的。（在实际问题中，高维空间中点的各个坐标表示了一个特征，属性表示了特征代表的结果。）它可以说是最简单的机器学习算法了，但具有这高数据敏感性和算法复杂度高的问题。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>模仿sklearn的借口，写了简单的一个KNN的类</p><p>KNN算法最终有个投票环节，简单版本是A点周围K个点，每个点都只能投一票，更复杂的版本是根据距离来设定投票的权重，距离近则权重大，可以拥有不止一票的投票权</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN_classifier</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> k &gt;= <span class="number">1</span>, <span class="string">"k must be valid"</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self._X_train = <span class="keyword">None</span></span><br><span class="line">        self._y_train = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> X_train.shape[<span class="number">0</span>] == y_train.shape[<span class="number">0</span>], \</span><br><span class="line">            <span class="string">"the size of X_train must be equal to the size of y_train"</span></span><br><span class="line">        <span class="keyword">assert</span> self.k &lt;= X_train.shape[<span class="number">0</span>], \</span><br><span class="line">            <span class="string">"the size of X_train must be at least k."</span></span><br><span class="line"></span><br><span class="line">        self._X_train = X_train</span><br><span class="line">        self._y_train = y_train</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_predict)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self._X_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._y_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \</span><br><span class="line">                <span class="string">"must fit before predict!"</span></span><br><span class="line">        <span class="keyword">assert</span> X_predict.shape[<span class="number">1</span>] == self._X_train.shape[<span class="number">1</span>], \</span><br><span class="line">                <span class="string">"the feature number of X_predict must be equal to X_train"</span></span><br><span class="line"></span><br><span class="line">        y_predict = [self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> X_predict]</span><br><span class="line">        <span class="keyword">return</span> np.array(y_predict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.shape[<span class="number">0</span>] == self._X_train.shape[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">"the feature number of x must be equal to X_train"</span></span><br><span class="line"></span><br><span class="line">        distances = [sqrt(np.sum((x_train - x) ** <span class="number">2</span>))</span><br><span class="line">                     <span class="keyword">for</span> x_train <span class="keyword">in</span> self._X_train]</span><br><span class="line">        nearest = np.argsort(distances)</span><br><span class="line"></span><br><span class="line">        topK_y = [self._y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:self.k]]</span><br><span class="line">        votes = Counter(topK_y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"KNN(k=%d)"</span> % self.k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_adavnce</span><span class="params">(self, X_test)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> X_test.shape[<span class="number">1</span>] == self._X_train.shape[<span class="number">1</span>], <span class="string">"the number of features in train set and test set must be equal"</span></span><br><span class="line">        <span class="keyword">assert</span> self._X_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._y_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \</span><br><span class="line">                <span class="string">"must fit before predict!"</span></span><br><span class="line">        y_predict = [self._predict_advance(x) <span class="keyword">for</span> x <span class="keyword">in</span> X_test]</span><br><span class="line">        <span class="keyword">return</span> np.array(y_predict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_predict_advance</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.shape[<span class="number">0</span>] == self._X_train.shape[<span class="number">1</span>], <span class="string">"the number of features in train set and test set must be equal"</span></span><br><span class="line">        distances = [sqrt(np.sum((train - x)**<span class="number">2</span>)) <span class="keyword">for</span> train <span class="keyword">in</span> self._X_train]</span><br><span class="line">        <span class="comment">#print(sum(i == 0 for i in distances))</span></span><br><span class="line">        nearest = np.argsort(distances)</span><br><span class="line">        topK_y = [self._y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:self.k]]</span><br><span class="line">        <span class="comment">#print(topK_y)</span></span><br><span class="line">        votes = Counter()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:self.k]:</span><br><span class="line">            votes[self._y_train[i]] += (<span class="number">1</span>/(distances[i]**<span class="number">2</span> + <span class="number">1</span>)) <span class="comment"># 1/i</span></span><br><span class="line">            <span class="comment">#print(votes.most_common(1)[0][0])</span></span><br><span class="line">        <span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_test_split</span><span class="params">(self, X,y, text_ratio, seed = <span class="number">0</span> )</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> X.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>], <span class="string">"the size of X must be equal tp teh size of y"</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0.0</span> &lt;= text_ratio &lt;= <span class="number">1.0</span>, <span class="string">"text_ratio must be valid"</span></span><br><span class="line">        <span class="keyword">if</span> seed:</span><br><span class="line">            np.random.seed(seed)</span><br><span class="line">        shuffleindex = np.random.permutation(len(X))</span><br><span class="line">        text_size = int(len(X) * text_ratio)</span><br><span class="line">        train_index = shuffleindex[text_size:]</span><br><span class="line">        test_index = shuffleindex[:text_size]</span><br><span class="line">        x_train = X[train_index]</span><br><span class="line">        y_train = y[train_index]</span><br><span class="line">        x_test = X[test_index]</span><br><span class="line">        y_test = y[test_index]</span><br><span class="line">        <span class="keyword">return</span> x_train, x_test,y_train,y_test</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, x_test, y_test)</span>:</span></span><br><span class="line">        y_predict = self.predict(x_test)</span><br><span class="line">        <span class="keyword">return</span> accuracy_score(y_test, y_predict)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(test, predict)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> sum(test == predict) / len(predict)</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">temp = KNN_classifier(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>)</span><br><span class="line">my_knn_clf = KNN_classifier(k=<span class="number">3</span>)</span><br><span class="line">my_knn_clf.fit(X_train, y_train)</span><br><span class="line"><span class="comment">#y_predict = my_knn_clf.predict(X_test)</span></span><br><span class="line">print(X_test.shape)</span><br><span class="line">y_predict = my_knn_clf.predict_adavnce(X_test)</span><br><span class="line">print(y_predict.shape)</span><br><span class="line">print(y_test.shape)</span><br><span class="line">print(sum(y_test == y_predict) / len(y_test))</span><br></pre></td></tr></table></figure><p>以上代码中predict_advance是高级版本的实现，利用手写数字的数据集发现写的代码没错，准确率还是蛮高的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;导论&quot;&gt;&lt;a href=&quot;#导论&quot; class=&quot;headerlink&quot; title=&quot;导论&quot;&gt;&lt;/a&gt;导论&lt;/h2&gt;&lt;p&gt;KNN算法可以形象的理解成是找出离点A最近的K个点，根据这K个点中不同属性的个数来确定A的属性是怎样的。（在实际问题中，高维空间中点的各个坐
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>semantic 学习</title>
    <link href="http://yyhyplxyz.github.io/2018/01/28/semantic-%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yyhyplxyz.github.io/2018/01/28/semantic-学习/</id>
    <published>2018-01-28T07:25:02.000Z</published>
    <updated>2018-01-28T08:42:39.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Semantic UI是一个在github上已经获得39264个star的漂亮的css框架，它具有语义化特点，也就是说语法特别容易上手。</p><h3 id="安装与简单使用"><a href="#安装与简单使用" class="headerlink" title="安装与简单使用"></a>安装与简单使用</h3><p>安装教程在<a href="https://semantic-ui.com/introduction/getting-started.html" target="_blank" rel="noopener">官网</a>。</p><p>分为简单安装和完全安装。简单安装只要下载了对应的css和js文件即可，我们在写前端时，引用对应的文件即可。</p><p>完全安装较麻烦一些，但可以支持更换主题，定制各按钮，表格样式等操作。</p><p>注意，如果只是简单安装的话，官网上给出的include in your html需要注意更改文件目录</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-284.png" alt="git"></p><p>我们可以在官方文档中找到多个样式的代码，看哪个自己喜欢的就复制粘贴一下，如下面的代码自己就能在网页中显示图中的效果。</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-285.png" alt="git"></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"/Users/yangyuanhao/semantic/dist/semantic.min.css"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span></span></span><br><span class="line"><span class="tag">  <span class="attr">src</span>=<span class="string">"https://code.jquery.com/jquery-3.1.1.min.js"</span></span></span><br><span class="line"><span class="tag">  <span class="attr">integrity</span>=<span class="string">"sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8="</span></span></span><br><span class="line"><span class="tag">  <span class="attr">crossorigin</span>=<span class="string">"anonymous"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"semantic/dist/semantic.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">button</span> <span class="attr">class</span>=<span class="string">"ui button"</span>&gt;</span>Follow<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="更换主题与样式修改"><a href="#更换主题与样式修改" class="headerlink" title="更换主题与样式修改"></a>更换主题与样式修改</h3><p>当我们使用Semantic UI来构建网页时，我们有时候会发现自己的网页打开的比较慢，这是由于国内的网络环境造成的。<img src="http://p35v64prn.bkt.clouddn.com/2018-1-286.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-287.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-288.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-289.png" alt="git"></p><h3 id="网站的整体布局"><a href="#网站的整体布局" class="headerlink" title="网站的整体布局"></a>网站的整体布局</h3><p>inverted 反色处理</p><p>左右边距可以用container</p><p>segment左右无空，用于网页底部等</p><p>grid</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;Semantic UI是一个在github上已经获得39264个star的漂亮的css框架，它具有语义化特点，也就是说语法特别容易上手。&lt;/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>经济机器是如何运转的</title>
    <link href="http://yyhyplxyz.github.io/2018/01/27/%E7%BB%8F%E6%B5%8E%E6%9C%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%90%E8%BD%AC%E7%9A%84/"/>
    <id>http://yyhyplxyz.github.io/2018/01/27/经济机器是如何运转的/</id>
    <published>2018-01-26T16:38:39.000Z</published>
    <updated>2018-01-26T16:38:39.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>奥斯维辛:一部尘封的历史</title>
    <link href="http://yyhyplxyz.github.io/2018/01/27/%E5%A5%A5%E6%96%AF%E7%BB%B4%E8%BE%9B-%E4%B8%80%E9%83%A8%E5%B0%98%E5%B0%81%E7%9A%84%E5%8E%86%E5%8F%B2/"/>
    <id>http://yyhyplxyz.github.io/2018/01/27/奥斯维辛-一部尘封的历史/</id>
    <published>2018-01-26T16:35:54.000Z</published>
    <updated>2018-01-26T16:35:54.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>jquery实现记事本</title>
    <link href="http://yyhyplxyz.github.io/2018/01/27/jquery%E5%AE%9E%E7%8E%B0%E8%AE%B0%E4%BA%8B%E6%9C%AC/"/>
    <id>http://yyhyplxyz.github.io/2018/01/27/jquery实现记事本/</id>
    <published>2018-01-26T16:34:58.000Z</published>
    <updated>2018-01-26T16:34:58.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>大明1556</title>
    <link href="http://yyhyplxyz.github.io/2018/01/27/%E5%A4%A7%E6%98%8E1556/"/>
    <id>http://yyhyplxyz.github.io/2018/01/27/大明1556/</id>
    <published>2018-01-26T16:26:08.000Z</published>
    <updated>2018-01-26T16:30:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>当国家（政府）有难时，牺牲的一定是平民百姓或商人。</p><p>（当民情汹涌时，）为了社会稳定，政府一定会牺牲政治斗争失败的官员或商人。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当国家（政府）有难时，牺牲的一定是平民百姓或商人。&lt;/p&gt;
&lt;p&gt;（当民情汹涌时，）为了社会稳定，政府一定会牺牲政治斗争失败的官员或商人。&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>东野圭吾的《白金数据》</title>
    <link href="http://yyhyplxyz.github.io/2018/01/26/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE%E7%9A%84%E3%80%8A%E7%99%BD%E9%87%91%E6%95%B0%E6%8D%AE%E3%80%8B/"/>
    <id>http://yyhyplxyz.github.io/2018/01/26/东野圭吾的《白金数据》/</id>
    <published>2018-01-26T15:56:06.000Z</published>
    <updated>2018-01-26T16:29:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>考完试后回到家里，闲来无事便读了一个下午读完了这本新出的侦探小说。比较符合我对东野圭吾的预期，小说很有悬念，比较刺激，但文学性和思想性不足，只是消遣性读物。</p><p>后面会涉及严重剧透=======================================</p><p>故事主要描述了日本想全国范围内收集民众的DNA数据，从而通过罪犯遗留下来的任何可能携带基因的物品，达到快速找到罪犯亲属或罪犯的目的。但这样巨大的工程容易造成民众隐私的泄漏，同时在小说中通过一系列无法通过基因数据库找到罪犯消息的犯罪案件和主角的被嫁祸，几个重要配角的反转，揭示了政府重要官员在数据库中做的手脚——他们的亲属犯罪将无法通过基因匹配找到罪犯的信息，自然的也将无法通过基因匹配查到他们身上。</p><p>这些政府官员的数据就是白金数据。</p><p>小说想向我们揭示大数据技术下个人隐私泄漏的可能，这不过是老生常谈。支付宝之前的年度账单事件就拔出萝卜带着泥，牵扯了一大批不合理读取用户个人数据的app。可以想象对掌控着我们所有社交信息的腾讯公司而言，我们几乎就是白纸一张，一切都能被人一眼望到底。</p><p>作者还想引起我们的思考，人心到底是不是完全物质化的呢？毕竟激素，神经递质，他们主宰着我们的情绪和思想。随着技术的进步，我们完全可以人造情感和思想。无奈太过浅尝辄止，作者本身并没有通过故事深入讨论这个话题。</p><p>东野圭吾应该也试图展示政府高级官员的龌龊。太阳底下没有新鲜事，全国范围内收集民众的DNA数据是一个涉及法律和公安执法的变革，历朝历代的变革根本目的都是更好地维护统治阶级的利益。屁股决定脑袋，换成说是日本政坛大佬都会这样做的。君王一怒，伏尸百万，流血漂橹。相比之下，放点儿白金数据都是小儿科的了。网上不就有纪录片和帖子讨论朴槿惠为了邪教献祭牺牲了岁月号三百多名学生的事情吗？想想令人毛骨悚然，但再想想史书不绝于笔的“族”，又为之泰然。<a href="https://www.zhihu.com/question/52059063" target="_blank" rel="noopener">详情</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;考完试后回到家里，闲来无事便读了一个下午读完了这本新出的侦探小说。比较符合我对东野圭吾的预期，小说很有悬念，比较刺激，但文学性和思想性不足，只是消遣性读物。&lt;/p&gt;
&lt;p&gt;后面会涉及严重剧透=======================================&lt;/p
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>利用随机森林法预测Titanic乘客生存率</title>
    <link href="http://yyhyplxyz.github.io/2018/01/26/%E5%88%A9%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%B3%95%E9%A2%84%E6%B5%8BTitanic%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E7%8E%87/"/>
    <id>http://yyhyplxyz.github.io/2018/01/26/利用随机森林法预测Titanic乘客生存率/</id>
    <published>2018-01-26T11:50:06.000Z</published>
    <updated>2018-01-26T13:28:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在kaggle上有一个竞赛题目，是如何根据泰坦尼克号上的已知的乘客数据来预测某一乘客在该轮船上能否存活。<a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">题目</a>本身就是一道供初学者进行数据分析学习的问题，也有很多大佬给出了自己的数据训练的tutorial，自己就在这里分享下自己在借鉴了一些大佬的经验之后给出的自己的解决方案。</p><h2 id="解决流程"><a href="#解决流程" class="headerlink" title="解决流程"></a>解决流程</h2><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>数据清洗是所有数据分析问题第一步要解决的，我们首先来看下官方对数据集的说明。</p><ul><li>PassengerId: 编号 </li><li>Survived: 0 = 死亡，1 = 生存</li><li>Pclass: 船票级别 1 = 高级， 2 = 中等， 3 = 低等</li><li>Name: 名称</li><li>Sex: male = 男性，female = 女性</li><li>Age: 年龄</li><li>SibSp: 在 Titanic 上的兄弟姐妹以及配偶的人数</li><li>Parch: 在 Titanic 上的父母以及子女的人数</li><li>Ticket: 船票编号</li><li>Fare: 工资</li><li>Cabin: 所在的船舱</li><li>Embarked: 登船的港口 C = Cherbourg, Q = Queenstown, S = Southampton</li></ul><p>接下来我们让我们读取数据并对数据有一个初步的感性认识。</p><p>我们首先来做定性分析，看一下特征类别分布是否平衡。类别平衡指分类样例不同类别的训练样例数目差别不大。当差别很大时，为类别不平衡。当类别不平衡的时候，例如正反比为 9:1，学习器将所有样本判别为正例的正确率都能达到 0.9。这时候，我们就需要使用 “再缩放”、“欠采样”、“过采样”、“阈值移动” 等方法。如下图，我们发现总体而言还是特征分布还是平衡的，活下来的人和死亡人数没有偏差过多。</p><h4 id="删除无必要数据"><a href="#删除无必要数据" class="headerlink" title="删除无必要数据"></a>删除无必要数据</h4><h4 id="空数据处理"><a href="#空数据处理" class="headerlink" title="空数据处理"></a>空数据处理</h4><p>我们来看看有多少空数据</p><p>对空数据我们怎么处理呢，这就要分情况讨论啦。</p><h6 id="Age"><a href="#Age" class="headerlink" title="Age"></a>Age</h6><p>作图 Age ~ Survived。年龄较小的孩子生存的几率大。<img src="http://p35v64prn.bkt.clouddn.com/7.png" alt="git"></p><p>因为年龄是一个连续值，而且他会对预测结果产生影响，同时我们发现不同群体中年龄的分布是不同的。<img src="http://p35v64prn.bkt.clouddn.com/4.png" alt="git"><img src="http://p35v64prn.bkt.clouddn.com/5.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/6.png" alt="git"></p><p>因此我们根据票的等级，在 Titanic 上的兄弟姐妹以及配偶的人数，在 Titanic 上的父母以及子女的人数来将数据分为不同的集合，再用缺失数据所在集合的平均值来进行填充。并判断最后的对 Age ~ Survived 的性质并未产生影响。<img src="http://p35v64prn.bkt.clouddn.com/8.png" alt="git"></p><h5 id="Embarked"><a href="#Embarked" class="headerlink" title="Embarked"></a>Embarked</h5><p>它缺少的数据只有两个，直接用众数填充即可。<img src="http://p35v64prn.bkt.clouddn.com/9.png" alt="git"></p><h5 id="Cabin"><a href="#Cabin" class="headerlink" title="Cabin"></a>Cabin</h5><p>他的数据较为复杂，Cabin 特征值由字母开头，判断船舱按字母分为A，B，C…</p><p>于是我们仅提取字母编号，降低维度。然后使用新的字母‘U’填充缺失数据。我们发现缺失数据的游客主要是三等舱的，并且这部分游客的生存率相对较低。<img src="http://p35v64prn.bkt.clouddn.com/10.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/11.png" alt="git"></p><h4 id="数值化和标准化"><a href="#数值化和标准化" class="headerlink" title="数值化和标准化"></a>数值化和标准化</h4><h5 id="数值化"><a href="#数值化" class="headerlink" title="数值化"></a>数值化</h5><p>Ticket 特征值中的一串数字编号对我们没有意义，忽略。下面代码中，我们用正则表达式过滤掉这串数字，并使用 pandas get_dummies 函数进行数值化（以 Ticket 特征值 作为新的特征，0,1 作为新的特征值）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Ticket=[]</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">r=re.compile(<span class="string">r'\w*'</span>)<span class="comment">#正则表达式，查找所有单词字符[a-z/A-Z/0-9]</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="string">'Ticket'</span>]:</span><br><span class="line">    sp=i.split(<span class="string">' '</span>)<span class="comment">#拆分空格前后字符串，返回列表</span></span><br><span class="line">    <span class="keyword">if</span> len(sp)==<span class="number">1</span>:</span><br><span class="line">       Ticket.append(<span class="string">'U'</span>)<span class="comment">#对于只有一串数字的 Ticket，Ticket 增加字符 'U'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">       t=r.findall(sp[<span class="number">0</span>])<span class="comment">#查找所有单词字符，忽略符号，返回列表</span></span><br><span class="line">       Ticket.append(<span class="string">''</span>.join(t))<span class="comment">#将 t 中所有字符串合并</span></span><br><span class="line">data[<span class="string">'Ticket'</span>]=Ticket</span><br><span class="line">data=pd.get_dummies(data,columns=[<span class="string">'Ticket'</span>],prefix=<span class="string">'T'</span>)<span class="comment">#get_dummies：如果DataFrame的某一列中含有k个不同的值，则可以派生出一个k列矩阵或DataFrame（其值全为1和0）</span></span><br></pre></td></tr></table></figure><p>getdummies是处理类别醒数据很好的一种方式，这样我们可以将离散的分类变成具体的0，1特征向量，很大程度的加速了电脑计算的速度和监督学习最后训练得到的准确率。对cabin和embarked同样做此操作。最后得到的特征向量如图</p><p><img src="http://p35v64prn.bkt.clouddn.com/12.png" alt="git"></p><h5 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h5><h6 id="偏态分布"><a href="#偏态分布" class="headerlink" title="偏态分布"></a>偏态分布</h6><p>偏态分布的数据有时不利于模型发现数据中的规律，我们可以使用 Log Transformation 来处理数据，这样可以提高训练的准确度。比如Fare这一特征就存在明显的偏态分布，我们skitlearn提供的函数进行处理，参考 <a href="http://www.statisticshowto.com/probability-and-statistics/skewed-distribution/" target="_blank" rel="noopener">Skewed Distribution and Log Transformation</a></p><p><img src="http://p35v64prn.bkt.clouddn.com/14.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/15.png" alt="git"></p><h6 id="离群点删除"><a href="#离群点删除" class="headerlink" title="离群点删除"></a>离群点删除</h6><p>离群点是显著偏离数据集中其余对象的点。离群点来源于操作失误，数据本身的可变性等。我们这里采用箱线法,检测特征 [‘Age’, ‘Parch’, ‘SibSp’, ‘Fare’]的离群点。参考<a href="http://blog.csdn.net/littlely_ll/article/details/68486537" target="_blank" rel="noopener">离群点和箱线法</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outlier_detect</span><span class="params">(n, df, features)</span>:</span><span class="comment">#定义函数 outlier_detect 探测离群点，输入变量 n, df, features，返回 outlier</span></span><br><span class="line">    outlier_index = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        Q1 = np.percentile(df[feature], <span class="number">25</span>)<span class="comment">#计算上四分位数（1/4）</span></span><br><span class="line">        Q3 = np.percentile(df[feature], <span class="number">75</span>)<span class="comment">#计算下四分位数（3/4）</span></span><br><span class="line">        IQR = Q3 - Q1</span><br><span class="line">        outlier_span = <span class="number">1.5</span> * IQR</span><br><span class="line">        col = ((data[data[feature] &gt; Q3 + outlier_span]) |</span><br><span class="line">               (data[data[feature] &lt; Q1 - outlier_span])).index</span><br><span class="line">        outlier_index.extend(col)</span><br><span class="line">        print(<span class="string">'%s: %f (Q3+1.5*IQR) , %f (Q1-1.5*QIR) )'</span> %</span><br><span class="line">              (feature, Q3 + outlier_span, Q1 - outlier_span))</span><br><span class="line">    outlier_index = Counter(outlier_index)<span class="comment">#计数</span></span><br><span class="line">    outlier = list(i <span class="keyword">for</span> i, j <span class="keyword">in</span> outlier_index.items() <span class="keyword">if</span> j &gt;= n)</span><br><span class="line">    print(<span class="string">'number of outliers: %d'</span> % len(outlier))</span><br><span class="line">    print(df[[<span class="string">'Age'</span>, <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Fare'</span>]].loc[outlier])</span><br><span class="line">    <span class="keyword">return</span> outlier</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">outlier = outlier_detect(<span class="number">3</span>, data, [<span class="string">'Age'</span>, <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Fare'</span>])<span class="comment">#调用函数 outlier_detect</span></span><br><span class="line">data = data.drop(outlier)</span><br></pre></td></tr></table></figure><p><img src="http://p35v64prn.bkt.clouddn.com/13.png" alt="git"></p><h3 id="模型选择与训练"><a href="#模型选择与训练" class="headerlink" title="模型选择与训练"></a>模型选择与训练</h3><h4 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h4><h6 id="Boosting模型与bagging模型"><a href="#Boosting模型与bagging模型" class="headerlink" title="Boosting模型与bagging模型"></a>Boosting模型与bagging模型</h6><p>Bagging：假设我有一个大小为n的训练集D，bagging会从D中有放回的均匀地抽样，假设我用bagging生成了m个新的训练集Di，每个Di的大小为j。由于我有放回的进行抽样，那么在Di中的样本有可能是重复的。如果j=n，这种取样称为bootstrap取样。现在，我们可以用上面的m个训练集来拟合m个模型，然后结合这些模型进行预测。对于回归问题来说，我们平均这些模型的输出;对于分类问题来说，我们进行投票（voting）。</p><p>Boosting：Boosting与Bagging主要的不同是：Boosting的base分类器是按顺序训练的（in sequence），训练每个base分类器时所使用的训练集是加权重的，而训练集中的每个样本的权重系数取决于前一个base分类器的性能。如果前一个base分类器错误分类地样本点，那么这个样本点在下一个base分类器训练时会有一个更大的权重。一旦训练完所有的base分类器，我们组合所有的分类器给出最终的预测结果。过程如下图：</p><p><img src="http://img.blog.csdn.net/20160523164415904" alt="git"></p><h6 id="Adaboost与RandomForest算法"><a href="#Adaboost与RandomForest算法" class="headerlink" title="Adaboost与RandomForest算法"></a>Adaboost与RandomForest算法</h6><p>Adaboost是一种基于boosting模型的迭代算法，其核心思想是针对同一个训练集训练不同的<a href="http://baike.baidu.com/view/895803.htm" target="_blank" rel="noopener">分类器</a>(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。</p><p>Adaboost的结构:最后的分类器YM是由数个弱分类器（weak classifier）组合而成的,相当于最后m个弱分类器来投票决定分类结果，而且每个弱分类器的“话语权”因子α大小不一样。</p><p>Randomforest是基于bangging模型实现的，他的元分类器是决策树。过程简要概括如下：</p><ol><li>从原始训练集中进行bootstrap抽样</li><li>用步骤1中的bootstrap样本生成决策树<ul><li>随机选择特征子集</li><li>用上面的特征子集来拆分树的节点</li></ul></li><li>重复1和2两个步骤</li><li>集成所有生成的决策树进行预测</li></ol><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><p>我们采用k 折交叉验证法，更具体的是10 折交叉验证法。</p><ul><li>k 折交叉验证（k-fold cross validation）：将 D 划分 k 个大小相似的子集（每份子集尽可能保持数据分布的一致性：子集中不同类别的样本数量比例与 D 基本一致），其中一份作为测试集，剩下 k-1 份为训练集 T，操作 k 次。 例如 D 划分为 D1，D2，… ，D10，第一次使用 D1 作为训练集，第二次使用 D2，第三次使用 D3， … ， 第十次使用 D10 作为测试集。最后计算 k 次测试误差的平均值近似泛化误差。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y = data[<span class="string">'Survived'</span>]</span><br><span class="line">X = data.drop([<span class="string">'Survived'</span>], axis=<span class="number">1</span>).values</span><br><span class="line"></span><br><span class="line">classifiers = [AdaBoostClassifier(</span><br><span class="line">    random_state=<span class="number">2</span>), RandomForestClassifier(random_state=<span class="number">2</span>)]</span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> classifiers:</span><br><span class="line">    score = cross_val_score(clf, X, y, cv=<span class="number">10</span>, scoring=<span class="string">'accuracy'</span>)<span class="comment">#cv=10：10 折交叉验证法，scoring='accuracy'：返回测试精度</span></span><br><span class="line">    print([np.mean(score)])<span class="comment">#显示测试精度平均值</span></span><br></pre></td></tr></table></figure><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-251.png" alt="git"></p><p>我们可以发现随机森林分类器的准确率要高不少。</p><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>但是随机森林是基于决策树的，决策树一直存在着过拟合的问题。</p><p>过拟合是学习器性能过好，把样本的一些特性当做了数据的一般性质，从而导致训练误差低但泛化误差高。学习曲线是判断过拟合的一种方式，同时可以判断学习器的表现。学习曲线包括训练误差（或精度）随样例数目的变化曲线与测试误差（或精度）随样例数目的变化曲线。</p><p>接下来通过绘制学习曲线，我们发现训练误差始终接近 0，而测试误差始终偏高，说明存在过拟合的问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, cv=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>)</span>)</span>:</span><span class="comment">#定义函数 plot_learning_curve 绘制学习曲线。train_sizes 初始化为 array([ 0.1  ,  0.325,  0.55 ,  0.775,  1.   ]),cv 初始化为 10，以后调用函数时不再输入这两个变量</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)<span class="comment">#设置图的 title</span></span><br><span class="line">    plt.xlabel(<span class="string">'Training examples'</span>)<span class="comment">#横坐标</span></span><br><span class="line">    plt.ylabel(<span class="string">'Score'</span>)<span class="comment">#纵坐标</span></span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv,</span><br><span class="line">                                                            train_sizes=train_sizes)<span class="comment">#使用 10 折交叉验证法，对 train_sizes*m（m为总的样例数目） 个的数据进行训练，返回训练精度 train_scores,测试精度 test_scores </span></span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)<span class="comment">#计算平均值</span></span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)<span class="comment">#计算标准差</span></span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()<span class="comment">#设置背景的网格</span></span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std,</span><br><span class="line">                     alpha=<span class="number">0.1</span>, color=<span class="string">'g'</span>)<span class="comment">#设置颜色</span></span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std,</span><br><span class="line">                     alpha=<span class="number">0.1</span>, color=<span class="string">'r'</span>)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">'g'</span>,</span><br><span class="line">             label=<span class="string">'traning score'</span>)<span class="comment">#绘制训练精度曲线</span></span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>,</span><br><span class="line">             label=<span class="string">'testing score'</span>)<span class="comment">#绘制测试精度曲线</span></span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">g = plot_learning_curve(RandomForestClassifier(), <span class="string">'RFC'</span>, X, y)<span class="comment">#调用函数 plot_learning_curve 绘制随机森林学习器学习曲线</span></span><br></pre></td></tr></table></figure><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-25.png" alt="git"></p><p>要解决这一问题只能通过调参。</p><p>skitlearn中的randomforestclassifer函数具有非常多的参数，列举几个如下。</p><table><thead><tr><th>参数</th><th></th><th>特点</th></tr></thead><tbody><tr><td>n_estimators</td><td>基学习器数目（默认值10）</td><td>基本趋势是值越大精度越高 ，直到达到一个上限</td></tr><tr><td>criterion</td><td>选择算法 gini 或者 entropy (默认 gini)</td><td>视具体情况定</td></tr><tr><td>max_features</td><td>2.2.3节中子集的大小，即k值（默认 sqrt(n_features)）</td><td></td></tr><tr><td>max_depth</td><td>决策树深度</td><td>过小基学习器欠拟合，过大基学习器过拟合。粗调节</td></tr><tr><td>max_leaf_nodes</td><td>最大叶节点数（默认无限制）</td><td>粗调节</td></tr><tr><td>min_samples_split</td><td>分裂时最小样本数，默认2</td><td>细调节,越小模型越复杂</td></tr><tr><td>min_samples_leaf</td><td>叶节点最小样本数，默认2</td><td>细调节，越小模型越复杂</td></tr><tr><td>bootstrap</td><td>是否采用自助法进行样本抽样（默认使用）</td><td>决定基学习器样本是否一致</td></tr></tbody></table><p>我们先通过尝试找到最好的n_estimators和max_depth，max_leaf_nodes  参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">para_tune</span><span class="params">(para, X, y)</span>:</span> <span class="comment">#</span></span><br><span class="line">    clf = RandomForestClassifier(n_estimators=para) <span class="comment">#n_estimators 设置为 para</span></span><br><span class="line">    score = np.mean(cross_val_score(clf, X, y, scoring=<span class="string">'accuracy'</span>))</span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accurate_curve</span><span class="params">(para_range, X, y, title)</span>:</span></span><br><span class="line">    score = []</span><br><span class="line">    <span class="keyword">for</span> para <span class="keyword">in</span> para_range:</span><br><span class="line">        score.append(para_tune(para, X, y))</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.xlabel(<span class="string">'Paramters'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Score'</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.plot(para_range, score, <span class="string">'o-'</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">g = accurate_curve([<span class="number">2</span>, <span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">150</span>], X, y, <span class="string">'n_estimator tuning'</span>)</span><br></pre></td></tr></table></figure><p>与上面代码类似的我们可以得到下面三张图</p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-253.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-254.png" alt="git"></p><p><img src="http://p35v64prn.bkt.clouddn.com/2018-1-255.png" alt="git"></p><p>接着我们就可以固定这三个影响较大的参数，再利用自动调参函数GridSearchCV来对其他的参数进行粗略的调节。<img src="http://p35v64prn.bkt.clouddn.com/2018-1-257.png" alt="git"></p><p>如此我们得到了最终0.8204的准确率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;在kaggle上有一个竞赛题目，是如何根据泰坦尼克号上的已知的乘客数据来预测某一乘客在该轮船上能否存活。&lt;a href=&quot;h
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python作图</title>
    <link href="http://yyhyplxyz.github.io/2018/01/25/python%E4%BD%9C%E5%9B%BE/"/>
    <id>http://yyhyplxyz.github.io/2018/01/25/python作图/</id>
    <published>2018-01-25T03:11:19.000Z</published>
    <updated>2018-01-26T16:29:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导论"><a href="#导论" class="headerlink" title="导论"></a>导论</h2><p>作图工具如恒河沙数，最好用的就是excel的图表功能，还有powermap这样强大的插件，可以非常轻松的解决工作制作报表的问题。但是excel的图表自定义功能并不够强大，在工程和数据科学领域，excle也捉襟见肘。相比之下matlab，mathmeticas这样的商业软件就非常优秀了，然而我偏不它们。我接下来要讲的是python中的Matplotlib和seaborn这样的函数库。</p><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>在 Matplotlib 中，大部分图形样式的绘制方法都存在于 pyplot 模块中，一共有160多种图表绘制方法。</p><h3 id="2D图绘制"><a href="#2D图绘制" class="headerlink" title="2D图绘制"></a>2D图绘制</h3><p>我们首先用接下来的代码来引用科学计算函数库numpy和图形绘制库matplotlib</p><p>用npm可以很简单地安装好啦，我就不多说了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>然后我们来用下面的代码生成数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.linspace(<span class="number">-2</span>*np.pi,<span class="number">2</span>*np.pi,<span class="number">1000</span>) <span class="comment">#在-2*np.pi和2*np.pi之间等间隔的生成1000个数据点，X就是一个np 数组</span></span><br><span class="line">Y = np.sin(X)<span class="comment">#Y也是一个数组，对应了X的sin值</span></span><br></pre></td></tr></table></figure><p>接着我们键入一下代码，分别生成线型图，柱形图和散点图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.pyplot.plot(X,Y)</span><br><span class="line">plt.pyplot.bar(X,Y)</span><br><span class="line">plt.pyplot.scatter(X,Y)</span><br></pre></td></tr></table></figure><p>输出如下</p><p>再接着我们可以尝试着画饼状图，量场图和等高线图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">plt.pyplot.pie(z)<span class="comment">#饼被分成三块，每块的相对面积大小是1，2，3</span></span><br><span class="line">X, y = np.mgrid[<span class="number">0</span>:<span class="number">10</span>, <span class="number">0</span>:<span class="number">10</span>]<span class="comment">#表示的是一个矩阵</span></span><br><span class="line">plt.pyplot.quiver(X, y)</span><br></pre></td></tr></table></figure><p>图形输出如下</p><p>这样子的功能其实excel也可以实现，还比python要简单，下面是体现python强大的图形自定义功能的时刻到啦</p><h4 id="线型图"><a href="#线型图" class="headerlink" title="线型图"></a>线型图</h4><p>线型图通过 matplotlib.pyplot.plot(<em>args, *</em>kwargs) 方法绘出。其中，args 代表数据输入，而 kwargs 的部分就是用于设置样式参数了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = np.linspace(<span class="number">-2</span> * np.pi, <span class="number">2</span> * np.pi, <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 计算 sin() 对应的纵坐标</span></span><br><span class="line">y1 = np.sin(X)</span><br><span class="line"><span class="comment"># 计算 cos() 对应的纵坐标</span></span><br><span class="line">y2 = np.cos(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向方法中 `*args` 输入 X，y 坐标</span></span><br><span class="line">plt.pyplot.plot(X, y1, color=<span class="string">'r'</span>, linestyle=<span class="string">'--'</span>, linewidth=<span class="number">2</span>, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.pyplot.plot(X, y2, color=<span class="string">'b'</span>, linestyle=<span class="string">'-'</span>, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>我们就能见到这样漂亮的图拉</p><p>样式参数有很多，具体的可以见下表，更具体的麻烦查阅下官方文档</p><p><img src="../image/yangshi2.png" alt="git"></p><p><img src="../image/yangshi3.png" alt="git"></p><p><img src="../image/yangshi4.png" alt="git"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;导论&quot;&gt;&lt;a href=&quot;#导论&quot; class=&quot;headerlink&quot; title=&quot;导论&quot;&gt;&lt;/a&gt;导论&lt;/h2&gt;&lt;p&gt;作图工具如恒河沙数，最好用的就是excel的图表功能，还有powermap这样强大的插件，可以非常轻松的解决工作制作报表的问题。但是exce
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hexo + Github Pages博客创建历程</title>
    <link href="http://yyhyplxyz.github.io/2018/01/24/%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%88%9B%E5%BB%BA%E5%8E%86%E7%A8%8B/"/>
    <id>http://yyhyplxyz.github.io/2018/01/24/我的博客创建历程/</id>
    <published>2018-01-23T16:32:23.000Z</published>
    <updated>2018-01-24T15:51:48.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对程序员来说，纸上得来终觉浅，绝知此事要躬行，每分专业的能力绝不是看书看来的，听课听来的，而一定要是自己一行一行代码敲来的。不仅如此，每个不甘寂寞的程序猿（媛），还会有着创建一个博客，发布自己所见所感所学的想法或习惯。为了更好地促进自己的进步，我也来加入写博客的大军啦。</p><h2 id="怎样的博客"><a href="#怎样的博客" class="headerlink" title="怎样的博客"></a>怎样的博客</h2><p>应该以什么样的方式来创建博客是本节的重点。</p><p>在大一时，为了应付一门课的作业自己开通了cdsn的博客，有很多大牛的博客就扎根在了cdsn。但是对我而言，cdsn总有着烦人的广告，下载资料的时候还总要花钱充值，让我觉得很不爽，所以pass了这个选择。</p><p>还有很多人会选择简书，可那里是文艺青年的聚集地，程序员特有的geek氛围要远远差于cdsn。</p><p>至于知乎专栏和微信公众号，虽然也可以当作自己写随笔发布的园地，但与博客相比，总觉得差了点什么。</p><p>除此之外，还有wordpress和wix等建站方式，其实很方便的，可见即可得的操作，免费版的也不用花钱。主要原因还是自己嫌弃它们一点也不geek，就没有使用这种方式啦。说真的wix创建出来的网站可好看啦，而且官方的引导特别详尽，就像游戏里的引导操作一样，自己很容易就能建成一个很漂亮的网站，我当时</p><p>于是就开始考虑起自己搭建一个博客网站。自己之前尝试过用python的django框架搭建过博客网站，它自带有功能很强大的后台，当初自己实现的最终版也差强人意。一个完全由自己掌控的网站是能让人很有成就感的。可想了想自己还是没有选择这个自己实现的网站来作为自己的博客，因为除了后台功能外，自己所有都要手打代码，评论，搜索乃至前台的页面，出了bug要自己调试，万一调试不出来，博客就挂掉了。。。。</p><p>那么就考虑到了使用hexo框架来搭建博客，它并不是真正意义上代码的框架，使用它基本不需要编写代码，简介，美观，好用而功能强大。而且它与github pages可以无间合作，还可以省去了购买云主机，云服务器的花销。</p><h2 id="创建博客的准备"><a href="#创建博客的准备" class="headerlink" title="创建博客的准备"></a>创建博客的准备</h2><p>我们首先需要在电脑上安装好<a href="https://nodejs.org/en/" target="_blank" rel="noopener">node.js</a> , <a href="https://git-scm.com/" target="_blank" rel="noopener">git</a>和一些下载包管理工具如npm，homebrew（    mac自带），apt-get（Ubuntu自带），yum（centos自带）。</p><p>接着进入<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo官网</a>，按照上面的步骤安装hexo。（此处非常有必要阅读一下hexo官方对自己的介绍）</p><p>我们此时已经安装好了hexo，可以输入以下命令检查是否安装成功，windows用户可能需要再配置一下环境变量，</p><p>接着我们输入</p><p><br><br>hexo init <code>yourname</code><br>cd <code>yourname</code><br>npm install<br><br></p><p>这样就可以创建自己的博客文件夹，里面有网站的各个静态文件，自己以后的文章也会放在这个文件夹里，进入这个文件夹后，用npm命令来安装各项依赖。</p><p>此时我们用</p><p>hexo new “文章标题”</p>就可以新建一个文章，我们进入source文件夹内的_posts文件夹，就会发现多出了一篇md后缀的文件，这就是我们刚刚创建的文章啦，我们也可以直接在里面创建markdowm文件。接着我们用<p>hexo s</p>命令就能在本地运行这个简单的博客网站了。<p></p><p>然后在<a href="https://github.com/" target="_blank" rel="noopener">github</a>新建一个代码仓库，仓库的名字一定要是”yourusername.github.io”。如我github账户名是yyhyplxyz，我这个代码仓库名字就是“yyhyplxyz.github.io”。</p><p>接着去github上添加ssh key，可以参考此网址<a href="https://www.cnblogs.com/ayseeing/p/3572582.html" target="_blank" rel="noopener">ssh</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;对程序员来说，纸上得来终觉浅，绝知此事要躬行，每分专业的能力绝不是看书看来的，听课听来的，而一定要是自己一行一行代码敲来的。不仅如此，每个不
      
    
    </summary>
    
    
      <category term="coding" scheme="http://yyhyplxyz.github.io/tags/coding/"/>
    
  </entry>
  
</feed>
